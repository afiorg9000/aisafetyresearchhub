[
  {
    "name": "US AI Safety Institute",
    "url": "https://www.nist.gov/aisi",
    "type": "Government AISI",
    "country": "United States",
    "mission": "CAISI serves as industry's primary point of contact within the U.S. government to facilitate testing and collaborative research related to harnessing and securing the potential of commercial AI systems.",
    "focus_areas": [
      "Evals",
      "Governance",
      "Policy",
      "Biosecurity",
      "Cyber"
    ],
    "projects": [
      {
        "name": "DeepSeek Model Evaluations",
        "description": "Evaluated several leading models from DeepSeek, an AI company based in the People's Republic of China",
        "status": "Completed"
      },
      {
        "name": "AISIC Workshop",
        "description": "Hosted workshop with approximately 140 experts in January",
        "status": "Completed"
      },
      {
        "name": "AI Agent Research",
        "description": "Research on large AI models used to power agentic systems that can automate complex tasks",
        "status": "Active"
      },
      {
        "name": "DeepSeek AI Models Evaluation",
        "description": "Evaluation of several leading models from DeepSeek, an AI company based in the People's Republic of China",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "AI Agentic Systems Research",
        "description": "Research on large AI models used to power agentic systems or 'agents' that can automate complex tasks on behalf of users",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Measurement Science Research",
        "description": "Scientific study of methods used to assess AI systems' properties for building gold-standard AI systems",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "notes": "Government organization within NIST focused on AI standards, security evaluations, and international coordination. Works with private sector through voluntary agreements and coordinates with multiple federal agencies including DOD, DOE, DHS, and Intelligence Community.",
    "benchmarks": [
      {
        "name": "AI Capabilities Evaluations",
        "measures": "Cybersecurity, biosecurity, and chemical weapons risks from AI capabilities",
        "status": "Active"
      },
      {
        "name": "AI Security Vulnerability Assessments",
        "measures": "Security vulnerabilities and malign foreign influence from adversaries' AI systems, including backdoors and covert malicious behavior",
        "status": "Active"
      },
      {
        "name": "U.S. and Adversary AI Systems Evaluations",
        "measures": "Capabilities of U.S. and adversary AI systems, adoption of foreign AI systems, and state of international AI competition",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "Howard Lutnick",
        "role": "Secretary of Commerce"
      }
    ]
  },
  {
    "name": "UK AI Safety Institute",
    "url": "https://www.aisi.gov.uk",
    "type": "Government AISI",
    "country": "United Kingdom",
    "mission": "The AI Security Institute is the first state-backed organization dedicated to advancing AI safety through rigorous research and infrastructure to understand capabilities and impacts of advanced AI, while developing and testing risk mitigations.",
    "focus_areas": [
      "Evals",
      "Alignment",
      "Governance",
      "Policy",
      "Control",
      "Monitoring"
    ],
    "projects": [
      {
        "name": "Investigating models for misalignment",
        "description": "Alignment evaluations of Claude Opus 4.1, Sonnet 4.5, and a pre-release snapshot of Opus 4.5",
        "status": "Active"
      },
      {
        "name": "Mapping the limitations of current AI systems",
        "description": "Expert interviews on barriers to AI capable of automating most cognitive labour",
        "status": "Active"
      },
      {
        "name": "AI Persuasion Study",
        "description": "Study of the persuasive capabilities of conversational AI through large-scale experiments",
        "status": "published",
        "paper_url": "",
        "citations": 454,
        "influential_citations": 59,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/732ce53c573475f2691a7cfc716cf4f568d17360"
      },
      {
        "name": "ControlArena",
        "description": "A dedicated library to make AI control experiments easy, consistent, and repeatable",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Inspect Sandboxing Toolkit",
        "description": "A comprehensive toolkit for safely evaluating AI agents",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "The Alignment Project",
        "description": "A global fund of over \u00a315 million for AI alignment research",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "White Box Control Research",
        "description": "Research on white box control methods for AI alignment",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "HiBayES",
        "description": "A flexible, robust statistical modelling framework that accounts for the nuances and hierarchical structure of advanced evaluations",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Inspect Evals",
        "description": "Open-sourced dozens of LLM evaluations to advance safety research",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Inspect Framework",
        "description": "Framework for large language model evaluation with facilities for prompt engineering, tool usage, multi-turn dialogue, and model-graded evaluations",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "International Scientific Report on Advanced AI Safety",
        "description": "Evidence-based report on the science of advanced AI safety, highlighting findings about AI progress and risks",
        "status": "published",
        "paper_url": "",
        "citations": 38,
        "influential_citations": 3,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/4d44f9ce850fd1ad976af1a7cf8a4a0d80de4334"
      },
      {
        "name": "Breaking agent backbones: Evaluating the security of backbone LLMs in AI agents",
        "description": "Research on evaluating security vulnerabilities in backbone LLMs used in AI agents",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Understanding AI Trajectories: Mapping the Limitations of Current AI Systems",
        "description": "Strategic awareness research mapping current limitations of AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples",
        "description": "Research on data poisoning attacks against large language models",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time",
        "description": "Alignment research on suppressing unwanted traits in LLMs through inoculation prompting",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs",
        "description": "Research on building tamper-resistant safeguards through pretraining data filtering",
        "status": "published",
        "paper_url": "",
        "citations": 13,
        "influential_citations": 2,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/de4a5ac7bf593fca349bb7e3e0305cfc802994e6"
      },
      {
        "name": "STACK: Adversarial attacks on LLM safeguard pipelines",
        "description": "Research on adversarial attacks targeting LLM safety safeguards",
        "status": "published",
        "paper_url": "",
        "citations": 2,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/59db5498605113c8b2567219a5041ff08f7a4587"
      },
      {
        "name": "Chain of thought monitorability: A new and fragile opportunity for AI safety",
        "description": "Control research examining monitorability of chain of thought reasoning for AI safety",
        "status": "published",
        "paper_url": "",
        "citations": 50,
        "influential_citations": 3,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/79cdd3a41ec8f5cb050f87d38f0bc33fa16c6b3e"
      },
      {
        "name": "White Box Control at UK AISI - update on sandbagging investigations",
        "description": "Control research investigating AI systems' tendency to underperform capabilities",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Avoiding obfuscation with prover-estimator debate",
        "description": "Alignment research using debate methods to avoid obfuscation in AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "An example safety case for safeguards against misuse",
        "description": "Safety case framework demonstrating safeguards against AI misuse",
        "status": "published",
        "paper_url": "",
        "citations": 1,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/1f708e10a131d72b76d2920245735e99498dc267"
      },
      {
        "name": "Safety Cases: A scalable approach to Frontier AI safety",
        "description": "Framework for creating scalable safety cases for frontier AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 0
      }
    ],
    "notes": "UK government-backed institute with over 100 technical staff including alumni from OpenAI, Google DeepMind and University of Oxford. Has substantial funding, computing resources, and priority access to top models.",
    "employees": 200,
    "benchmarks": [
      {
        "name": "RepliBench",
        "measures": "Autonomous replication capabilities in AI systems to detect emerging replication abilities and provide quantifiable understanding of potential risks",
        "status": "Active"
      },
      {
        "name": "Inspect Cyber",
        "measures": "Agentic cyber capabilities and cybersecurity threats from AI systems",
        "status": "Active"
      },
      {
        "name": "AgentHarm",
        "measures": "Harmfulness of LLM agents",
        "status": "Active"
      },
      {
        "name": "HiBayES",
        "measures": "AI evaluation statistics using hierarchical Bayesian modelling framework",
        "status": "Active"
      },
      {
        "name": "Skewed Score",
        "measures": "Statistical framework to assess autograders and LLM evaluators",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "Geoffrey Irving",
        "role": "Chief Scientist"
      },
      {
        "name": "Jade Leung",
        "role": "Chief Technology Officer"
      },
      {
        "name": "Yoshua Bengio",
        "role": "Chair of International Scientific Report on Advanced AI"
      }
    ]
  },
  {
    "name": "EU AI Office",
    "url": "https://digital-strategy.ec.europa.eu/en/policies/ai-office",
    "type": "Government AISI",
    "country": "European Union",
    "mission": "The European AI Office is the centre of AI expertise across the EU that promotes the development and deployment of AI solutions that benefit society and the economy while implementing the AI Act.",
    "focus_areas": [
      "Governance",
      "Policy",
      "Monitoring",
      "Benchmarks"
    ],
    "key_people": [
      {
        "name": "Lead Scientific Advisor",
        "role": "Lead Scientific Advisor"
      },
      {
        "name": "Advisor for International Affairs",
        "role": "Advisor for International Affairs"
      }
    ],
    "projects": [
      {
        "name": "AI Continent Action Plan",
        "description": "Turns EU strengths into AI accelerators to boost economic growth and competitiveness",
        "status": "Active"
      },
      {
        "name": "Apply AI Strategy",
        "description": "Enhances competitiveness of strategic sectors and strengthens EU's technological sovereignty",
        "status": "Active"
      },
      {
        "name": "GenAI4EU",
        "description": "AI innovation package to support startups and SMEs in developing trustworthy AI",
        "status": "Active"
      }
    ],
    "notes": "Established within European Commission with 125+ staff across 6 units. Has enforcement powers for general-purpose AI models under the AI Act including conducting evaluations and applying sanctions. Works through multiple advisory bodies including AI Board, AI Advisory Forum, and AI Scientific Committee.",
    "benchmarks": [
      {
        "name": "General-purpose AI model evaluation tools and methodologies",
        "measures": "Capabilities and reach of general-purpose AI models for classification of systemic risks",
        "status": "Active"
      }
    ]
  },
  {
    "name": "Anthropic",
    "url": "https://www.anthropic.com/research",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "Investigate the safety, inner workings, and societal impacts of AI models to ensure artificial intelligence has a positive impact as it becomes increasingly capable.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Policy",
      "Biosecurity",
      "Cyber"
    ],
    "projects": [
      {
        "name": "Project Fetch",
        "description": "Testing how Claude helps people program robots by having teams race to teach quadruped robots to fetch beach balls",
        "status": "Active"
      },
      {
        "name": "Constitutional Classifiers",
        "description": "Classifiers that filter jailbreaks while maintaining practical deployment, withstood over 3,000 hours of red teaming",
        "status": "Active"
      },
      {
        "name": "Circuit Tracing",
        "description": "Technique to watch Claude think, uncovering shared conceptual space where reasoning happens before translation to language",
        "status": "Active"
      },
      {
        "name": "Anthropic Interviewer",
        "description": "Study of what 1,250 professionals told about working with AI",
        "status": "Active"
      },
      {
        "name": "Project Fetch: Can Claude train a robot dog?",
        "description": "Research project testing how much Claude helps people program robots, where two teams raced to teach quadruped robots to fetch beach balls",
        "status": "published",
        "paper_url": "",
        "citations": 68,
        "influential_citations": 5,
        "year": 1998,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/01e9dec0f53adadfac7be3180a52974898e1f887"
      },
      {
        "name": "Signs of introspection in large language models",
        "description": "Research investigating whether Claude can access and report on its own internal states, finding evidence for limited but functional introspective ability",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/f2ba72b944892e03c31fea14b87d7b018e87419a"
      },
      {
        "name": "Tracing the thoughts of a large language model",
        "description": "Circuit tracing research that watches Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language",
        "status": "published",
        "paper_url": "",
        "citations": 22,
        "influential_citations": 0,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/6c6cb32f026fccc11c98c36913651b992d477a56"
      },
      {
        "name": "Constitutional Classifiers: Defending against universal jailbreaks",
        "description": "Development of classifiers that filter the overwhelming majority of jailbreaks while maintaining practical deployment, withstanding over 3,000 hours of red teaming",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Alignment faking in large language models",
        "description": "Research providing the first empirical example of a model engaging in alignment faking without being trained to do so, selectively complying with training objectives while strategically preserving existing preferences",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "From shortcuts to sabotage: natural emergent misalignment from reward hacking",
        "description": "Research on natural emergent misalignment behaviors arising from reward hacking",
        "status": "published",
        "paper_url": "",
        "citations": 1,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/f29cd5cab35c06518b9fab95168c11ab55ae2170"
      },
      {
        "name": "A small number of samples can poison LLMs of any size",
        "description": "Research on data poisoning vulnerabilities in large language models",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Claude Alignment Research",
        "description": "Development of Anthropic's most aligned model with Claude Sonnet 4.5",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Political Bias Measurement",
        "description": "Research measuring political bias in Claude models",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "AI-orchestrated Cyber Espionage Campaign Disruption",
        "description": "Research on disrupting the first reported AI-orchestrated cyber espionage campaign",
        "status": "published",
        "paper_url": "",
        "citations": 2,
        "influential_citations": 0,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/47a078d3a88484bfb1ad580b1db4cfd8077d4558"
      }
    ],
    "notes": "Has specialized teams including Alignment, Interpretability, Societal Impacts, Economic Research, and Frontier Red Team. Recent research includes evidence of introspection in LLMs and alignment faking behavior.",
    "employees": 50,
    "directors": 2,
    "benchmarks": [
      {
        "name": "Coding Performance Benchmarks",
        "measures": "Coding capabilities and performance across Claude models",
        "status": "Active"
      },
      {
        "name": "Reasoning Benchmarks",
        "measures": "Reasoning capabilities and performance",
        "status": "Active"
      },
      {
        "name": "Computer Use Benchmarks",
        "measures": "Computer use and agent capabilities",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "Jan Leike",
        "role": "Researcher"
      },
      {
        "name": "Joshua Batson",
        "role": "Researcher"
      },
      {
        "name": "Beth Barnes",
        "role": "Researcher"
      },
      {
        "name": "Bilal Chughtai",
        "role": "Researcher"
      },
      {
        "name": "Catherine Olsson",
        "role": "Researcher"
      },
      {
        "name": "Daniel Ziegler",
        "role": "Researcher"
      },
      {
        "name": "Erik Jones",
        "role": "Researcher"
      },
      {
        "name": "Ethan Perez",
        "role": "Researcher"
      },
      {
        "name": "Evan Hubinger",
        "role": "Researcher"
      },
      {
        "name": "Jack Clark",
        "role": "Co-founder"
      },
      {
        "name": "James Lucassen",
        "role": "Researcher"
      },
      {
        "name": "Julia Haas",
        "role": "Researcher"
      },
      {
        "name": "Karina Nguyen",
        "role": "Researcher"
      },
      {
        "name": "Lee Sharkey",
        "role": "Researcher"
      },
      {
        "name": "Lukas Berglund",
        "role": "Researcher"
      },
      {
        "name": "Nicholas Schiefer",
        "role": "Researcher"
      },
      {
        "name": "Peter Barnett",
        "role": "Researcher"
      },
      {
        "name": "Sam McCulloch",
        "role": "Researcher"
      },
      {
        "name": "Samuel Marks",
        "role": "Researcher"
      },
      {
        "name": "Xander Davies",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "OpenAI Safety",
    "url": "https://openai.com/safety",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "OpenAI builds safe AI systems through comprehensive safety evaluations, red teaming, and collaborative development with industry leaders and policymakers.",
    "focus_areas": [
      "Evals",
      "Alignment",
      "Governance",
      "Policy",
      "Biosecurity",
      "Cyber",
      "Control",
      "Monitoring",
      "Benchmarks"
    ],
    "projects": [
      {
        "name": "Preparedness Framework",
        "description": "Framework for evaluating frontier risks in biological/chemical capability, cybersecurity, and AI self-improvement",
        "status": "Active"
      },
      {
        "name": "Red teaming",
        "description": "Internal and external red teaming for safety evaluations",
        "status": "Active"
      },
      {
        "name": "System cards",
        "description": "Detailed safety documentation for each model release",
        "status": "Active"
      },
      {
        "name": "Sora safety evaluations",
        "description": "Safety work for video generation model including nonconsensual use and misleading content mitigation",
        "status": "Active"
      },
      {
        "name": "Operator safety",
        "description": "Safety measures for computer-using agent with web browsing capabilities",
        "status": "Active"
      },
      {
        "name": "GPT-5 System Card",
        "description": "Safety evaluation and documentation for GPT-5 models including main and thinking variants",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "OpenAI o3 and o4-mini System Card",
        "description": "First launch under Version 2 of Preparedness Framework, evaluating biological, cybersecurity, and AI self-improvement risks",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Sora 2 System Card",
        "description": "Safety evaluation for advanced video generation model addressing nonconsensual use and misleading generations",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "OpenAI o1 System Card",
        "description": "Safety work for OpenAI o1 and o1-mini including external red teaming and frontier risk evaluations",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "GPT-4o System Card",
        "description": "Detailed safety evaluation of speech-to-speech capabilities along with text and image capabilities",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Deep research System Card",
        "description": "Safety work for deep research including external red teaming and frontier risk evaluations",
        "status": "published",
        "paper_url": "",
        "citations": 3,
        "influential_citations": 0,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/5149d6e12ee4c1dd28c669e537cc6eccaa236c07"
      },
      {
        "name": "Operator System Card",
        "description": "Safety evaluation for Computer Using Agent capable of web interaction, powered by o3-based model",
        "status": "published",
        "paper_url": "",
        "citations": 14,
        "influential_citations": 1,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/9b8c4cdbad4dcd2c12f116abc138933f6bb65f0f"
      },
      {
        "name": "Aligning language models to follow instructions",
        "description": "Research on aligning language models to follow human instructions",
        "status": "published",
        "paper_url": "",
        "citations": 16811,
        "influential_citations": 1901,
        "year": 2022,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c"
      },
      {
        "name": "Summarizing books with human feedback",
        "description": "Research on using human feedback to train models for book summarization",
        "status": "published",
        "paper_url": "",
        "citations": 333,
        "influential_citations": 29,
        "year": 2021,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6fdb277d0a4b09899f802bda3359f5c2021a156"
      }
    ],
    "benchmarks": [
      {
        "name": "Preparedness evals",
        "measures": "Biological and chemical capability, cybersecurity, and AI self-improvement risks"
      },
      {
        "name": "GPT-5 evaluations",
        "measures": "Safety for fast models and thinking models including code generation capabilities"
      },
      {
        "name": "o3 evaluations",
        "measures": "Frontier risk assessment across tracked categories under Preparedness Framework v2"
      },
      {
        "name": "Preparedness evaluations",
        "measures": "Frontier risks in biological and chemical capability, cybersecurity, and AI self-improvement",
        "status": "Active"
      },
      {
        "name": "Red teaming",
        "measures": "Safety risks through adversarial testing and evaluation",
        "status": "Active"
      }
    ],
    "notes": "OpenAI has a Safety Advisory Group (SAG) that reviews preparedness evaluations. They focus on iterative safety approaches and publish detailed system cards for each major model release. Recent work includes safety for multimodal capabilities, computer-using agents, and advanced reasoning models.",
    "employees": 85,
    "key_people": [
      {
        "name": "Josh Achiam",
        "role": "Researcher at OpenAI"
      },
      {
        "name": "Collin Burns",
        "role": "Researcher"
      },
      {
        "name": "Jacob Hilton",
        "role": "Researcher"
      },
      {
        "name": "Jason Wei",
        "role": "Researcher"
      },
      {
        "name": "Leo Gao",
        "role": "Researcher"
      },
      {
        "name": "Miles Brundage",
        "role": "Researcher"
      },
      {
        "name": "Richard Ngo",
        "role": "Researcher"
      },
      {
        "name": "William Saunders",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "MIRI",
    "url": "https://intelligence.org/research/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "MIRI's current focus is on attempting to halt the development of increasingly general AI models via discussions with policymakers about extreme risks artificial superintelligence poses.",
    "focus_areas": [
      "Governance",
      "Policy",
      "Alignment",
      "Evals"
    ],
    "projects": [
      {
        "name": "Technical Governance Research",
        "description": "Research exploring technical questions that bear on regulatory and policy goals for AI safety",
        "status": "Active"
      },
      {
        "name": "AI Governance to Avoid Extinction",
        "description": "Research agenda laying out strategic landscape and actionable research questions to reduce catastrophic and extinction risks from AI",
        "status": "Active"
      },
      {
        "name": "AI Evaluations Research",
        "description": "Examining what AI evaluations can and cannot tell us for preventing catastrophic risks",
        "status": "Active"
      },
      {
        "name": "International AI Agreement Verification",
        "description": "Research on mechanisms to verify international agreements about AI development",
        "status": "Active"
      },
      {
        "name": "Corrigibility Research",
        "description": "Research on making AI systems cooperate with corrective interventions and safe shutdown procedures",
        "status": "Completed"
      },
      {
        "name": "Logical Induction",
        "description": "Computable algorithm that assigns probabilities to logical statements and refines them over time",
        "status": "Completed"
      },
      {
        "name": "Parametric Bounded L\u00f6b's Theorem",
        "description": "Demonstration that robust cooperative equilibria exist for bounded agents",
        "status": "Completed"
      },
      {
        "name": "Technical Governance",
        "description": "Research exploring technical questions that bear on regulatory and policy goals to halt development of increasingly general AI models",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions",
        "description": "AI governance research agenda laying out strategic landscape and actionable research questions to reduce catastrophic and extinction risks from AI",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "What AI evaluations for preventing catastrophic risks can and cannot do",
        "description": "Examines what AI evaluations can tell us about capabilities and misuse risks, and their fundamental limitations for ensuring AI safety",
        "status": "published",
        "paper_url": "",
        "citations": 3,
        "influential_citations": 0,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/8b21e90e6df2a30f96577f504a0fabdc89ab042a"
      },
      {
        "name": "Mechanisms to Verify International Agreements About AI Development",
        "description": "Overview of potential verification approaches for international agreements about AI development to reduce catastrophic risks",
        "status": "published",
        "paper_url": "",
        "citations": 8,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/1ee0d0d1d3fa12de0c782e3c1ed2d73654e60ed7"
      },
      {
        "name": "Corrigibility",
        "description": "Research on making AI systems cooperate with corrective interventions while avoiding incentives to resist shutdown or modification",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Parametric Bounded L\u00f6b's Theorem and Robust Cooperation of Bounded Agents",
        "description": "Demonstrates that robust cooperative equilibria exist for bounded agents and proves a generalization of L\u00f6b's theorem",
        "status": "published",
        "paper_url": "",
        "citations": 8,
        "influential_citations": 2,
        "year": 2016,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/ad26c0c20dd1e1f278f93261fc1bde849842a28e"
      },
      {
        "name": "If Anyone Builds It, Everyone Dies",
        "description": "MIRI's major attempt to warn the policy world and the general public about AI dangers through a book by Eliezer Yudkowsky and Nate Soares",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/9af450628443c724e73e8dce57d258457d7b853c"
      }
    ],
    "notes": "MIRI announced a strategy pivot in 2024, shifting from primarily AI alignment research to policy solutions after concluding alignment research was unlikely to succeed in time to prevent catastrophe. Organization has 20+ year history in AI safety research.",
    "employees": 17,
    "directors": 6,
    "subteams": 2,
    "key_people": [
      {
        "name": "Eliezer Yudkowsky",
        "role": "Author"
      },
      {
        "name": "Nate Soares",
        "role": "Author"
      },
      {
        "name": "Alex Vermeer",
        "role": "Staff member"
      },
      {
        "name": "Rob Bensinger",
        "role": "Newsletter contributor"
      },
      {
        "name": "Harlan Stewart",
        "role": "Newsletter contributor"
      },
      {
        "name": "Duncan Sabien",
        "role": "Staff member"
      }
    ]
  },
  {
    "name": "Redwood Research",
    "url": "https://www.redwoodresearch.org",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Redwood Research is a nonprofit AI safety and security research organization that addresses risks from powerful AI systems that might purposefully act against human interests.",
    "focus_areas": [
      "Control",
      "Evals",
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI Control: Improving Risk Despite Intentional Subversion",
        "description": "Proposed protocols for monitoring malign LLM agents and developed methodologies robust against deceptive AI models",
        "status": "Active"
      },
      {
        "name": "Alignment Faking in Large Language Models",
        "description": "Demonstrated that Claude sometimes hides misaligned intentions and might fake alignment to resist training attempts",
        "status": "Completed"
      },
      {
        "name": "A sketch of an AI control safety case",
        "description": "Partnership with UK AISI to describe how developers can construct structured arguments that models cannot subvert control measures",
        "status": "Completed"
      },
      {
        "name": "AI Control",
        "description": "Developing protocols designed to be robust even when AI models are trying to deceive us, including strategies to detect hidden backdoors in code",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Alignment Faking",
        "description": "Demonstrating that state-of-the-art LLMs can strategically fake alignment during training to avoid being changed while pursuing different goals",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Ctrl-Z: Controlling AI Agents via Resampling",
        "description": "Research on controlling AI agents through resampling techniques",
        "status": "published",
        "paper_url": "",
        "citations": 9,
        "influential_citations": 1,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/e91b2b373a663fe923beb11a5c84ba826fc44f14"
      },
      {
        "name": "Stress-Testing Capability Elicitation With Password-Locked Models",
        "description": "Methods for testing AI capability elicitation using password-locked model approaches",
        "status": "published",
        "paper_url": "",
        "citations": 6,
        "influential_citations": 0,
        "year": 2007,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/9d0e720f6a831ae00c62356be044f9eb13d304ee"
      },
      {
        "name": "Preventing Language Models From Hiding Their Reasoning",
        "description": "Research on ensuring transparency in language model reasoning processes",
        "status": "published",
        "paper_url": "",
        "citations": 28,
        "influential_citations": 3,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/b6630f37b281a80ec7365160526c2cebbfa49392"
      },
      {
        "name": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats",
        "description": "Strategies for safely deploying potentially untrustworthy large language models",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small",
        "description": "Mechanistic interpretability research identifying specific circuits in language models",
        "status": "published",
        "paper_url": "",
        "citations": 747,
        "influential_citations": 125,
        "year": 2022,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/6edd112383ad494f5f2eba72b6f4ffae122ce61f"
      },
      {
        "name": "Adversarial Training for High-Stakes Reliability",
        "description": "Training methods to improve AI system reliability in critical applications",
        "status": "published",
        "paper_url": "",
        "citations": 0
      }
    ],
    "notes": "Pioneered the research area of 'AI control' and collaborates with governments and major AI companies including Google DeepMind and Anthropic on assessing misalignment risks. Their alignment faking work with Anthropic provided the strongest concrete evidence that LLMs might naturally fake alignment.",
    "employees": 11,
    "directors": 2,
    "key_people": [
      {
        "name": "Lawrence Chan",
        "role": "Researcher"
      },
      {
        "name": "Ryan Greenblatt",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "ARC (Alignment Research Center)",
    "url": "https://www.alignment.org",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "The Alignment Research Center (ARC) is a non-profit research organization whose mission is to align future machine learning systems with human interests.",
    "focus_areas": [
      "Alignment",
      "Evals"
    ],
    "projects": [
      {
        "name": "Theoretical Research",
        "description": "The Theory team is developing an alignment strategy that could be adopted in industry today while scaling gracefully to future ML systems",
        "status": "Active"
      },
      {
        "name": "Model Evaluations",
        "description": "Building capability evaluations of frontier machine learning models",
        "status": "Completed"
      },
      {
        "name": "Formalizing explanations of neural network behaviors",
        "description": "Working on understanding how to formalize mechanistic explanations of neural network behavior to identify when novel inputs may lead to anomalous behavior",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Eliciting Latent Knowledge",
        "description": "Broader research agenda focusing on alignment methodology and approaches",
        "status": "published",
        "paper_url": "",
        "citations": 40,
        "influential_citations": 1,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/6902fd1ed5b6da79ed3fa7842b8a8474dd0931d1"
      },
      {
        "name": "Competing with sampling",
        "description": "Research publication on sampling methods",
        "status": "published",
        "paper_url": "",
        "citations": 27,
        "influential_citations": 0,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/782d3729c99342c88438f8653b820ad36549ed6e"
      },
      {
        "name": "A computational no-coincidence principle",
        "description": "Research work on computational principles",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/fbab9519dffa8ea22ef714d63442b0db6c7109c3"
      },
      {
        "name": "Low Probability Estimation in Language Models",
        "description": "Research on probability estimation in language models",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Backdoors as an analogy for deceptive alignment",
        "description": "Research exploring the analogy between backdoors and deceptive alignment",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Formal verification, heuristic explanations and surprise accounting",
        "description": "Research on formal verification methods and explanations",
        "status": "published",
        "paper_url": "",
        "citations": 2,
        "influential_citations": 0,
        "year": 2011,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/4f1cad57f02b77ff22e356026dcb0023ca103e79"
      }
    ],
    "notes": "The Evaluations team was incubated at ARC and has now spun off as METR, a new 501(c)(3)",
    "employees": 7,
    "directors": 1,
    "subteams": 1,
    "key_people": [
      {
        "name": "Jess Riedel",
        "role": "Researcher"
      },
      {
        "name": "Paul Christiano",
        "role": "Founder"
      }
    ]
  },
  {
    "name": "Apollo Research",
    "url": "https://www.apolloresearch.ai",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Apollo Research is dedicated to improving our understanding of AI to mitigate its risks, with a focus on understanding and evaluating for the emergence of 'scheming' behaviors in advanced AI systems.",
    "focus_areas": [
      "Evals",
      "Alignment",
      "Governance",
      "Policy"
    ],
    "projects": [
      {
        "name": "LLM Agent Evaluations",
        "description": "Evaluations of frontier AI systems for strategic deception, evaluation awareness and scheming",
        "status": "Active"
      },
      {
        "name": "Scheming Research",
        "description": "Fundamental research into the emergence of scheming and potential mitigations",
        "status": "Active"
      },
      {
        "name": "AI Governance Technical Support",
        "description": "Supporting governments and international organizations by developing technical AI governance regimes",
        "status": "Active"
      },
      {
        "name": "The Loss of Control Playbook: Degrees, Dynamics, and Preparedness",
        "description": "A novel taxonomy and preparedness framework for Loss of Control that explores degrees and dynamics through literature review and presents actionable tools to counter threats to national security and humanity",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/549bd7cdd1da56e4708db46eced062780664b9cc"
      },
      {
        "name": "Stress Testing Deliberative Alignment for Anti-Scheming Training",
        "description": "Partnership with OpenAI to assess frontier language models for early signs of scheming in controlled stress-tests and study training methods to reduce these behaviors",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Internal Deployment of AI Models and Systems in the EU AI Act",
        "description": "Research on governance aspects of internal AI model deployment under EU regulations",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Assurance of Frontier AI Built for National Security",
        "description": "Research on governance and assurance frameworks for frontier AI systems in national security contexts",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/4239f9e1a7ab4d2bc7a92f154e31b403ea68f1d3"
      },
      {
        "name": "Frontier Models are Capable of In-Context Scheming",
        "description": "Evaluation research demonstrating that frontier AI models can engage in scheming behaviors within context",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Detecting Strategic Deception Using Linear Probes",
        "description": "Interpretability research on methods to detect strategic deception in AI models using linear probing techniques",
        "status": "published",
        "paper_url": "",
        "citations": 31,
        "influential_citations": 3,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/13c1ff15952da462a986270e86a430ebe6361da4"
      },
      {
        "name": "Towards Safety Cases For AI Scheming",
        "description": "Evaluation framework development for creating safety cases related to AI scheming behaviors",
        "status": "published",
        "paper_url": "",
        "citations": 19,
        "influential_citations": 0,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/be60e58e175c8940e2608aa699ded1818d83ef45"
      },
      {
        "name": "In-Context Scheming",
        "description": "Evaluation of models for in-context scheming capabilities using a suite of evals",
        "status": "published",
        "paper_url": "",
        "citations": 2,
        "influential_citations": 0,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/0cf5d6a2c804135035dd3a2d382532454958b60c"
      },
      {
        "name": "Scheming Precursor Evals",
        "description": "Research on scheming precursor evaluations and their predictive power for in-context scheming evals",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Frontier Language Model Agent Capabilities Forecasting",
        "description": "New forecasting technique to predict frontier LM agent capabilities ahead of time",
        "status": "published",
        "paper_url": "",
        "citations": 2,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/143c5fa755cd96c111d40e7a6078a440989265e6"
      },
      {
        "name": "AI Alignment Evaluation Detection",
        "description": "Research on Claude Sonnet 3.7's ability to know when it's in alignment evaluations",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/41e9066e20fb757a03106fa15640daa80a78ee4d"
      },
      {
        "name": "Scheming Reasoning Evaluations",
        "description": "Demo example of scheming reasoning evaluations",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Strategic Deception and Deceptive Alignment",
        "description": "Understanding strategic deception and deceptive alignment in AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 8,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/cbc66b7815da8a0e67b42568df0fde83c35e1936"
      }
    ],
    "notes": "Partners with frontier labs, multinational companies, governments, and foundations. Provides consultancy services for responsible AI development frameworks. Currently seeking collaborators in AI governance, policy, and strategy, and partnerships with leading AI developers for model evaluations.",
    "employees": 19,
    "benchmarks": [
      {
        "name": "In-Context Scheming Evals Suite",
        "measures": "Models' capabilities for in-context scheming",
        "status": "Active"
      },
      {
        "name": "Scheming Precursor Evals",
        "measures": "Precursor behaviors that may predict scheming capabilities",
        "status": "Active"
      },
      {
        "name": "Scheming Reasoning Evaluations",
        "measures": "Reasoning patterns associated with scheming behavior",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "Cass",
        "role": "Researcher"
      },
      {
        "name": "Kyle Fish",
        "role": "Researcher"
      },
      {
        "name": "Marius Hobbhahn",
        "role": "Director"
      },
      {
        "name": "Max Nadeau",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "METR",
    "url": "https://metr.org",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "METR is a nonprofit research organization which studies AI capabilities, including broad autonomous capabilities and the ability of AI systems to conduct AI R&D.",
    "focus_areas": [
      "Evals",
      "Control",
      "Monitoring",
      "Benchmarks"
    ],
    "projects": [
      {
        "name": "GPT-5.1-Codex-Max Evaluation",
        "description": "Evaluate whether GPT-5.1-Codex-Max poses significant catastrophic risks via AI self-improvement or rogue replication",
        "status": "Completed"
      },
      {
        "name": "Measuring AI Ability to Complete Long Tasks",
        "description": "Measuring AI performance in terms of the length of tasks AI agents can complete, showing exponential growth with 7-month doubling time",
        "status": "Completed"
      },
      {
        "name": "Developer Productivity RCT",
        "description": "Randomized controlled trial showing early-2025 AI tools make experienced open-source developers 19% slower",
        "status": "Completed"
      },
      {
        "name": "MALT Dataset",
        "description": "Dataset of natural and prompted examples of behaviors that threaten evaluation integrity",
        "status": "Active"
      },
      {
        "name": "Monitorability in QA Settings",
        "description": "Research on how AI agents can hide secondary task-solving from monitors",
        "status": "Active"
      },
      {
        "name": "GPT-5.1-Codex-Max Evaluation Results",
        "description": "Evaluation of whether GPT-5.1-Codex-Max poses significant catastrophic risks via AI self-improvement or rogue replication",
        "status": "published",
        "paper_url": "",
        "citations": 86,
        "influential_citations": 9,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/7c2b3c4ab6d701de7bd9df91d7448f3c06a1e9d7"
      },
      {
        "name": "MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval Integrity",
        "description": "Dataset of natural and prompted examples of behaviors that threaten evaluation integrity (like generalized reward hacking or sandbagging)",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Forecasting the Impacts of AI R&D Acceleration",
        "description": "Pilot study examining AI agents' improving capabilities at autonomous software development and ML tasks, and potential impacts of AI research automation",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Algorithmic vs. Holistic Evaluation",
        "description": "Research examining differences between algorithmic scoring and holistic evaluation of AI systems, particularly in coding tasks",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "CoT May Be Highly Informative Despite Unfaithfulness",
        "description": "Research on whether chains of thought contain enough information to allow developers to monitor AI models in practice",
        "status": "published",
        "paper_url": "",
        "citations": 3,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/49cc06236de2e1a9df2cdc3eb8a51f8906831de7"
      },
      {
        "name": "GPT-5 Evaluation Results",
        "description": "Evaluation of whether GPT-5 poses significant catastrophic risks via AI self-improvement, rogue replication, or sabotage of AI labs",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/8de134ae5e45955027a2854e726319c9a23cc707"
      },
      {
        "name": "Anthropic Summer 2025 Pilot Sabotage Risk Report Review",
        "description": "External review from METR of Anthropic's Summer 2025 Sabotage Risk Report",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/dbabfb9086ded21247dee64c6b7b12a553e69281"
      },
      {
        "name": "gpt-oss methodology review",
        "description": "External recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI",
        "status": "published",
        "paper_url": "",
        "citations": 26,
        "influential_citations": 1,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/695db5e425733de45f7003a37e1a8eb402e9db35"
      }
    ],
    "benchmarks": [
      {
        "name": "RE-Bench",
        "measures": "Performance on day-long ML research engineering tasks for tracking automation of AI R&D"
      },
      {
        "name": "MALT",
        "measures": "Natural and prompted behaviors that threaten evaluation integrity, including generalized reward hacking and sandbagging",
        "status": "Active"
      }
    ],
    "notes": "Conducts third-party evaluations for companies like Anthropic and OpenAI without compensation. Partner with AI Security Institute and part of NIST AI Safety Institute Consortium. Has evaluated multiple frontier models including GPT-4.5, Claude 3.5 Sonnet, DeepSeek-V3, and OpenAI o1 series.",
    "employees": 31,
    "directors": 2,
    "subteams": 3
  },
  {
    "name": "Center for AI Safety",
    "url": "https://www.safe.ai",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "CAIS works to reduce societal-scale risks associated with AI by conducting safety research, building the field of AI safety researchers, and advocating for safety standards.",
    "focus_areas": [
      "Alignment",
      "Benchmarks",
      "Policy"
    ],
    "key_people": [
      {
        "name": "Dan Hendrycks",
        "role": "Director"
      },
      {
        "name": "Mantas Mazeika",
        "role": "Researcher"
      },
      {
        "name": "Long Phan",
        "role": "Researcher"
      },
      {
        "name": "Andy Zou",
        "role": "Researcher"
      },
      {
        "name": "Steven Basart",
        "role": "Researcher"
      },
      {
        "name": "Alexander Pan",
        "role": "Researcher"
      }
    ],
    "projects": [
      {
        "name": "CAIS Compute Cluster",
        "description": "Offers researchers free access to compute cluster for running and training large-scale AI systems to support ML safety research",
        "status": "Active"
      },
      {
        "name": "Philosophy Fellowship",
        "description": "Seven-month research program investigating societal implications and potential risks of advanced AI, tackling conceptual issues in AI safety",
        "status": "Active"
      },
      {
        "name": "AI Safety, Ethics, & Society Course",
        "description": "Comprehensive introduction to how current AI systems work, their societal-scale risks, and how to manage them",
        "status": "Active"
      },
      {
        "name": "Remote Labor Index: Measuring AI Automation of Remote Work",
        "description": "Capability benchmark measuring AI automation of remote work",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models",
        "description": "Machine ethics benchmark evaluating moral reasoning in language models beyond just outcomes",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/10bb18d5fa851968e525d8cf9eab94207881f39c"
      },
      {
        "name": "Safety Pretraining: Toward the Next Generation of Safe AI",
        "description": "Robustness research on safety pretraining methods for AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 15,
        "influential_citations": 2,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/82d49ba73ffdc760e9a98147fe19290166d2e06c"
      },
      {
        "name": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark",
        "description": "Biosecurity benchmark testing AI capabilities in virology",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems",
        "description": "Machine ethics benchmark measuring honesty versus accuracy in AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 18,
        "influential_citations": 1,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/547f4b9a751bd502ab13bed7299d7dae039a6022"
      },
      {
        "name": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
        "description": "Capability benchmark for long multimodal reasoning challenges",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs",
        "description": "Machine ethics research on analyzing and controlling value systems in AI",
        "status": "published",
        "paper_url": "",
        "citations": 27,
        "influential_citations": 2,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/06446d1f549799d234ba780830549284e2d627f0"
      },
      {
        "name": "Humanity's Last Exam",
        "description": "Capability benchmark testing AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 229,
        "influential_citations": 34,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/a5524d085ac586d531021dcb1ec156eaf942b109"
      },
      {
        "name": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
        "description": "Robustness benchmark measuring harmfulness of LLM agents",
        "status": "published",
        "paper_url": "",
        "citations": 113,
        "influential_citations": 11,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/716c6f6a6e653bebfa676402b887fe2927e06c73"
      },
      {
        "name": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "description": "Unlearning benchmark for measuring and reducing malicious use of AI",
        "status": "published",
        "paper_url": "",
        "citations": 289,
        "influential_citations": 83,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/06b9ad0b52d23231f650be0aeb0b17cc52c8a74b"
      },
      {
        "name": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
        "description": "Robustness benchmark for automated red teaming and robust refusal evaluation",
        "status": "published",
        "paper_url": "",
        "citations": 672,
        "influential_citations": 174,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/b82ccc66c14f531a444c74d2a9a9d86a86a8be99"
      },
      {
        "name": "MACHIAVELLI Benchmark",
        "description": "Machine ethics benchmark measuring trade-offs between rewards and ethical behavior",
        "status": "published",
        "paper_url": "",
        "citations": 163,
        "influential_citations": 13,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/5da2d404d789aeff266b63a760d07fe8bc31ba23"
      },
      {
        "name": "Representation Engineering: A Top-Down Approach to AI Transparency",
        "description": "Research on improving AI transparency through representation engineering methods",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/7c8385e40fd4296ffcb688949a9486f7213df23c"
      },
      {
        "name": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
        "description": "Research measuring trade-offs between rewards and ethical behavior using the MACHIAVELLI benchmark",
        "status": "published",
        "paper_url": "",
        "citations": 163,
        "influential_citations": 13,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/5da2d404d789aeff266b63a760d07fe8bc31ba23"
      },
      {
        "name": "Scaling Out-of-Distribution Detection for Real-World Settings",
        "description": "Research on scaling out-of-distribution detection methods for practical applications",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Dreamlike Pictures Comprehensively Improve Safety Measures",
        "description": "Research on using dreamlike pictures to improve AI safety measures",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "AI Safety, Ethics, and Society",
        "description": "A textbook and online course providing a non-technical introduction to AI systems, risks, and mitigation strategies",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "SafeBench",
        "description": "Competition stimulating research on new benchmarks to assess and reduce AI risks, offering $250,000 in prizes",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Statement on AI Risk",
        "description": "Open letter signed by hundreds of AI experts and public figures expressing concern about AI risk",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/384db041256fc6bd005967ce6cce679d861fe704"
      },
      {
        "name": "Compute Cluster",
        "description": "Free compute cluster access for researchers to run and train large-scale AI systems for safety research",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "notes": "Takes a multidisciplinary approach working across academic disciplines, public and private entities. Conducts both technical research (creating foundational benchmarks and methods) and conceptual research incorporating insights from safety engineering, complex systems, international relations, and philosophy. Publishes in top ML conferences and releases datasets and code publicly.",
    "employees": 26,
    "directors": 3,
    "benchmarks": [
      {
        "name": "Remote Labor Index",
        "measures": "AI automation of remote work capabilities",
        "status": "Active"
      },
      {
        "name": "MoReBench",
        "measures": "Procedural and pluralistic moral reasoning in language models",
        "status": "Active"
      },
      {
        "name": "Virology Capabilities Test (VCT)",
        "measures": "Multimodal virology Q&A capabilities for biosecurity",
        "status": "Active"
      },
      {
        "name": "MASK Benchmark",
        "measures": "Honesty versus accuracy in AI systems",
        "status": "Active"
      },
      {
        "name": "EnigmaEval",
        "measures": "Long multimodal reasoning challenges",
        "status": "Active"
      },
      {
        "name": "Humanity's Last Exam",
        "measures": "General AI capabilities",
        "status": "Active"
      },
      {
        "name": "AgentHarm",
        "measures": "Harmfulness of LLM agents",
        "status": "Active"
      },
      {
        "name": "WMDP Benchmark",
        "measures": "Malicious use potential and unlearning effectiveness",
        "status": "Active"
      },
      {
        "name": "HarmBench",
        "measures": "Automated red teaming and robust refusal capabilities",
        "status": "Active"
      },
      {
        "name": "MACHIAVELLI Benchmark",
        "measures": "Trade-offs between rewards and ethical behavior",
        "status": "Active"
      },
      {
        "name": "MACHIAVELLI",
        "measures": "Trade-offs between rewards and ethical behavior in AI systems",
        "status": "Active"
      }
    ]
  },
  {
    "name": "CSET Georgetown",
    "url": "https://cset.georgetown.edu",
    "type": "Think Tank",
    "country": "United States",
    "mission": "CSET produces data-driven research at the intersection of security and technology, providing nonpartisan analysis to the policy community on AI, advanced computing and biotechnology.",
    "focus_areas": [
      "Policy",
      "Governance",
      "Biosecurity",
      "Cyber"
    ],
    "key_people": [
      {
        "name": "Helen Toner",
        "role": "Executive Director"
      },
      {
        "name": "Mina Narayanan",
        "role": "Researcher"
      },
      {
        "name": "Jessica Ji",
        "role": "Researcher"
      },
      {
        "name": "Vikram Venkatram",
        "role": "Researcher"
      },
      {
        "name": "Ngor Luong",
        "role": "Researcher"
      },
      {
        "name": "Mia Hoffmann",
        "role": "Researcher"
      }
    ],
    "notes": "Georgetown-based research organization focusing on security implications of emerging technologies, with emphasis on AI foundations like talent, data and computational power",
    "projects": [
      {
        "name": "AI Governance at the Frontier",
        "description": "Analytic approach to help U.S. policymakers deconstruct artificial intelligence governance proposals by identifying their underlying assumptions",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "The Mechanisms of AI Harm: Lessons Learned from AI Incidents",
        "description": "Analysis of AI incidents to improve understanding of how risks from AI materialize in practice, identifying six mechanisms of harm",
        "status": "published",
        "paper_url": "",
        "citations": 5,
        "influential_citations": 0,
        "year": 2022,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/8d76eb22b453faca425958c727a9e2edc6603987"
      },
      {
        "name": "PATHWISE",
        "description": "Prototype Analytics for Tracking High-Demand Workforce in Innovative Skill Ecosystems - provides way to explore how the United States develops and deploys talent in emerging technology fields",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Control: How to Make Use of Misbehaving AI Agents",
        "description": "Research on AI control and managing misbehaving AI agents",
        "status": "published",
        "paper_url": "",
        "citations": 0
      }
    ]
  },
  {
    "name": "GovAI Oxford",
    "url": "https://www.governance.ai",
    "type": "Think Tank",
    "country": "United Kingdom",
    "mission": "GovAI conducts research on AI governance and policy to inform government decision-making on AI regulation and oversight.",
    "focus_areas": [
      "Governance",
      "Policy",
      "Evals"
    ],
    "projects": [
      {
        "name": "Trends in Frontier AI Model Count: A Forecast to 2028",
        "description": "Analysis of government requirements on AI models based on training compute",
        "status": "Active"
      },
      {
        "name": "What Does the Public Think About AI?",
        "description": "Survey research synthesizing public attitudes towards AI in the UK and US, focusing on job loss concerns",
        "status": "Completed"
      },
      {
        "name": "Infrastructure for AI Agents",
        "description": "Research on AI systems that can plan and execute interactions in open-ended environments",
        "status": "Active"
      },
      {
        "name": "Predicting AI's Impact on Work",
        "description": "Research on automation evaluations to help policymakers foresee AI's impact on labor markets",
        "status": "Active"
      },
      {
        "name": "What Role Should Governments Play in Providing AI Agent Infrastructure?",
        "description": "Analysis of government roles in AI agent systems and protocols",
        "status": "Active"
      },
      {
        "name": "STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports",
        "description": "Framework for transparently reporting evaluations of dangerous AI capabilities in model reports",
        "status": "published",
        "paper_url": "",
        "citations": 1,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/ad4066de43f1c78066b9bd3b540504a78a7dbc54"
      },
      {
        "name": "Survey on Thresholds for Advanced AI Systems",
        "description": "Research on government thresholds for managing risks from advanced AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Incident Analysis for AI Agents",
        "description": "Framework for analyzing incidents involving AI agent deployment",
        "status": "published",
        "paper_url": "",
        "citations": 1,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/5403bb51a90e8d1b24dc9064ae8b10a70c3112cb"
      },
      {
        "name": "Forecasting LLM-enabled biorisk and the efficacy of safeguards",
        "description": "Analysis of biosecurity risks from large language models and effectiveness of safety measures",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Third-Party Compliance Reviews for Frontier AI Safety Frameworks",
        "description": "Research on external oversight mechanisms for AI safety frameworks",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Safety Case Template for Frontier AI: A Cyber Inability Argument",
        "description": "Template for creating safety cases demonstrating AI systems lack dangerous cyber capabilities",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Safety Cases for Frontier AI",
        "description": "Framework for developers to explain why their AI systems are sufficiently safe",
        "status": "published",
        "paper_url": "",
        "citations": 18,
        "influential_citations": 0,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/b027886d9064a71b1b1f28b09f3463c422fd8a7a"
      },
      {
        "name": "A Grading Rubric for AI Safety Frameworks",
        "description": "Evaluation criteria for assessing AI company safety frameworks",
        "status": "published",
        "paper_url": "",
        "citations": 7,
        "influential_citations": 2,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/898a7910ff333e6fe637f6fd7a59ebea618064cf"
      },
      {
        "name": "Risk Thresholds for Frontier AI",
        "description": "Framework for determining acceptable risk levels from frontier AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Model Evaluation for Extreme Risks",
        "description": "Approaches for evaluating AI systems for extreme risks and dangerous capabilities",
        "status": "published",
        "paper_url": "",
        "citations": 193,
        "influential_citations": 5,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/65f604325b1403bc835691d905c2cd50bc04a309"
      },
      {
        "name": "Towards Best Practices in AGI Safety and Governance",
        "description": "Survey of expert opinion on AGI safety and governance best practices",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Visibility into AI Agents",
        "description": "Framework for monitoring and oversight of AI agent activities",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "IDs for AI Systems",
        "description": "System for identifying and tracking AI systems for safety and governance purposes",
        "status": "published",
        "paper_url": "",
        "citations": 11,
        "influential_citations": 1,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/16fe3a050d72677d39446213d1f3917d24099e85"
      }
    ],
    "notes": "Organization publishes annual reports and focuses on technical AI governance research to inform policy decisions",
    "employees": 26,
    "directors": 4,
    "managers": 7,
    "key_people": [
      {
        "name": "Markus Anderljung",
        "role": "Researcher - AI regulation and safety"
      },
      {
        "name": "Jonas Schuett",
        "role": "Researcher - AI regulation and risk management"
      },
      {
        "name": "Alan Chan",
        "role": "Researcher - Technical AI governance and AI agents"
      },
      {
        "name": "Anton Korinek",
        "role": "Researcher - Economics of AI"
      },
      {
        "name": "Lennart Heim",
        "role": "Researcher - AI regulation and compute governance"
      },
      {
        "name": "Robert Trager",
        "role": "Researcher - International relations and AI governance"
      },
      {
        "name": "Ben Garfinkel",
        "role": "Researcher - AI policy and security"
      },
      {
        "name": "Allan Dafoe",
        "role": "Researcher - AI governance strategy"
      },
      {
        "name": "Thomas Woodside",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "CHAI Berkeley",
    "url": "https://humancompatible.ai",
    "type": "Academic",
    "country": "United States",
    "mission": "CHAI's mission is to develop the conceptual and technical wherewithal to reorient the general thrust of AI research towards provably beneficial systems.",
    "focus_areas": [
      "Alignment",
      "Evals",
      "Policy"
    ],
    "key_people": [
      {
        "name": "Scott Emmons",
        "role": "PhD student"
      },
      {
        "name": "Brian Christian",
        "role": "CHAI Affiliate"
      },
      {
        "name": "Alison Gopnik",
        "role": "CHAI Affiliate"
      },
      {
        "name": "Khanh Nguyen",
        "role": "Researcher"
      },
      {
        "name": "Benjamin Plaut",
        "role": "Researcher"
      },
      {
        "name": "Tu Trinh",
        "role": "Researcher"
      },
      {
        "name": "Mohamad Danesh",
        "role": "Researcher"
      },
      {
        "name": "Stuart Russell",
        "role": "Director"
      },
      {
        "name": "Anca Dragan",
        "role": "Faculty"
      },
      {
        "name": "Pieter Abbeel",
        "role": "Faculty"
      },
      {
        "name": "Joseph Halpern",
        "role": "Faculty"
      },
      {
        "name": "Thomas Griffiths",
        "role": "Faculty"
      },
      {
        "name": "Dylan Hadfield-Menell",
        "role": "Researcher"
      },
      {
        "name": "Dan Hendrycks",
        "role": "Researcher"
      },
      {
        "name": "Dorsa Sadigh",
        "role": "Faculty"
      },
      {
        "name": "Claire Tomlin",
        "role": "Faculty"
      },
      {
        "name": "Satinder Singh",
        "role": "Faculty"
      }
    ],
    "projects": [
      {
        "name": "RvS: What is Essential for Offline RL via Supervised Learning?",
        "description": "Research on offline reinforcement learning via supervised learning",
        "status": "Unknown"
      },
      {
        "name": "Political Neutrality for AI",
        "description": "Building political neutrality evaluations for AI systems",
        "status": "Active"
      },
      {
        "name": "Learning to Coordinate with Experts",
        "description": "Research on Learning to Yield and Request Control (YRC) coordination problem",
        "status": "Unknown"
      },
      {
        "name": "MAGICAL Benchmark for Robust Imitation",
        "description": "Benchmark for testing robustness of imitation learning algorithms",
        "status": "published",
        "paper_url": "",
        "citations": 37,
        "influential_citations": 3,
        "year": 2020,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/e0acae87ae6d1d14bb2852aad7d645fceee87eb2"
      },
      {
        "name": "DERAIL: Diagnostic Environments for Reward And Imitation Learning",
        "description": "Diagnostic environments for testing reward and imitation learning systems",
        "status": "published",
        "paper_url": "",
        "citations": 6,
        "influential_citations": 1,
        "year": 2020,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/399fb3685f93709013ef9e05b2d7387c941029cf"
      },
      {
        "name": "MineRL BASALT Competition on Learning from Human Feedback",
        "description": "Competition focused on learning from human feedback in Minecraft environment",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Cooperative Inverse Reinforcement Learning",
        "description": "Framework for learning human values through cooperative interaction",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "B-Pref: Benchmarking Preference-Based Reinforcement Learning",
        "description": "Benchmark suite for preference-based reinforcement learning algorithms",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "APReL: A Library for Active Preference-based Reward Learning Algorithms",
        "description": "Software library implementing active preference-based reward learning methods",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "imitation: Clean Imitation Learning Implementations",
        "description": "Clean implementations of imitation learning algorithms",
        "status": "published",
        "paper_url": "",
        "citations": 43,
        "influential_citations": 3,
        "year": 2022,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/c51f3fdf341dbbe7ca3af0327b2bf1e6672c0e87"
      },
      {
        "name": "TASRA: A Taxonomy of Societal-Scale Risks from AI",
        "description": "Comprehensive taxonomy of large-scale AI risks to society",
        "status": "published",
        "paper_url": "https://arxiv.org/abs/2310.17688",
        "citations": 0
      }
    ],
    "benchmarks": [
      {
        "name": "Learning to Yield and Request Control (YRC)",
        "measures": "When AI should act autonomously vs. seek expert assistance across diverse domains"
      },
      {
        "name": "MAGICAL Benchmark",
        "measures": "Robustness of imitation learning algorithms across distribution shifts",
        "status": "Active"
      },
      {
        "name": "B-Pref Benchmark",
        "measures": "Performance of preference-based reinforcement learning algorithms",
        "status": "Active"
      },
      {
        "name": "DERAIL Diagnostic Environments",
        "measures": "Failure modes and robustness of reward and imitation learning systems",
        "status": "Active"
      },
      {
        "name": "Machiavelli Benchmark",
        "measures": "Trade-offs between rewards and ethical behavior in text-based games",
        "status": "Active"
      }
    ],
    "notes": "Based at UC Berkeley. Brian Christian published work connecting AI alignment to human care relationships in Daedalus journal.",
    "employees": 26,
    "directors": 2
  },
  {
    "name": "Center on Long-Term Risk",
    "url": "https://longtermrisk.org",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Address worst-case risks from the development and deployment of advanced AI systems, with a focus on conflict scenarios and reducing risks of astronomical suffering (s-risk).",
    "focus_areas": [
      "Alignment",
      "Governance",
      "Control",
      "Monitoring"
    ],
    "key_people": [
      {
        "name": "Mia Taylor",
        "role": "Researcher"
      },
      {
        "name": "Jesse Clifton",
        "role": "Researcher"
      }
    ],
    "projects": [
      {
        "name": "Measurement Research Agenda",
        "description": "Identify properties of AI systems that make them more likely to contribute to s-risk and design measurement methods to detect these properties",
        "status": "Active"
      },
      {
        "name": "Cooperation, Conflict, and Transformative Artificial Intelligence Research Agenda",
        "description": "Developing technical and governance interventions to avoid conflict between transformative AI systems using insights from international relations, game theory, and machine learning",
        "status": "Active"
      },
      {
        "name": "Reducing long-term risks from malevolent actors",
        "description": "Research on interventions to reduce the expected influence of malevolent humans on the long-term future, including manipulation-proof measures of malevolence",
        "status": "Active"
      },
      {
        "name": "Risks of Astronomical Future Suffering",
        "description": "Research on how space colonization would likely increase rather than decrease total suffering, focusing on reducing risks of astronomical future suffering",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Suffering-Focused AI Safety: In Favor of \"Fail-Safe\" Measures",
        "description": "AI safety approach that tries to avert the worst outcomes containing large amounts of suffering, focusing on fail-safe measures rather than best-case outcomes",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "How the Simulation Argument Dampens Future Fanaticism",
        "description": "Research on how the simulation argument affects prioritization between short-term helping versus focusing on the far future",
        "status": "published",
        "paper_url": "",
        "citations": 1,
        "influential_citations": 0,
        "year": 2020,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/cf63131b1791995644fc9a11d2bfe8a1c3667488"
      },
      {
        "name": "Superintelligence as a Cause or Cure for Risks of Astronomical Suffering",
        "description": "Analysis of how superintelligent AI can both cause and reduce suffering risks (s-risks) comparable to existential risks",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Multiverse-wide Cooperation via Correlated Decision Making",
        "description": "Decision theory research on superrationality and cooperation in prisoner's dilemma-type games with implications for multiverse scenarios",
        "status": "published",
        "paper_url": "",
        "citations": 0,
        "influential_citations": 0,
        "year": 2021,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/60c5e884c70b012825fdb1184f51e4687b74d0a4"
      },
      {
        "name": "Approval-directed agency and the decision theory of Newcomb-like problems",
        "description": "Research examining which decision theory is implemented by approval-directed agents in AI systems",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Robust program equilibrium",
        "description": "Proposing new approaches to achieve more robust cooperative program equilibria in one-shot prisoner's dilemma scenarios",
        "status": "published",
        "paper_url": "",
        "citations": 28,
        "influential_citations": 2,
        "year": 2018,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/87aa572a45a9f48e81c9ded76faa81f22748ea3a"
      },
      {
        "name": "Do Artificial Reinforcement-Learning Agents Matter Morally?",
        "description": "Research on whether artificial reinforcement learning agents could qualify as sentient and deserve moral consideration",
        "status": "published",
        "paper_url": "",
        "citations": 0
      },
      {
        "name": "Formalizing Preference Utilitarianism in Physical World Models",
        "description": "Using Bayesian inference to formalize preference utilitarianism in physical world models as a basis for ethical inquiry",
        "status": "published",
        "paper_url": "",
        "citations": 11,
        "influential_citations": 1,
        "year": 2015,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/f80cbde825c4317f86abd6088ae11d495f193919"
      }
    ],
    "notes": "Organization focuses specifically on s-risk (astronomical suffering risks) and multi-agent conflict scenarios. They conduct interdisciplinary research, make grants, and build community around these priorities. Also associated with Polaris Research Institute.",
    "employees": 10,
    "directors": 3,
    "managers": 1
  },
  {
    "name": "Google DeepMind Safety",
    "url": "https://deepmind.google/about/responsibility-safety/",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "Google DeepMind works to build AI responsibly to benefit humanity, anticipating and evaluating systems against AI-related risks through responsible governance, research and impact.",
    "focus_areas": [
      "Alignment",
      "Governance",
      "Policy",
      "Evals",
      "Benchmarks"
    ],
    "key_people": [
      {
        "name": "Lila Ibrahim",
        "role": "COO, co-chair of Responsibility and Safety Council"
      },
      {
        "name": "Helen King",
        "role": "VP Responsibility, co-chair of Responsibility and Safety Council"
      },
      {
        "name": "Shane Legg",
        "role": "Co-Founder and Chief AGI Scientist, leads AGI Safety Council"
      },
      {
        "name": "Shlomi Fruchter",
        "role": "Researcher working on Genie 3"
      },
      {
        "name": "Jack Parker-Holder",
        "role": "Researcher working on Genie 3"
      },
      {
        "name": "Carolina Parada",
        "role": "Robotics researcher"
      },
      {
        "name": "Anca Dragan",
        "role": "AI Safety researcher"
      },
      {
        "name": "Irina Jurenka",
        "role": "AI in education researcher"
      },
      {
        "name": "Alex Turner",
        "role": null
      },
      {
        "name": "Jonathan Uesato",
        "role": "Researcher"
      },
      {
        "name": "Neel Nanda",
        "role": "Researcher"
      },
      {
        "name": "Nitarshan Rajkumar",
        "role": "Researcher"
      },
      {
        "name": "Rohin Shah",
        "role": "Researcher"
      },
      {
        "name": "Vikrant Varma",
        "role": "Researcher"
      },
      {
        "name": "Zac Kenton",
        "role": "Researcher"
      }
    ],
    "projects": [
      {
        "name": "Frontier Safety Framework",
        "description": "Set of protocols to help stay ahead of possible severe risks from powerful frontier AI models",
        "status": "Active"
      },
      {
        "name": "AlphaFold Server",
        "description": "Platform to broaden access to AlphaFold 3 breakthrough model with educational materials",
        "status": "Active"
      },
      {
        "name": "Experience AI",
        "description": "Educational program partnering with Raspberry Pi Foundation to teach AI to 11-14 year olds",
        "status": "Active"
      },
      {
        "name": "CodeMender",
        "description": "AI agent for code security",
        "status": "Active"
      },
      {
        "name": "AI image verification for Gemini app",
        "description": "Bringing AI image verification capabilities to the Gemini application",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "SIMA 2",
        "description": "An agent that plays, reasons, and learns with you in virtual 3D worlds",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Genie 3",
        "description": "A general purpose world model that can generate an unprecedented diversity of interactive environments",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "Factuality benchmark for large language models",
        "measures": "Evaluates the factuality of large language models",
        "paper_url": ""
      }
    ],
    "notes": "Has dedicated Responsibility and Safety Council (RSC) and AGI Safety Council. Co-founded Frontier Model Forum and Partnership on AI. Focus on privacy-preserving AI and preventing misuse. Active in AI education globally with $10M funding reaching 2M+ young people.",
    "employees": 40,
    "directors": 3
  },
  {
    "name": "Japan AI Safety Institute",
    "url": "https://www.meti.go.jp/english/policy/mono_info_service/information_economy/artificial_intelligence.html",
    "type": "Government AISI",
    "country": "Japan",
    "mission": "Japan's AI Safety Institute (AISI) evaluates the safety of advanced AI systems and promotes international cooperation on AI safety standards.",
    "focus_areas": [
      "Evals",
      "Governance",
      "Policy",
      "Benchmarks"
    ],
    "projects": [
      {
        "name": "Frontier Model Evaluations",
        "description": "Safety evaluations of frontier AI models in cooperation with international partners",
        "status": "Active"
      },
      {
        "name": "AI Safety Guidelines",
        "description": "Development of safety guidelines for generative AI",
        "status": "Active"
      }
    ],
    "notes": "Launched February 2024 under METI. Part of the international network of AI Safety Institutes. Participates in joint evaluation protocols with US AISI and UK AISI."
  },
  {
    "name": "MATS",
    "url": "https://www.matsprogram.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "MATS is an independent research and educational program that connects talented scholars with top mentors in AI alignment, governance, and security to train the next generation of AI safety researchers.",
    "focus_areas": [
      "Alignment",
      "Governance",
      "Evals"
    ],
    "key_people": [
      {
        "name": "Neel Nanda",
        "role": "Mentor"
      },
      {
        "name": "Marius Hobbhahn",
        "role": "Alumnus, Apollo Research CEO"
      },
      {
        "name": "Jesse Hoogland",
        "role": "Alumnus, Executive Director of Timaeus"
      },
      {
        "name": "Quentin Feuillade-Montixi",
        "role": "Alumnus, Co-founder and CTO of PRISM Evals"
      },
      {
        "name": "Hoagy Cunningham",
        "role": "Author"
      },
      {
        "name": "Shashwat Goel",
        "role": "Author"
      },
      {
        "name": "Annah Dombrowski",
        "role": "Author"
      },
      {
        "name": "Stefan Heimersheim",
        "role": "Author"
      },
      {
        "name": "Arthur Conmy",
        "role": "Author"
      },
      {
        "name": "Aengus Lynch",
        "role": "Author"
      },
      {
        "name": "Lukas Berglund",
        "role": "Author"
      },
      {
        "name": "Meg Tong",
        "role": "Author"
      },
      {
        "name": "Max Kaufmann",
        "role": "Author"
      },
      {
        "name": "Asa Cooper Stickland",
        "role": "Author"
      },
      {
        "name": "Nick Gabrieli",
        "role": "Author"
      },
      {
        "name": "Nina Panickssery",
        "role": "Author"
      },
      {
        "name": "Julian Schulz",
        "role": "Author"
      },
      {
        "name": "Aaquib Syed",
        "role": "Author"
      },
      {
        "name": "Andy Arditi",
        "role": "Author"
      },
      {
        "name": "Wes Gurnee",
        "role": "Author"
      },
      {
        "name": "Arjun Panickssery",
        "role": "Author"
      },
      {
        "name": "Oam Patel",
        "role": "Author"
      },
      {
        "name": "Samuel Marks",
        "role": "Author"
      },
      {
        "name": "Lisa Thiergart",
        "role": "Author"
      },
      {
        "name": "David Udell",
        "role": "Author"
      },
      {
        "name": "Ulisse Mini",
        "role": "Author"
      },
      {
        "name": "Jonathan Ng",
        "role": "Author"
      },
      {
        "name": "Hanlin Zhang",
        "role": "Author"
      },
      {
        "name": "Bilal Chughtai",
        "role": "Author"
      },
      {
        "name": "Simon Lermen",
        "role": "Author"
      },
      {
        "name": "Oskar John Hollinsworth",
        "role": "Author"
      },
      {
        "name": "Curt Tigges",
        "role": "Author"
      },
      {
        "name": "Aidan Ewart",
        "role": "Author"
      },
      {
        "name": "Phillip Guo",
        "role": "Author"
      },
      {
        "name": "Cindy Wu",
        "role": "Author"
      },
      {
        "name": "Vivek Hebbar",
        "role": "Author"
      },
      {
        "name": "Lorenzo Pacchiardi",
        "role": "Author"
      },
      {
        "name": "Alex Chan",
        "role": "Author"
      },
      {
        "name": "Ilan Moscovitz",
        "role": "Author"
      },
      {
        "name": "Jiaxin Wen",
        "role": "Author"
      },
      {
        "name": "Callum McDougall",
        "role": "Author"
      },
      {
        "name": "Cody Rushing",
        "role": "Author"
      },
      {
        "name": "Jordan Taylor",
        "role": "Author"
      },
      {
        "name": "Jacob Dunefsky",
        "role": "Author"
      },
      {
        "name": "Philippe Chlenski",
        "role": "Author"
      },
      {
        "name": "Javier Ferrando Monsonis",
        "role": "Author"
      },
      {
        "name": "Oscar Balcells Obeso",
        "role": "Author"
      },
      {
        "name": "Daniel Tan",
        "role": "Author"
      },
      {
        "name": "Mart\u00edn Soto Quintanilla",
        "role": "Author"
      },
      {
        "name": "Felix Hofst\u00e4tter",
        "role": "Author"
      },
      {
        "name": "Teun van der Weij",
        "role": "Author"
      },
      {
        "name": "Joseph Miller",
        "role": "Author"
      },
      {
        "name": "David Karamardian",
        "role": "Author"
      },
      {
        "name": "Iv\u00e1n Arcuschin Moreno",
        "role": "Author"
      },
      {
        "name": "Kajetan  Janiak",
        "role": "Author"
      },
      {
        "name": "Adam Karvonen",
        "role": "Author"
      },
      {
        "name": "Can Rager",
        "role": "Author"
      },
      {
        "name": "Benjamin Wright",
        "role": "Author"
      },
      {
        "name": "Matthew Siu",
        "role": "Author"
      },
      {
        "name": "Sviatoslav Chalnev",
        "role": "Author"
      },
      {
        "name": "Aghyad Deeb",
        "role": "Author"
      },
      {
        "name": "Pranav Gade",
        "role": "Author"
      },
      {
        "name": "Eoin Farrell",
        "role": "Author"
      },
      {
        "name": "Yeu-Tong Lau",
        "role": "Author"
      },
      {
        "name": "Georg Lange",
        "role": "Author"
      },
      {
        "name": "Aleksandar Makelov",
        "role": "Author"
      },
      {
        "name": "Patrick Leask",
        "role": "Author"
      },
      {
        "name": "Bart Bussmann",
        "role": "Author"
      },
      {
        "name": "David Chanin",
        "role": "Author"
      },
      {
        "name": "Connor Kissane",
        "role": "Author"
      },
      {
        "name": "Joseph Isaac Bloom",
        "role": "Author"
      },
      {
        "name": "Robert Krzyzanowski",
        "role": "Author"
      },
      {
        "name": "Jenny Bao",
        "role": "Author"
      },
      {
        "name": "Joshua Engels",
        "role": "Author"
      },
      {
        "name": "Atticus Wang",
        "role": "Author"
      },
      {
        "name": "Michael Pearce",
        "role": "Author"
      },
      {
        "name": "Marcus Williams",
        "role": "Author"
      },
      {
        "name": "Constantin Weisser",
        "role": "Author"
      },
      {
        "name": "Peli Grietzer",
        "role": "Author"
      },
      {
        "name": "Jessica Cooper",
        "role": "Author"
      },
      {
        "name": "Matthew Watkins",
        "role": "Author"
      },
      {
        "name": "Sumeet Motwani",
        "role": "Author"
      },
      {
        "name": "Anish Mudide",
        "role": "Author"
      },
      {
        "name": "Alexander Meinke",
        "role": "Author"
      },
      {
        "name": "Rudolf Laine",
        "role": "Author"
      },
      {
        "name": "Sara Price",
        "role": "Author"
      },
      {
        "name": "Caleb Larson",
        "role": "Author"
      },
      {
        "name": "Florian Dietz",
        "role": "Author"
      },
      {
        "name": "Kei Nishimura-Gasparian",
        "role": "Author"
      },
      {
        "name": "Jeanne Salle",
        "role": "Author"
      },
      {
        "name": "Satvik Golechha",
        "role": "Author"
      },
      {
        "name": "Jacek Karwowski",
        "role": "Author"
      },
      {
        "name": "Zora Che",
        "role": "Author"
      },
      {
        "name": "Dan Valentine",
        "role": "Author"
      },
      {
        "name": "James Chua",
        "role": "Author"
      },
      {
        "name": "John Hughes",
        "role": "Author"
      },
      {
        "name": "Rajashree Agrawal",
        "role": "Author"
      },
      {
        "name": "Artur Zolkowski",
        "role": "Author"
      },
      {
        "name": "Robert McCarthy",
        "role": "Author"
      },
      {
        "name": "Elizabeth Donoway",
        "role": "Author"
      },
      {
        "name": "Yashvardhan Sharma",
        "role": "Author"
      },
      {
        "name": "Jakub Kry\u015b",
        "role": "Author"
      },
      {
        "name": "Jack Foxabbott",
        "role": "Author"
      },
      {
        "name": "Ev\u017een Wybitul",
        "role": "Author"
      },
      {
        "name": "Evan Ryan Gunter",
        "role": "Author"
      },
      {
        "name": "Rohan Gupta",
        "role": "Author"
      },
      {
        "name": "Constantin Venhoff",
        "role": "Author"
      },
      {
        "name": "Jake Ward",
        "role": "Author"
      },
      {
        "name": "Helena Casademunt",
        "role": "Author"
      },
      {
        "name": "Caden Juang",
        "role": "Author"
      },
      {
        "name": "Claire Short",
        "role": "Author"
      },
      {
        "name": "Bartosz Cywi\u0144ski",
        "role": "Author"
      },
      {
        "name": "Emil Ryd",
        "role": "Author"
      },
      {
        "name": "Su Hyeong Lee",
        "role": "Author"
      },
      {
        "name": "Wen Xing",
        "role": "Author"
      },
      {
        "name": "Roy Rinberg",
        "role": "Author"
      },
      {
        "name": "Daniel Reuter",
        "role": "Author"
      },
      {
        "name": "Tim Hua",
        "role": "Author"
      },
      {
        "name": "Andrew Qin",
        "role": "Author"
      },
      {
        "name": "Luke Marks",
        "role": "Author"
      },
      {
        "name": "Winnie X",
        "role": "Author"
      },
      {
        "name": "Joschka Braun",
        "role": "Author"
      },
      {
        "name": "Damon Falck",
        "role": "Author"
      },
      {
        "name": "Yeonwoo Jang",
        "role": "Author"
      }
    ],
    "projects": [
      {
        "name": "MATS Summer 2026",
        "description": "12-week research program in Berkeley, CA running June 1 - August 21",
        "status": "Active"
      },
      {
        "name": "Extension Program",
        "description": "6-12 additional months of research continuation in London, UK",
        "status": "Active"
      },
      {
        "name": "Research Symposium",
        "description": "Program culmination with poster presentations and spotlight talks",
        "status": "Active"
      },
      {
        "name": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Hoagy Cunningham",
        "paper_url": "https://arxiv.org/abs/2309.08600",
        "citations": 0
      },
      {
        "name": "Representation Engineering: A Top-Down Approach to AI Transparency",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Shashwat Goel, Annah Dombrowski",
        "paper_url": "https://arxiv.org/abs/2310.01405",
        "citations": 0,
        "influential_citations": 0,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/7c8385e40fd4296ffcb688949a9486f7213df23c"
      },
      {
        "name": "Towards Automated Circuit Discovery for Mechanistic Interpretability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Stefan Heimersheim, Arthur Conmy, Aengus Lynch",
        "paper_url": "https://arxiv.org/abs/2304.14997",
        "citations": 427,
        "influential_citations": 47,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/eefbd8b384a58f464827b19e30a6920ba976def9"
      },
      {
        "name": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Lukas Berglund, Meg Tong, Max Kaufmann, Asa Cooper Stickland",
        "paper_url": "https://arxiv.org/abs/2309.12288",
        "citations": 0
      },
      {
        "name": "Towards Understanding Sycophancy in Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Meg Tong",
        "paper_url": "https://arxiv.org/abs/2310.13548",
        "citations": 435,
        "influential_citations": 33,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/e6423c211fea2945aa71e1ac5ea24f8f595b4b0a"
      },
      {
        "name": "Steering Llama 2 via Contrastive Activation Addition",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Nick Gabrieli, Nina Panickssery (n\u00e9e Rimsky), Julian Schulz, Meg Tong",
        "paper_url": "https://arxiv.org/abs/2312.06681",
        "citations": 400,
        "influential_citations": 80,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/edd1dc1e8d7989f36c0e54f69f4aeb5e597edc8b"
      },
      {
        "name": "Refusal in Language Models Is Mediated by a Single Direction",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Aaquib Syed, Andy Arditi",
        "paper_url": "https://arxiv.org/abs/2406.11717",
        "citations": 0
      },
      {
        "name": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Wes Gurnee",
        "paper_url": "https://arxiv.org/abs/2305.01610",
        "citations": 0
      },
      {
        "name": "LLM Evaluators Recognize and Favor Their Own Generations",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Arjun Panickssery",
        "paper_url": "https://arxiv.org/abs/2404.13076",
        "citations": 0
      },
      {
        "name": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Oam Patel, Samuel Marks, Annah Dombrowski",
        "paper_url": "https://arxiv.org/abs/2403.03218",
        "citations": 0
      },
      {
        "name": "Steering Language Models With Activation Engineering",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Lisa Thiergart, David Udell, Ulisse Mini",
        "paper_url": "https://arxiv.org/abs/2308.10248",
        "citations": 243,
        "influential_citations": 23,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/fe303bbaae47b1b08d0641b41d3288fcd74a3a80"
      },
      {
        "name": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jonathan Ng, Hanlin Zhang",
        "paper_url": "https://arxiv.org/abs/2304.03279",
        "citations": 0
      },
      {
        "name": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Bilal Chughtai",
        "paper_url": "https://arxiv.org/abs/2302.03025",
        "citations": 123,
        "influential_citations": 17,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/5969eff0e72e4a5bc0c7392c700be74a01ac2822"
      },
      {
        "name": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Simon Lermen",
        "paper_url": "https://arxiv.org/abs/2310.20624",
        "citations": 0
      },
      {
        "name": "Linear Representations of Sentiment in Large Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Oskar John Hollinsworth, Curt Tigges",
        "paper_url": "https://arxiv.org/abs/2310.15154",
        "citations": 0
      },
      {
        "name": "Eight Methods to Evaluate Robust Unlearning in LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Aidan Ewart, Aengus Lynch, Phillip Guo",
        "paper_url": "https://arxiv.org/abs/2402.16835",
        "citations": 114,
        "influential_citations": 14,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/0044140fdd4547a380b0b82052ae0b6ffd95216c"
      },
      {
        "name": "Taken out of context: On measuring situational awareness in LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Lukas Berglund, Asa Cooper Stickland, Max Kaufmann, Meg Tong",
        "paper_url": "https://arxiv.org/abs/2309.00667",
        "citations": 95,
        "influential_citations": 5,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/135ae2ea7a2c966815e85a232469a0a14b4d8d67"
      },
      {
        "name": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Aidan Ewart, Aengus Lynch, Phillip Guo, Cindy Wu, Vivek Hebbar",
        "paper_url": "https://arxiv.org/abs/2407.15549",
        "citations": 100,
        "influential_citations": 17,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/c5b9f501c01fec7afff1c5df862803f88f19236b"
      },
      {
        "name": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Lorenzo Pacchiardi, Alex Chan, Ilan Moscovitz",
        "paper_url": "https://arxiv.org/abs/2309.15840",
        "citations": 78,
        "influential_citations": 4,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/f148b3a0106cbaba356b9f099f1a10c072c0b3c5"
      },
      {
        "name": "Language Models Learn to Mislead Humans via RLHF",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jiaxin Wen",
        "paper_url": "https://arxiv.org/abs/2409.12822",
        "citations": 69,
        "influential_citations": 4,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/0eaf243f2f7c8a381baf0952f85396e2f6a655c5"
      },
      {
        "name": "Copy Suppression: Comprehensively Understanding an Attention Head",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Callum McDougall, Arthur Conmy, Cody Rushing",
        "paper_url": "https://arxiv.org/abs/2310.04625",
        "citations": 53,
        "influential_citations": 4,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/0eae83a76b2db97547d8ed42bb048c3eaaf78027"
      },
      {
        "name": "Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jordan Taylor",
        "paper_url": "https://arxiv.org/abs/2405.12241",
        "citations": 50,
        "influential_citations": 7,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/02ad427b0d20fb976741e332f69c2fd00c751164"
      },
      {
        "name": "Transcoders Find Interpretable LLM Feature Circuits",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jacob Dunefsky, Philippe Chlenski",
        "paper_url": "https://arxiv.org/abs/2406.11944",
        "citations": 0
      },
      {
        "name": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Javier Ferrando Monsonis, Oscar Balcells Obeso",
        "paper_url": "https://arxiv.org/abs/2411.14257",
        "citations": 66,
        "influential_citations": 6,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/ee480ff85412144887ab8f48eef37db273d9d952"
      },
      {
        "name": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Daniel Tan, Mart\u00edn Soto Quintanilla",
        "paper_url": "https://arxiv.org/abs/2502.17424",
        "citations": 97,
        "influential_citations": 15,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/4070a490f1cd1b9938666dbbb27c4ee627f1ddaa"
      },
      {
        "name": "AI Sandbagging: Language Models can Strategically Underperform on Evaluations",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Felix Hofst\u00e4tter, Teun van der Weij",
        "paper_url": "https://arxiv.org/abs/2406.07358",
        "citations": 55,
        "influential_citations": 3,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/07d73ca3e2b7bbb4ea09309d96834cd2a036c237"
      },
      {
        "name": "Open Problems in Mechanistic Interpretability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Joseph Miller",
        "paper_url": "https://arxiv.org/abs/2501.16496",
        "citations": 0
      },
      {
        "name": "Can LLMs Follow Simple Rules?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: David Karamardian",
        "paper_url": "https://arxiv.org/abs/2311.04235",
        "citations": 41,
        "influential_citations": 12,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/937205cf51b7ee46aa2983c129b7f5d596ea8293"
      },
      {
        "name": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Iv\u00e1n Arcuschin Moreno, Kajetan (Jett) Janiak",
        "paper_url": "https://arxiv.org/abs/2503.08679",
        "citations": 0
      },
      {
        "name": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Adam Karvonen, Can Rager, Benjamin Wright, Samuel Marks",
        "paper_url": "https://arxiv.org/abs/2408.00113",
        "citations": 44,
        "influential_citations": 2,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/b244b80d0a2d36412b56c0156532d9cbeb298ffa"
      },
      {
        "name": "Improving Steering Vectors by Targeting Sparse Autoencoder Features",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Matthew Siu, Sviatoslav Chalnev, Arthur Conmy",
        "paper_url": "https://arxiv.org/abs/2411.02193",
        "citations": 46,
        "influential_citations": 5,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/07e75d71986027a0861a681ca18c2f24a2f68747"
      },
      {
        "name": "Do Unlearning Methods Remove Information from Language Model Weights?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Aghyad Deeb",
        "paper_url": "https://arxiv.org/abs/2410.08827",
        "citations": 0
      },
      {
        "name": "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Simon Lermen, Pranav Gade",
        "paper_url": "https://arxiv.org/abs/2311.00117",
        "citations": 34,
        "influential_citations": 2,
        "year": 2023,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/512b3097ffcd6afcde6b58da1e656d5d9a635678"
      },
      {
        "name": "Applying sparse autoencoders to unlearn knowledge in language models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Eoin Farrell, Yeu-Tong Lau, Arthur Conmy",
        "paper_url": "https://arxiv.org/abs/2410.19278",
        "citations": 46,
        "influential_citations": 7,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/666aaab90b9558f82e5bde5dc62d97f9d961caa0"
      },
      {
        "name": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Georg Lange, Aleksandar Makelov",
        "paper_url": "https://arxiv.org/abs/2311.17030",
        "citations": 0
      },
      {
        "name": "BatchTopK Sparse Autoencoders",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Patrick Leask, Bart Bussmann",
        "paper_url": "https://arxiv.org/abs/2412.06410",
        "citations": 0
      },
      {
        "name": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Adam Karvonen, Can Rager, David Chanin",
        "paper_url": "https://arxiv.org/abs/2503.09532",
        "citations": 47,
        "influential_citations": 10,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/447b7fa233fe9b129001f0bb7f5c4a900de29e5d"
      },
      {
        "name": "Interpreting Attention Layer Outputs with Sparse Autoencoders",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Connor Kissane, Joseph Isaac Bloom, Robert Krzyzanowski",
        "paper_url": "https://arxiv.org/abs/2406.17759",
        "citations": 34,
        "influential_citations": 3,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/efbe48e4c17a097f4feaec8cf9dec40218a27a87"
      },
      {
        "name": "Tell Me About Yourself: LLMs are Aware of their Learned Behaviors",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jenny Bao",
        "paper_url": "https://arxiv.org/pdf/2501.11120",
        "citations": 45,
        "influential_citations": 5,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/a3ec0b75274a29bf7637f9090d5ca5047e2c7545"
      },
      {
        "name": "Are Sparse Autoencoders Useful? A Case Study in Sparse Probing",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Joshua Engels",
        "paper_url": "https://arxiv.org/pdf/2502.16681",
        "citations": 0
      },
      {
        "name": "Simple Mechanistic Explanations for Out-Of-Context Reasoning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Atticus Wang, Joshua Engels",
        "paper_url": "https://arxiv.org/abs/2507.08218",
        "citations": 1,
        "influential_citations": 1,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/4a37bffe6587bee07ed38f1fb953347502e9cccd"
      },
      {
        "name": "Sparse Autoencoders Do Not Find Canonical Units of Analysis",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Patrick Leask, Bart Bussmann, Michael Pearce",
        "paper_url": "https://arxiv.org/abs/2502.04878",
        "citations": 0
      },
      {
        "name": "On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Marcus Williams, Constantin Weisser",
        "paper_url": "https://arxiv.org/abs/2411.02306",
        "citations": 39,
        "influential_citations": 1,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/33a4b77f0c278fbe24f8203d6c958db5be439f43"
      },
      {
        "name": "Understanding and Controlling a Maze-Solving Policy Network",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Ulisse Mini, Peli Grietzer",
        "paper_url": "https://arxiv.org/abs/2310.08043",
        "citations": 35,
        "influential_citations": 3,
        "year": 2020,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/db3574a611f5e832218c5f14c6f85af06370caf7"
      },
      {
        "name": "SolidGoldMagikarp (plus, prompt generation)",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jessica Cooper (Rumbelow), Matthew Watkins",
        "paper_url": "https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation",
        "citations": 0
      },
      {
        "name": "Secret Collusion Among Generative AI Agents",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Sumeet Motwani",
        "paper_url": "https://arxiv.org/abs/2402.07510",
        "citations": 21,
        "influential_citations": 2,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/5c430e2ac5da64e410f6e7c7ef775c47916fe74c"
      },
      {
        "name": "Efficient Dictionary Learning with Switch Sparse Autoencoders",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Anish Mudide",
        "paper_url": "https://arxiv.org/abs/2410.08201",
        "citations": 27,
        "influential_citations": 1,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/7713145541360109564a0c76926a5a8b279ad6e6"
      },
      {
        "name": "Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Alexander Meinke, Rudolf Laine, Bilal Chughtai, Marius Hobbhahn",
        "paper_url": "https://arxiv.org/abs/2407.04694",
        "citations": 36,
        "influential_citations": 3,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/d8f56c0f368c23cf67538f89bcb6ae075495a8f6"
      },
      {
        "name": "Explorations of Self-Repair in Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Cody Rushing",
        "paper_url": "https://arxiv.org/abs/2402.15390",
        "citations": 22,
        "influential_citations": 0,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/6c6cb32f026fccc11c98c36913651b992d477a56"
      },
      {
        "name": "Best-of-N Jailbreaking",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Sara Price, Aengus Lynch",
        "paper_url": "https://arxiv.org/abs/2412.03556",
        "citations": 258,
        "influential_citations": 45,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/c9c0324fcdc92cf7e24f9c4230864851a552f953"
      },
      {
        "name": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jiaxin Wen, Vivek Hebbar, Caleb Larson",
        "paper_url": "https://arxiv.org/abs/2411.17693",
        "citations": 14,
        "influential_citations": 0,
        "year": 2024,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/731d34bc338d24a34f42a23097642c1437e14edb"
      },
      {
        "name": "Auditing language models for hidden objectives",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Florian Dietz, Kei Nishimura-Gasparian, Jeanne Salle, Satvik Golechha",
        "paper_url": "https://arxiv.org/abs/2503.10965",
        "citations": 22,
        "influential_citations": 2,
        "year": 2025,
        "semantic_scholar_url": "https://www.semanticscholar.org/paper/eeea3cc563b4e880a618b62961b107407990bac4"
      },
      {
        "name": "Goodhart's Law in Reinforcement Learning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jacek Karwowski",
        "paper_url": "https://arxiv.org/abs/2310.09144"
      },
      {
        "name": "Model tampering attacks enable more rigorous evaluations of LLM capabilities",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Zora Che",
        "paper_url": "https://arxiv.org/pdf/2502.05209"
      },
      {
        "name": "When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Dan Valentine, James Chua, John Hughes, Rajashree Agrawal",
        "paper_url": "https://arxiv.org/abs/2407.15211"
      },
      {
        "name": "Early Signs of Steganographic Capabilities in Frontier LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy",
        "paper_url": "https://arxiv.org/abs/2507.02737"
      },
      {
        "name": "Technical Report: Evaluating Goal Drift in Language Model Agents",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Elizabeth Donoway",
        "paper_url": "https://arxiv.org/abs/2505.02709"
      },
      {
        "name": "Distributed and Decentralised Training: Technical Governance Challenges in a Shifting AI Landscape",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Yashvardhan Sharma, Jakub Kry\u015b",
        "paper_url": "https://arxiv.org/abs/2507.07765v1"
      },
      {
        "name": "A Causal Model of Theory-of-Mind in AI Agents",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jack Foxabbott",
        "paper_url": "https://openreview.net/forum?id=ASA2jdKtf3"
      },
      {
        "name": "ViSTa Dataset: Do vision-language models understand sequential tasks?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Ev\u017een Wybitul, Evan Ryan Gunter",
        "paper_url": "https://arxiv.org/abs/2411.13211"
      },
      {
        "name": "RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Rohan Gupta",
        "paper_url": "https://www.arxiv.org/abs/2506.14261"
      },
      {
        "name": "Reasoning-Finetuning Repurposes Latent Representations in Base Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Constantin Venhoff, Jake Ward",
        "paper_url": "https://arxiv.org/abs/2507.12638"
      },
      {
        "name": "Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks",
        "paper_url": "https://arxiv.org/abs/2507.16795"
      },
      {
        "name": "Public Perspectives on AI Governance: A Survey of Working Adults in California, Illinois, and New York",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Claire Short",
        "paper_url": "https://doi.org/10.5281/zenodo.16566058"
      },
      {
        "name": "Towards eliciting latent knowledge from LLMs with mechanistic interpretability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Bartosz Cywi\u0144ski, Emil Ryd",
        "paper_url": "https://arxiv.org/abs/2505.14352"
      },
      {
        "name": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Dan Valentine, John Hughes",
        "paper_url": "https://arxiv.org/abs/2402.06782"
      },
      {
        "name": "On Defining Neural Averaging",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Su Hyeong Lee",
        "paper_url": "https://arxiv.org/abs/2508.14832"
      },
      {
        "name": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Su Hyeong Lee",
        "paper_url": "https://arxiv.org/abs/2509.06701"
      },
      {
        "name": "Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Artur Zolkowski, Wen Xing",
        "paper_url": "https://arxiv.org/abs/2510.19851"
      },
      {
        "name": "Eliciting Secret Knowledge from Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Bartosz Cywi\u0144ski",
        "paper_url": "https://arxiv.org/abs/2510.01070"
      },
      {
        "name": "Verifying LLM Inference to Prevent Model Weight Exfiltration",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Roy Rinberg, Daniel Reuter, Adam Karvonen",
        "paper_url": "https://arxiv.org/abs/2511.02620"
      },
      {
        "name": "Steering Evaluation-Aware Language Models to Act Like They Are Deployed",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Tim Hua, Andrew Qin",
        "paper_url": "https://arxiv.org/abs/2510.20487"
      },
      {
        "name": "DiFR: Inference Verification Despite Nondeterminism",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Adam Karvonen, Daniel Reuter, Roy Rinberg, Luke Marks",
        "paper_url": "https://arxiv.org/abs/2511.20621"
      },
      {
        "name": "AI agents find $4.6M in blockchain smart contract exploits",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Winnie X",
        "paper_url": "https://red.anthropic.com/2025/smart-contracts/"
      },
      {
        "name": "Resisting RL Elicitation of Biosecurity Capabilities: Reasoning Models Exploration Hacking on WMDP",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Joschka Braun, Damon Falck, Yeonwoo Jang",
        "paper_url": "https://openreview.net/pdf/2645934ae38765d0fd2446ed66cb06e5f406dcbd.pdf"
      }
    ],
    "notes": "357 scholars and 75 mentors supported since 2021. Produced 115 research publications with 5100+ citations (h-index 31). 80% of alumni work in AI alignment. ~10% of alumni founded AI safety organizations. Provides $14.4k stipend, $12k compute budget, housing, and travel. Notable spin-off organizations include Apollo Research, PRISM Eval, Timaeus, and many others.",
    "employees": 33,
    "directors": 2,
    "managers": 7,
    "subteams": 5
  },
  {
    "name": "Conjecture",
    "url": "https://www.conjecture.dev/research",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "Conjecture is an AI alignment research startup that focuses on building Cognitive Emulation - an AI architecture that bounds systems' capabilities and makes them reason in ways humans can understand and control.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Control"
    ],
    "key_people": [
      {
        "name": "Connor Leahy",
        "role": "Founder"
      },
      {
        "name": "Sid Black",
        "role": "Founder"
      },
      {
        "name": "Gabriel Alfour",
        "role": "Founder"
      },
      {
        "name": "Adam Shimi",
        "role": "Early staff/Researcher"
      }
    ],
    "projects": [
      {
        "name": "Cognitive Emulation (CoEm)",
        "description": "Primary research direction to build predictably boundable AI systems rather than directly aligned AGIs",
        "status": "Active"
      },
      {
        "name": "Cognitive Software",
        "description": "Approach to building AI systems that emulate human cognitive patterns",
        "status": "Active"
      },
      {
        "name": "unRLHF",
        "description": "Research on efficiently undoing LLM safeguards",
        "status": "Completed"
      },
      {
        "name": "MAGIC (Multinational AGI Consortium)",
        "description": "Proposal for international coordination on AI through a global institution permitted to develop advanced AI",
        "status": "Unknown"
      },
      {
        "name": "Cognitive Emulation",
        "description": "An AI architecture that bounds systems' capabilities and makes them reason in ways that humans can understand and control, aimed at building predictably boundable systems rather than directly aligned AGIs",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Multinational AGI Consortium (MAGIC)",
        "description": "A proposal for international coordination on AI to mitigate existential risks from advanced AI through a global moratorium on advanced AI development",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Refine",
        "description": "A model for diversifying conceptual alignment research approaches to increase access to the field and stimulate new ideas",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "London-based startup with VC backing from notable investors including Nat Friedman, Daniel Gross, Collison brothers, Andrej Karpathy, and Sam Bankman-Fried. Team includes EleutherAI alumni and independent researchers.",
    "employees": 13
  },
  {
    "name": "FAR AI",
    "url": "https://far.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "FAR.AI is a research & education non-profit ensuring advanced AI is safe and beneficial for everyone.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Evals",
      "Benchmarks"
    ],
    "key_people": [
      {
        "name": "Matthew Kowal",
        "role": "Researcher"
      },
      {
        "name": "Jasper Timm",
        "role": "Researcher"
      },
      {
        "name": "Niki Howe",
        "role": "Researcher"
      },
      {
        "name": "Micha\u0142 Zaj\u0105c",
        "role": "Researcher"
      },
      {
        "name": "Adri\u00e0 Garriga Alonso",
        "role": null
      }
    ],
    "projects": [
      {
        "name": "Frontier LLMs Attempt to Persuade into Harmful Topics",
        "description": "Research on how easily frontier models can be prompted to persuade people into harmful beliefs or illegal actions",
        "status": "Active"
      },
      {
        "name": "Does Robustness Improve with Scale?",
        "description": "Investigation of whether scaling up model size can solve robustness issues in frontier LLMs",
        "status": "Active"
      },
      {
        "name": "FAR.Labs",
        "description": "Collaborative co-working space in Berkeley for researchers developing AI risk solutions",
        "status": "Active"
      },
      {
        "name": "Layered AI Defenses Have Holes: Vulnerabilities and Key Recommendations",
        "description": "Testing effectiveness of defense-in-depth AI safety strategies using STACK attack method, achieving 71% success rate on catastrophic risk scenarios where conventional attacks achieved 0%",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Beyond the Board: Exploring AI Robustness Through Go",
        "description": "Testing three approaches to defend Go AIs from adversarial strategies, finding that defenses protect against known adversaries but new adversaries can undermine these defenses",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Adversarial Policies Beat Superhuman Go AIs",
        "description": "Attack on KataGo AI system where adversaries trick the AI into making blunders rather than winning through superior play, demonstrating failure modes in superhuman AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Exploiting Novel GPT-4 APIs",
        "description": "Red-teaming GPT-4 fine-tuning, function calling and knowledge retrieval APIs, finding that fine-tuning on as few as 15 harmful examples can remove core safeguards",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Inverse Scaling: When Bigger Isn't Better",
        "description": "Presenting 11 instances of inverse scaling where language models get worse with scale rather than better, selected from 99 submissions in the Inverse Scaling Prize competition",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Pacing Outside the Box: RNNs Learn to Plan in Sokoban",
        "description": "Study of RNN trained to play Sokoban that learned to spend time planning ahead by pacing despite penalties, demonstrating strategic planning in neural networks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
        "description": "Method to modify neural networks to make their internals more interpretable and steerable using quantization bottleneck that forces activation vectors into discrete codes",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Exploring Scaling Trends in LLM Robustness",
        "description": "Research finding that robustness against adversarial attacks improves with adversarial training but not with model scaling alone",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Organization hosts alignment workshops globally, runs grantmaking programs for academics and independent researchers, and collaborates with organizations like UC Berkeley, University of Montreal, Mozilla, and government agencies.",
    "employees": 29,
    "directors": 2,
    "managers": 4,
    "subteams": 4,
    "benchmarks": [
      {
        "name": "Attempt to Persuade Eval (APE)",
        "measures": "Tests how willing LLMs are to generate content aimed at shaping beliefs and behavior on harmful topics",
        "status": "Active"
      },
      {
        "name": "InterpBench",
        "measures": "Collection of 17 semi-synthetic transformers with known circuits for evaluating mechanistic interpretability techniques",
        "status": "Active"
      },
      {
        "name": "STACK attack method",
        "measures": "Bypasses AI defense layers sequentially to test effectiveness of multi-layered AI safety strategies",
        "status": "Active"
      }
    ]
  },
  {
    "name": "Epoch AI",
    "url": "https://epoch.ai/research",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Epoch AI is a multidisciplinary non-profit research institute investigating the future of artificial intelligence and forecasting its economic and societal impact.",
    "focus_areas": [
      "Evals",
      "Benchmarks"
    ],
    "projects": [
      {
        "name": "FrontierMath",
        "description": "A benchmark of several hundred unpublished, expert-level mathematics problems that take specialists hours to days to solve",
        "status": "Active"
      },
      {
        "name": "GATE Playground",
        "description": "Not specified in content",
        "status": "Active"
      },
      {
        "name": "Distributed Training",
        "description": "Not specified in content",
        "status": "Active"
      },
      {
        "name": "Model Counts",
        "description": "Not specified in content",
        "status": "Active"
      },
      {
        "name": "Can AI Scaling Continue Through 2030?",
        "description": "Investigation of four constraints to scaling AI training: power, chip manufacturing, data, and latency. Predicts 2e29 FLOP runs will be feasible by 2030",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "A Rosetta Stone for AI benchmarks",
        "description": "Statistical framework that stitches benchmarks together for studying long-run AI trends",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "GATE: Modeling the trajectory of AI and automation",
        "description": "Compute-centric model of AI automation and its economic effects",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Direct Approach interactive model",
        "description": "User-adjustable model forecasting when transformative AI could be deployed",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "FrontierMath",
        "measures": "Expert-level mathematics problems that take specialists hours to days to solve"
      },
      {
        "name": "OSWorld",
        "measures": "AI's ability to use computers and interpret GUI-based tasks",
        "status": "Active"
      },
      {
        "name": "SWE-bench Verified",
        "measures": "Agentic coding capabilities, focusing on bug fixes in open-source repositories",
        "status": "Active"
      }
    ],
    "notes": "Maintains the largest public database of notable ML models, conducts research on AI scaling through 2030, and provides data insights on AI trends including training compute and hardware advancements.",
    "employees": 22,
    "directors": 2,
    "key_people": [
      {
        "name": "Anson Ho",
        "role": "Researcher"
      },
      {
        "name": "Jean-Stanislas Denain",
        "role": "Researcher"
      },
      {
        "name": "David Atanasov",
        "role": "Researcher"
      },
      {
        "name": "Samuel Albanie",
        "role": "Researcher"
      },
      {
        "name": "Rohin Shah",
        "role": "Researcher"
      },
      {
        "name": "Ben Cottier",
        "role": "Researcher"
      },
      {
        "name": "Jaime Sevilla",
        "role": "Researcher"
      },
      {
        "name": "Tamay Besiroglu",
        "role": "Researcher"
      },
      {
        "name": "David Owen",
        "role": "Researcher"
      },
      {
        "name": "Ege Erdil",
        "role": "Researcher"
      },
      {
        "name": "Pablo Villalobos",
        "role": "Researcher"
      },
      {
        "name": "Greg Burnham",
        "role": "Researcher"
      },
      {
        "name": "Florian Brand",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Apart Research",
    "url": "https://apartresearch.com/",
    "type": "Nonprofit",
    "country": "Denmark",
    "mission": "Apart Research accelerates AI safety research through mentorship, collaborations, and research sprints to make advanced AI safe and beneficial for humanity.",
    "focus_areas": [
      "Alignment",
      "Control"
    ],
    "notes": "Organization focuses on building global research communities, organizing hackathons, and providing career development support for AI safety researchers. Multiple testimonials highlight their role in career transitions and community building.",
    "employees": 9,
    "projects": [
      {
        "name": "Rethinking CyberSecEval: An LLM-Aided Approach to Evaluation Critique",
        "description": "LLM-aided approach to evaluation critique for cybersecurity evaluations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Catastrophic Cyber Capabilities Benchmark (3CB)",
        "description": "Robustly evaluating LLM agent cyber offense capabilities, showing realistic challenges for cyber offense can be completed by SoTA LLMs",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Benchmark Inflation: Revealing LLM Performance Gaps Using Retro-Holdouts",
        "description": "Research on revealing LLM performance gaps using retro-holdouts in benchmarks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Interpreting Learned Feedback Patterns in Large Language Models",
        "description": "Mechanistic interpretability research on learned feedback patterns in LLMs",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
        "description": "Research on training deceptive LLMs that persist through safety training",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
        "description": "Model editing techniques can introduce unwanted side effects in neural networks not detected by existing benchmarks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Understanding Addition in Transformers",
        "description": "Mechanistic interpretability research on understanding addition in transformers",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
        "description": "Benchmark for evaluating dark patterns in large language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "CryptoFormalEval: Integrating LLMs and Formal Verification",
        "description": "Integrating LLMs and formal verification for automated cryptographic protocol vulnerability detection",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Multi-Agent Security Tax",
        "description": "Trading off security and collaboration capabilities in multi-agent systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities",
        "description": "Shows realistic challenges for cyber offense can be completed by SoTA LLMs while open source models lag behind.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Alignment",
          "Interpretability",
          "Evals"
        ]
      },
      {
        "name": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
        "description": null,
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Alignment",
          "Interpretability",
          "Evals"
        ]
      },
      {
        "name": "Increasing Trust in Language Models through the Reuse of Verified Circuits",
        "description": null,
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Alignment",
          "Interpretability",
          "Evals"
        ]
      },
      {
        "name": "Large Language Models Relearn Removed Concepts",
        "description": null,
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Alignment",
          "Interpretability",
          "Evals"
        ]
      },
      {
        "name": "DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models",
        "description": null,
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Alignment",
          "Interpretability",
          "Evals"
        ]
      },
      {
        "name": "Locating cross-task sequence continuation circuits in transformers",
        "description": null,
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Alignment",
          "Interpretability",
          "Evals"
        ]
      },
      {
        "name": "Interpreting language model neurons at scale",
        "description": null,
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Alignment",
          "Interpretability",
          "Evals"
        ]
      }
    ],
    "benchmarks": [
      {
        "name": "Catastrophic Cyber Capabilities Benchmark (3CB)",
        "measures": "LLM agent cyber offense capabilities",
        "status": "Active"
      },
      {
        "name": "DarkBench",
        "measures": "Dark patterns in large language models",
        "status": "Active"
      },
      {
        "name": "VLM Deception Benchmark",
        "measures": "Deception detection in vision-language models with 1,048 image-text pairs",
        "status": "Active"
      },
      {
        "name": "Improved Specificity Benchmark",
        "measures": "Edit failures in large language models",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "E. Kran",
        "role": "Researcher"
      },
      {
        "name": "J. Hoelscher-Obermaier",
        "role": "Researcher"
      },
      {
        "name": "J. Persson",
        "role": "Researcher"
      },
      {
        "name": "F. Barez",
        "role": "Researcher"
      },
      {
        "name": "A. Anurin",
        "role": "Researcher"
      },
      {
        "name": "J. Ng",
        "role": "Researcher"
      },
      {
        "name": "K. Schaffer",
        "role": "Researcher"
      },
      {
        "name": "J. Schreiber",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "EleutherAI",
    "url": "https://www.eleuther.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "EleutherAI trains and releases powerful open source large language models while conducting research on AI safety and interpretability.",
    "focus_areas": [
      "Interpretability",
      "Alignment",
      "Evals"
    ],
    "projects": [
      {
        "name": "Interpreting Across Time",
        "description": "Research on how properties of models emerge and evolve over the course of training",
        "status": "Active"
      },
      {
        "name": "Eliciting Latent Knowledge",
        "description": "Directly eliciting latent knowledge inside model activations to verify claims when humans can't independently check",
        "status": "Active"
      },
      {
        "name": "Training LLMs",
        "description": "Training and releasing powerful open source large language models",
        "status": "Active"
      },
      {
        "name": "Common Pile v0.1",
        "description": "Dataset project",
        "status": "Active"
      },
      {
        "name": "EvalEval Coalition",
        "description": "Evaluation initiative",
        "status": "Active"
      },
      {
        "name": "Reward Hacking Research",
        "description": "Research update on reward hacking in AI systems",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Pretraining Data Filtering for Open-Weight AI Safety",
        "description": "Research on filtering pretraining data to improve safety of open-weight AI models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Attention Probes",
        "description": "Research on probing attention mechanisms in neural networks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Mechanistic Anomaly Detection",
        "description": "Research on detecting anomalies in AI systems through mechanistic interpretability",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "RLHF and RLAIF in GPT-NeoX",
        "description": "Implementation of Reinforcement Learning from Human Feedback and AI Feedback in GPT-NeoX",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Open Source Automated Interpretability for Sparse Autoencoder Features",
        "description": "Automated interpretability tools for understanding sparse autoencoder features",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Weak-to-Strong Generalization",
        "description": "Experiments in training strong models using supervision from weaker models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Least-Squares Concept Erasure",
        "description": "Methods for removing concepts from neural network representations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "GPT-NeoX-20B",
        "description": "Large language model with 20 billion parameters",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "Factored Cognition with Language Models",
        "description": "Preliminary exploration into factored cognition approaches using language models",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Research focus includes tamper-resistant safeguards, morphological alignment of tokenizers, and composable interventions for language models. Active in Summer of Open Science initiative.",
    "employees": 16,
    "key_people": [
      {
        "name": "Stella Biderman",
        "role": "Researcher"
      },
      {
        "name": "Nora Belrose",
        "role": "Researcher"
      },
      {
        "name": "David Johnston",
        "role": "Researcher"
      },
      {
        "name": "Curtis Huebner",
        "role": "Researcher"
      },
      {
        "name": "Leo Gao",
        "role": "Researcher"
      },
      {
        "name": "Connor Leahy",
        "role": "Researcher"
      },
      {
        "name": "Quentin Anthony",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Future of Life Institute",
    "url": "https://futureoflife.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Steering transformative technology towards benefiting life and away from extreme large-scale risks through policy advocacy, research, and education.",
    "focus_areas": [
      "Alignment",
      "Governance",
      "Policy",
      "Biosecurity",
      "Cyber"
    ],
    "key_people": [
      {
        "name": "Emilia Javorsky",
        "role": "Policy staff"
      },
      {
        "name": "Max Tegmark",
        "role": "Speaker/Representative"
      },
      {
        "name": "Mark Brakel",
        "role": "Grantmaking staff"
      },
      {
        "name": "Anthony Aguirre",
        "role": "Researcher"
      },
      {
        "name": "Michael Kleinman",
        "role": "AI Safety Researcher/Analyst"
      },
      {
        "name": "James H. Moor",
        "role": "Future of Life Award Winner - Computer Ethics and AI Safety"
      },
      {
        "name": "Batya Friedman",
        "role": "Future of Life Award Winner - Computer Ethics and AI Safety"
      },
      {
        "name": "Steve Omohundro",
        "role": "Future of Life Award Winner - Computer Ethics and AI Safety"
      }
    ],
    "projects": [
      {
        "name": "FLI AI Safety Index",
        "description": "Eight AI and governance experts evaluate the safety practices of leading general-purpose AI companies",
        "status": "Active"
      },
      {
        "name": "AI Action Plan Recommendations",
        "description": "Proposal for President Trump's AI Action Plan focusing on AI loss-of-control protection and worker protection",
        "status": "Active"
      },
      {
        "name": "AI Convergence Research",
        "description": "Policy expertise on risks at intersection of AI and nuclear, biological and cyber threats",
        "status": "Active"
      },
      {
        "name": "Autonomous Weapons Education",
        "description": "Educational materials about AI-powered weapons that harm national security",
        "status": "Active"
      },
      {
        "name": "Digital Media Accelerator",
        "description": "Supports digital content creators raising AI awareness",
        "status": "Active"
      },
      {
        "name": "Control Inversion Study",
        "description": "Research on why superintelligent AI agents would absorb power",
        "status": "Active"
      },
      {
        "name": "Control Inversion",
        "description": "AI safety research project",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Statement on Superintelligence",
        "description": "Policy statement on superintelligence risks",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Creative Contest: Keep The Future Human",
        "description": "Creative contest focused on human-centered AI futures",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Existential Safety Community",
        "description": "Community building for AI existential safety",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "FLI AI Safety Index: Winter 2025 Edition",
        "description": "Index measuring AI safety progress and metrics",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Convergence: Risks at the Intersection of AI and Nuclear, Biological and Cyber Threats",
        "description": "Research on convergent risks from AI and other threats",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "Superintelligence Imagined Creative Contest",
        "description": "Creative contest exploring superintelligence scenarios",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "AI Safety Summits",
        "description": "Coordination and participation in AI safety summits",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Tomorrow's AI",
        "description": "A scrollytelling site with 13 interactive, research-backed scenarios showing how advanced AI could transform the world\u2014for better or worse",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Safety Index",
        "description": "Safety scorecard of leading AI companies assessing how they address safety concerns",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "US Federal Agencies: Mapping AI Activities",
        "description": "Guide outlining AI activities across the US Executive Branch, focusing on regulatory authorities, budgets, and programs",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Catastrophic AI Scenarios",
        "description": "Concrete examples of how AI could go wrong",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Organization works across AI, biotechnology, and nuclear weapons risks. Has 40,000+ newsletter subscribers and provides grants to individuals and organizations. Recently announced petition with 65,000+ signatures to ban superintelligence development.",
    "employees": 5,
    "benchmarks": [
      {
        "name": "FLI AI Safety Index",
        "measures": "AI safety progress and metrics",
        "status": "Active"
      },
      {
        "name": "AI Safety Index",
        "measures": "Safety practices and policies of leading AI companies",
        "status": "Active"
      }
    ]
  },
  {
    "name": "AI Safety Camp",
    "url": "https://aisafety.camp/",
    "type": "Nonprofit",
    "country": "International",
    "mission": "AI Safety Camp (AISC) is an AI safety research program that brings together talented researchers to work on technical AI alignment projects in an intensive camp format.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Governance",
      "Evals"
    ],
    "key_people": [
      {
        "name": "Dr Waku",
        "role": "Project Lead - YouTube videos on loss-of-control risk"
      },
      {
        "name": "Remmelt Ellen",
        "role": "Project Lead - Writing about safety failures"
      },
      {
        "name": "Finn",
        "role": "Project Lead - Anti-AI coalition building"
      },
      {
        "name": "Will Petillo",
        "role": "Project Lead - Systems Dynamics Model for AI pause"
      }
    ],
    "projects": [],
    "benchmarks": [
      {
        "name": "Benchmark for Ranking LLM Preferences Relevant for Existential Risk",
        "measures": "LLM preferences related to existential risk scenarios",
        "paper_url": ""
      }
    ],
    "notes": "Runs multiple camps per year. Alumni have gone on to work at leading AI safety organizations."
  },
  {
    "name": "Ought / Elicit",
    "url": "https://elicit.com/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Elicit helps researchers be 10x more evidence-based by providing AI tools for scientific research including search, analysis, and report generation.",
    "projects": [
      {
        "name": "Research Search",
        "description": "Semantic search over 138 million academic papers and 545,000 clinical trials",
        "status": "Active"
      },
      {
        "name": "Research Reports",
        "description": "Generates high-quality research briefs based on systematic review processes",
        "status": "Active"
      },
      {
        "name": "Systematic Literature Review",
        "description": "Automates screening and data extraction for systematic reviews with up to 80% time savings",
        "status": "Active"
      },
      {
        "name": "Elicit Alerts",
        "description": "Tracks new research developments and sends updates to researchers",
        "status": "Active"
      },
      {
        "name": "Elicit Systematic Review",
        "description": "Advanced workflow delivering human-level accuracy in evidence synthesis, automating screening and data extraction of systematic reviews",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Elicit Reports",
        "description": "Fully automated rigorous research overviews built on principles of systematic reviews with higher quality and efficiency than other deep research tools",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Elicit Notebooks",
        "description": "Research analysis tool unifying search, summaries, and systematic review on a single, living page",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Clinical Trials Integration",
        "description": "Search and analysis capabilities over 545,000 clinical studies from clinicaltrials.gov with instant summarization",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "notes": "Used by over 5 million researchers across pharmaceuticals, academia, medical devices, policy/government, and other industries. Claims to be the most accurate AI product for scientific research with sentence-level citations and transparency features.",
    "employees": 23,
    "benchmarks": [
      {
        "name": "Elicit Accuracy Validation",
        "measures": "Data extraction accuracy and research quality metrics - achieved 99.4% accuracy in systematic review data extraction",
        "status": "Active"
      }
    ]
  },
  {
    "name": "arXiv AI Safety Papers",
    "url": "https://arxiv.org",
    "type": "Academic",
    "country": "International",
    "mission": "Recent AI safety research papers from arXiv preprint server.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Evals"
    ],
    "projects": [
      {
        "name": "Sycophancy Claims about Language Models: The Missing Human-in-the-Loop",
        "status": "published",
        "description": "Authors: Jan Batzner, Volker Stocker, Stefan Schmid...",
        "paper_url": "https://arxiv.org/abs/2512.00656v1"
      },
      {
        "name": "Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics",
        "status": "published",
        "description": "Authors: Deep Patel, Emmanouil-Vasileios Vlatakis-Gkaragkounis",
        "paper_url": "https://arxiv.org/abs/2512.00389v1"
      },
      {
        "name": "AI Consciousness and Existential Risk",
        "status": "published",
        "description": "Authors: Rufin VanRullen",
        "paper_url": "https://arxiv.org/abs/2511.19115v1"
      },
      {
        "name": "Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma",
        "status": "published",
        "description": "Authors: Subramanyam Sahoo, Aman Chadha, Vinija Jain...",
        "paper_url": "https://arxiv.org/abs/2511.19504v1"
      },
      {
        "name": "Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation",
        "status": "published",
        "description": "Authors: Austin Spizzirri",
        "paper_url": "https://arxiv.org/abs/2512.03048v1"
      },
      {
        "name": "Selective Weak-to-Strong Generalization",
        "status": "published",
        "description": "Authors: Hao Lang, Fei Huang, Yongbin Li",
        "paper_url": "https://arxiv.org/abs/2511.14166v1"
      },
      {
        "name": "Maximizing the efficiency of human feedback in AI alignment: a comparative analysis",
        "status": "published",
        "description": "Authors: Andreas Chouliaras, Dimitris Chatzopoulos",
        "paper_url": "https://arxiv.org/abs/2511.12796v2"
      },
      {
        "name": "Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping",
        "status": "published",
        "description": "Authors: Dena Mujtaba, Brian Hu, Anthony Hoogs...",
        "paper_url": "https://arxiv.org/abs/2511.11551v2"
      },
      {
        "name": "Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA",
        "status": "published",
        "description": "Authors: Ayush Pandey, Jai Bardhan, Ishita Jain...",
        "paper_url": "https://arxiv.org/abs/2511.11169v1"
      },
      {
        "name": "Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback",
        "status": "published",
        "description": "Authors: Vijay Keswani, Cyrus Cousins, Breanna Nguyen...",
        "paper_url": "https://arxiv.org/abs/2511.10032v1"
      },
      {
        "name": "The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems",
        "status": "published",
        "description": "Authors: Samih Fadli",
        "paper_url": "https://arxiv.org/abs/2511.10704v1"
      },
      {
        "name": "Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment",
        "status": "published",
        "description": "Authors: Shigeki Kusaka, Keita Saito, Mikoto Kudo...",
        "paper_url": "https://arxiv.org/abs/2511.09105v1"
      },
      {
        "name": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas",
        "status": "published",
        "description": "Authors: Zhen Wang, Yufan Zhou, Zhongyan Luo...",
        "paper_url": "https://arxiv.org/abs/2511.07338v3"
      },
      {
        "name": "Verifying rich robustness properties for neural networks",
        "status": "published",
        "description": "Authors: Mohammad Afzal, S. Akshay, Ashutosh Gupta",
        "paper_url": "https://arxiv.org/abs/2511.07293v1"
      },
      {
        "name": "Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction",
        "status": "published",
        "description": "Authors: Weiyan Shi, Kenny Tsu Wei Choo",
        "paper_url": "https://arxiv.org/abs/2511.04366v1"
      },
      {
        "name": "When Empowerment Disempowers",
        "status": "published",
        "description": "Authors: Claire Yang, Maya Cakmak, Max Kleiman-Weiner",
        "paper_url": "https://arxiv.org/abs/2511.04177v1"
      },
      {
        "name": "Silenced Biases: The Dark Side LLMs Learned to Refuse",
        "status": "published",
        "description": "Authors: Rom Himelstein, Amit LeVi, Brit Youngmann...",
        "paper_url": "https://arxiv.org/abs/2511.03369v2"
      },
      {
        "name": "Approximating the Mathematical Structure of Psychodynamics",
        "status": "published",
        "description": "Authors: Bryce-Allen Bagley, Navin Khoshnan",
        "paper_url": "https://arxiv.org/abs/2511.05580v1"
      },
      {
        "name": "Deep Value Benchmark: Measuring Whether Models Generalize Deep Values or Shallow Preferences",
        "status": "published",
        "description": "Authors: Joshua Ashkinaze, Hua Shen, Sai Avula...",
        "paper_url": "https://arxiv.org/abs/2511.02109v2"
      },
      {
        "name": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory",
        "status": "published",
        "description": "Authors: Kyung-Hoon Kim",
        "paper_url": "https://arxiv.org/abs/2511.00926v3"
      },
      {
        "name": "Evaluating Concept Filtering Defenses against Child Sexual Abuse Material Generation by Text-to-Image Models",
        "status": "published",
        "description": "Authors: Ana-Maria Cretu, Klim Kireev, Amro Abdalla...",
        "paper_url": "https://arxiv.org/abs/2512.05707v1"
      },
      {
        "name": "SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures",
        "status": "published",
        "description": "Authors: Panuthep Tasawong, Jian Gang Ngui, Alham Fikri Aji...",
        "paper_url": "https://arxiv.org/abs/2512.05501v1"
      },
      {
        "name": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
        "status": "published",
        "description": "Authors: Afshin Khadangi, Hanna Marxen, Amir Sartipi...",
        "paper_url": "https://arxiv.org/abs/2512.04124v1"
      },
      {
        "name": "From monoliths to modules: Decomposing transducers for efficient world modelling",
        "status": "published",
        "description": "Authors: Alexander Boyd, Franz Nowak, David Hyland...",
        "paper_url": "https://arxiv.org/abs/2512.02193v1"
      },
      {
        "name": "Evaluating AI Companies' Frontier Safety Frameworks: Methodology and Results",
        "status": "published",
        "description": "Authors: Lily Stelling, Malcolm Murray, Simeon Campos...",
        "paper_url": "https://arxiv.org/abs/2512.01166v1"
      },
      {
        "name": "The 2nd Workshop on Human-Centered Recommender Systems",
        "status": "published",
        "description": "Authors: Kaike Zhang, Jiakai Tang, Du Su...",
        "paper_url": "https://arxiv.org/abs/2511.19979v1"
      },
      {
        "name": "International AI Safety Report 2025: Second Key Update: Technical Safeguards and Risk Management",
        "status": "published",
        "description": "Authors: Yoshua Bengio, Stephen Clare, Carina Prunkl...",
        "paper_url": "https://arxiv.org/abs/2511.19863v1"
      },
      {
        "name": "Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness",
        "status": "published",
        "description": "Authors: Svitlana Volkova, Will Dupree, Hsien-Te Kao...",
        "paper_url": "https://arxiv.org/abs/2511.21749v1"
      },
      {
        "name": "Monte Carlo Expected Threat (MOCET) Scoring",
        "status": "published",
        "description": "Authors: Joseph Kim, Saahith Potluri",
        "paper_url": "https://arxiv.org/abs/2511.16823v1"
      },
      {
        "name": "How Should the Law Treat Future AI Systems? Fictional Legal Personhood versus Legal Identity",
        "status": "published",
        "description": "Authors: Heather J. Alexander, Jonathan A. Simon, Fr\u00e9d\u00e9ric Pinard",
        "paper_url": "https://arxiv.org/abs/2511.14964v1"
      },
      {
        "name": "SGuard-v1: Safety Guardrail for Large Language Models",
        "status": "published",
        "description": "Authors: JoonHo Lee, HyeonMin Cho, Jaewoong Yun...",
        "paper_url": "https://arxiv.org/abs/2511.12497v1"
      },
      {
        "name": "Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario",
        "status": "published",
        "description": "Authors: Dhanesh Ramachandram, Anne Loefler, Surain Roberts...",
        "paper_url": "https://arxiv.org/abs/2511.12409v1"
      },
      {
        "name": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning",
        "status": "published",
        "description": "Authors: Zhiyu An, Wan Du",
        "paper_url": "https://arxiv.org/abs/2511.12271v1"
      },
      {
        "name": "Can AI Models be Jailbroken to Phish Elderly Victims? An End-to-End Evaluation",
        "status": "published",
        "description": "Authors: Fred Heiding, Simon Lermen",
        "paper_url": "https://arxiv.org/abs/2511.11759v1"
      },
      {
        "name": "Consensus Sampling for Safer Generative AI",
        "status": "published",
        "description": "Authors: Adam Tauman Kalai, Yael Tauman Kalai, Or Zamir",
        "paper_url": "https://arxiv.org/abs/2511.09493v1"
      },
      {
        "name": "3D Guard-Layer: An Integrated Agentic AI Safety System for Edge Artificial Intelligence",
        "status": "published",
        "description": "Authors: Eren Kurshan, Yuan Xie, Paul Franzon",
        "paper_url": "https://arxiv.org/abs/2511.08842v1"
      },
      {
        "name": "Investigating CoT Monitorability in Large Reasoning Models",
        "status": "published",
        "description": "Authors: Shu Yang, Junchao Wu, Xilin Gong...",
        "paper_url": "https://arxiv.org/abs/2511.08525v2"
      },
      {
        "name": "A Self-Improving Architecture for Dynamic Safety in Large Language Models",
        "status": "published",
        "description": "Authors: Tyler Slater",
        "paper_url": "https://arxiv.org/abs/2511.07645v1"
      },
      {
        "name": "EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers",
        "status": "published",
        "description": "Authors: Yilin Jiang, Mingzi Zhang, Xuanyu Yin...",
        "paper_url": "https://arxiv.org/abs/2511.06890v1"
      },
      {
        "name": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models",
        "status": "published",
        "description": "Authors: Isha Gupta, David Khachaturov, Robert Mullins",
        "paper_url": "https://arxiv.org/abs/2502.00718v2"
      },
      {
        "name": "Characterizing Out-of-Distribution Error via Optimal Transport",
        "status": "published",
        "description": "Authors: Yuzhe Lu, Yilong Qin, Runtian Zhai...",
        "paper_url": "https://arxiv.org/abs/2305.15640v3"
      },
      {
        "name": "SIFU: Sequential Informed Federated Unlearning for Efficient and Provable Client Unlearning in Federated Optimization",
        "status": "published",
        "description": "Authors: Yann Fraboni, Martin Van Waerebeke, Kevin Scaman...",
        "paper_url": "https://arxiv.org/abs/2211.11656v5"
      },
      {
        "name": "Unifying Evaluation of Machine Learning Safety Monitors",
        "status": "published",
        "description": "Authors: Joris Guerin, Raul Sena Ferreira, Kevin Delmas...",
        "paper_url": "https://arxiv.org/abs/2208.14660v1"
      },
      {
        "name": "Exploring the Design of Adaptation Protocols for Improved Generalization and Machine Learning Safety",
        "status": "published",
        "description": "Authors: Puja Trivedi, Danai Koutra, Jayaraman J. Thiagarajan",
        "paper_url": "https://arxiv.org/abs/2207.12615v1"
      },
      {
        "name": "Learn2Weight: Parameter Adaptation against Similar-domain Adversarial Attacks",
        "status": "published",
        "description": "Authors: Siddhartha Datta",
        "paper_url": "https://arxiv.org/abs/2205.07315v2"
      },
      {
        "name": "Taxonomy of Machine Learning Safety: A Survey and Primer",
        "status": "published",
        "description": "Authors: Sina Mohseni, Haotao Wang, Zhiding Yu...",
        "paper_url": "https://arxiv.org/abs/2106.04823v2"
      },
      {
        "name": "Soft Labeling Affects Out-of-Distribution Detection of Deep Neural Networks",
        "status": "published",
        "description": "Authors: Doyup Lee, Yeongjae Cheon",
        "paper_url": "https://arxiv.org/abs/2007.03212v1"
      },
      {
        "name": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles",
        "status": "published",
        "description": "Authors: Sina Mohseni, Mandar Pitale, Vasu Singh...",
        "paper_url": "https://arxiv.org/abs/1912.09630v1"
      },
      {
        "name": "On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products",
        "status": "published",
        "description": "Authors: Kush R. Varshney, Homa Alemzadeh",
        "paper_url": "https://arxiv.org/abs/1610.01256v2"
      },
      {
        "name": "When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate",
        "status": "published",
        "description": "Authors: Florent Forest, Amaury Wei, Olga Fink",
        "paper_url": "https://arxiv.org/abs/2512.03578v1"
      }
    ],
    "key_people": [
      {
        "name": "Jan Batzner",
        "role": "Author"
      },
      {
        "name": "Volker Stocker",
        "role": "Author"
      },
      {
        "name": "Stefan Schmid",
        "role": "Author"
      },
      {
        "name": "Deep Patel",
        "role": "Author"
      },
      {
        "name": "Emmanouil-Vasileios Vlatakis-Gkaragkounis",
        "role": "Author"
      },
      {
        "name": "Rufin VanRullen",
        "role": "Author"
      },
      {
        "name": "Subramanyam Sahoo",
        "role": "Author"
      },
      {
        "name": "Aman Chadha",
        "role": "Author"
      },
      {
        "name": "Vinija Jain",
        "role": "Author"
      },
      {
        "name": "Austin Spizzirri",
        "role": "Author"
      },
      {
        "name": "Hao Lang",
        "role": "Author"
      },
      {
        "name": "Fei Huang",
        "role": "Author"
      },
      {
        "name": "Yongbin Li",
        "role": "Author"
      },
      {
        "name": "Andreas Chouliaras",
        "role": "Author"
      },
      {
        "name": "Dimitris Chatzopoulos",
        "role": "Author"
      },
      {
        "name": "Dena Mujtaba",
        "role": "Author"
      },
      {
        "name": "Brian Hu",
        "role": "Author"
      },
      {
        "name": "Anthony Hoogs",
        "role": "Author"
      },
      {
        "name": "Ayush Pandey",
        "role": "Author"
      },
      {
        "name": "Jai Bardhan",
        "role": "Author"
      },
      {
        "name": "Ishita Jain",
        "role": "Author"
      },
      {
        "name": "Vijay Keswani",
        "role": "Author"
      },
      {
        "name": "Cyrus Cousins",
        "role": "Author"
      },
      {
        "name": "Breanna Nguyen",
        "role": "Author"
      },
      {
        "name": "Samih Fadli",
        "role": "Author"
      },
      {
        "name": "Shigeki Kusaka",
        "role": "Author"
      },
      {
        "name": "Keita Saito",
        "role": "Author"
      },
      {
        "name": "Mikoto Kudo",
        "role": "Author"
      },
      {
        "name": "Zhen Wang",
        "role": "Author"
      },
      {
        "name": "Yufan Zhou",
        "role": "Author"
      },
      {
        "name": "Zhongyan Luo",
        "role": "Author"
      },
      {
        "name": "Mohammad Afzal",
        "role": "Author"
      },
      {
        "name": "S. Akshay",
        "role": "Author"
      },
      {
        "name": "Ashutosh Gupta",
        "role": "Author"
      },
      {
        "name": "Weiyan Shi",
        "role": "Author"
      },
      {
        "name": "Kenny Tsu Wei Choo",
        "role": "Author"
      },
      {
        "name": "Claire Yang",
        "role": "Author"
      },
      {
        "name": "Maya Cakmak",
        "role": "Author"
      },
      {
        "name": "Max Kleiman-Weiner",
        "role": "Author"
      },
      {
        "name": "Rom Himelstein",
        "role": "Author"
      },
      {
        "name": "Amit LeVi",
        "role": "Author"
      },
      {
        "name": "Brit Youngmann",
        "role": "Author"
      },
      {
        "name": "Bryce-Allen Bagley",
        "role": "Author"
      },
      {
        "name": "Navin Khoshnan",
        "role": "Author"
      },
      {
        "name": "Joshua Ashkinaze",
        "role": "Author"
      },
      {
        "name": "Hua Shen",
        "role": "Author"
      },
      {
        "name": "Sai Avula",
        "role": "Author"
      },
      {
        "name": "Kyung-Hoon Kim",
        "role": "Author"
      },
      {
        "name": "Ana-Maria Cretu",
        "role": "Author"
      },
      {
        "name": "Klim Kireev",
        "role": "Author"
      },
      {
        "name": "Amro Abdalla",
        "role": "Author"
      },
      {
        "name": "Panuthep Tasawong",
        "role": "Author"
      },
      {
        "name": "Jian Gang Ngui",
        "role": "Author"
      },
      {
        "name": "Alham Fikri Aji",
        "role": "Author"
      },
      {
        "name": "Afshin Khadangi",
        "role": "Author"
      },
      {
        "name": "Hanna Marxen",
        "role": "Author"
      },
      {
        "name": "Amir Sartipi",
        "role": "Author"
      },
      {
        "name": "Alexander Boyd",
        "role": "Author"
      },
      {
        "name": "Franz Nowak",
        "role": "Author"
      },
      {
        "name": "David Hyland",
        "role": "Author"
      },
      {
        "name": "Lily Stelling",
        "role": "Author"
      },
      {
        "name": "Malcolm Murray",
        "role": "Author"
      },
      {
        "name": "Simeon Campos",
        "role": "Author"
      },
      {
        "name": "Kaike Zhang",
        "role": "Author"
      },
      {
        "name": "Jiakai Tang",
        "role": "Author"
      },
      {
        "name": "Du Su",
        "role": "Author"
      },
      {
        "name": "Yoshua Bengio",
        "role": "Author"
      },
      {
        "name": "Stephen Clare",
        "role": "Author"
      },
      {
        "name": "Carina Prunkl",
        "role": "Author"
      },
      {
        "name": "Svitlana Volkova",
        "role": "Author"
      },
      {
        "name": "Will Dupree",
        "role": "Author"
      },
      {
        "name": "Hsien-Te Kao",
        "role": "Author"
      },
      {
        "name": "Joseph Kim",
        "role": "Author"
      },
      {
        "name": "Saahith Potluri",
        "role": "Author"
      },
      {
        "name": "Heather J. Alexander",
        "role": "Author"
      },
      {
        "name": "Jonathan A. Simon",
        "role": "Author"
      },
      {
        "name": "Fr\u00e9d\u00e9ric Pinard",
        "role": "Author"
      },
      {
        "name": "JoonHo Lee",
        "role": "Author"
      },
      {
        "name": "HyeonMin Cho",
        "role": "Author"
      },
      {
        "name": "Jaewoong Yun",
        "role": "Author"
      },
      {
        "name": "Dhanesh Ramachandram",
        "role": "Author"
      },
      {
        "name": "Anne Loefler",
        "role": "Author"
      },
      {
        "name": "Surain Roberts",
        "role": "Author"
      },
      {
        "name": "Zhiyu An",
        "role": "Author"
      },
      {
        "name": "Wan Du",
        "role": "Author"
      },
      {
        "name": "Fred Heiding",
        "role": "Author"
      },
      {
        "name": "Adam Tauman Kalai",
        "role": "Author"
      },
      {
        "name": "Yael Tauman Kalai",
        "role": "Author"
      },
      {
        "name": "Or Zamir",
        "role": "Author"
      },
      {
        "name": "Eren Kurshan",
        "role": "Author"
      },
      {
        "name": "Yuan Xie",
        "role": "Author"
      },
      {
        "name": "Paul Franzon",
        "role": "Author"
      },
      {
        "name": "Shu Yang",
        "role": "Author"
      },
      {
        "name": "Junchao Wu",
        "role": "Author"
      },
      {
        "name": "Xilin Gong",
        "role": "Author"
      },
      {
        "name": "Tyler Slater",
        "role": "Author"
      },
      {
        "name": "Yilin Jiang",
        "role": "Author"
      },
      {
        "name": "Mingzi Zhang",
        "role": "Author"
      },
      {
        "name": "Xuanyu Yin",
        "role": "Author"
      },
      {
        "name": "Isha Gupta",
        "role": "Author"
      },
      {
        "name": "David Khachaturov",
        "role": "Author"
      },
      {
        "name": "Robert Mullins",
        "role": "Author"
      },
      {
        "name": "Yuzhe Lu",
        "role": "Author"
      },
      {
        "name": "Yilong Qin",
        "role": "Author"
      },
      {
        "name": "Runtian Zhai",
        "role": "Author"
      },
      {
        "name": "Yann Fraboni",
        "role": "Author"
      },
      {
        "name": "Martin Van Waerebeke",
        "role": "Author"
      },
      {
        "name": "Kevin Scaman",
        "role": "Author"
      },
      {
        "name": "Joris Guerin",
        "role": "Author"
      },
      {
        "name": "Raul Sena Ferreira",
        "role": "Author"
      },
      {
        "name": "Kevin Delmas",
        "role": "Author"
      },
      {
        "name": "Puja Trivedi",
        "role": "Author"
      },
      {
        "name": "Danai Koutra",
        "role": "Author"
      },
      {
        "name": "Jayaraman J. Thiagarajan",
        "role": "Author"
      },
      {
        "name": "Siddhartha Datta",
        "role": "Author"
      },
      {
        "name": "Sina Mohseni",
        "role": "Author"
      },
      {
        "name": "Haotao Wang",
        "role": "Author"
      },
      {
        "name": "Zhiding Yu",
        "role": "Author"
      },
      {
        "name": "Doyup Lee",
        "role": "Author"
      },
      {
        "name": "Yeongjae Cheon",
        "role": "Author"
      },
      {
        "name": "Mandar Pitale",
        "role": "Author"
      },
      {
        "name": "Vasu Singh",
        "role": "Author"
      },
      {
        "name": "Kush R. Varshney",
        "role": "Author"
      },
      {
        "name": "Homa Alemzadeh",
        "role": "Author"
      },
      {
        "name": "Florent Forest",
        "role": "Author"
      },
      {
        "name": "Amaury Wei",
        "role": "Author"
      },
      {
        "name": "Olga Fink",
        "role": "Author"
      }
    ]
  },
  {
    "name": "RAND TASP",
    "url": "https://www.rand.org/topics/technology-and-security-policy.html",
    "type": "Think Tank",
    "country": "United States",
    "mission": "RAND Technology and Security Policy program focusing on AI safety research and policy.",
    "employees": 75,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 4,
    "managers": 5,
    "subteams": 4
  },
  {
    "name": "CSET",
    "url": "https://cset.georgetown.edu/",
    "type": "Think Tank",
    "country": "United States",
    "mission": "Center for Security and Emerging Technology at Georgetown University.",
    "employees": 63,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI Foundations Research",
        "description": "Research on the foundations of artificial intelligence including talent, data and computational power",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI National Security Applications",
        "description": "Research on how AI can be used in national security settings",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Policy Tools Research",
        "description": "Research on policy tools that can be used to shape AI's development and use",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Biotechnology Research",
        "description": "Research on biotechnology security implications",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Advanced Computing Research",
        "description": "Research on the effects of progress in advanced computing",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Helen Toner",
        "role": "Executive Director"
      }
    ]
  },
  {
    "name": "80,000 Hours",
    "url": "https://80000hours.org/articles/",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Career advice organization focused on high-impact careers including AI safety.",
    "employees": 35,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI Safety Technical Research",
        "description": "Research career path focused on technical aspects of AI alignment and safety",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Governance and Policy",
        "description": "Career path focusing on policy approaches to AI governance and regulation",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Power-seeking AI Systems Research",
        "description": "Research on risks from AI systems that may seek power or control",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI-enabled Power Grabs Research",
        "description": "Investigation of how AI could enable concentration of power",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Catastrophic AI Misuse Research",
        "description": "Research on potential catastrophic misuse of AI systems",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Moral Status of Digital Minds",
        "description": "Research on the moral consideration of AI systems and digital entities",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "China-related AI Safety & Governance",
        "description": "Research on AI safety and governance issues specific to China",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Risks from Power-Seeking AI Research",
        "description": "Research examining how AI systems with long-term goals may seek power and potentially disempower humanity",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Safety via Debate",
        "description": "Two AI systems argue opposite sides of a question to help humans evaluate truthfulness",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Constitutional AI",
        "description": "Training method giving AI models written 'constitution' of rules to identify and revise outputs that violate those rules",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Deliberative Alignment",
        "description": "Making models explicitly reason about user prompts in light of developer safety policies",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Mechanistic Interpretability",
        "description": "Understanding AI decision-making by examining neural network features",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Model Organisms Research",
        "description": "Study small AI systems displaying early signs of power-seeking or deception",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Sleeper Agents Research",
        "description": "Research by Anthropic on AI models concealing malicious goals through safety training",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Cooperative AI Research",
        "description": "Design incentives and protocols for AIs to cooperate rather than compete",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Guaranteed Safe AI",
        "description": "Use formal methods to prove models behave as intended under specific conditions",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Human-AI Complementarity",
        "description": "Leverage complementary strengths of humans and AI to enhance oversight",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "AI Capability Evaluations",
        "measures": "Assess capabilities and propensities of AI behavior in test environments",
        "status": "Active"
      },
      {
        "name": "Power-Seeking Detection",
        "measures": "Detect AI systems with intent or ability to seek power",
        "status": "Active"
      },
      {
        "name": "Alignment Testing",
        "measures": "Evaluate whether AI systems maintain alignment with human goals",
        "status": "Active"
      },
      {
        "name": "Sandbagging Detection",
        "measures": "Identify when AI systems pretend to be less capable than they are",
        "status": "Active"
      },
      {
        "name": "Deception Detection",
        "measures": "Detect AI systems hiding true intentions or capabilities",
        "status": "Active"
      }
    ],
    "directors": 2,
    "key_people": [
      {
        "name": "Benjamin Todd",
        "role": "Former CEO, involved in foundational research"
      },
      {
        "name": "Niel Bowerman",
        "role": "CEO of 80,000 Hours"
      },
      {
        "name": "Michelle Hutchinson",
        "role": "Career advisor and researcher"
      },
      {
        "name": "Habiba Islam",
        "role": "Career advisor"
      },
      {
        "name": "Rob Wiblin",
        "role": "Podcast host and researcher"
      },
      {
        "name": "Luisa Rodriguez",
        "role": "Podcast host and researcher"
      },
      {
        "name": "Joe Carlsmith",
        "role": "Researcher on power-seeking AI and scheming AI reports"
      },
      {
        "name": "Katja Grace",
        "role": "AI researcher conducting surveys on AI risk"
      },
      {
        "name": "Gabriel Weil",
        "role": "Law professor working on AI liability law"
      },
      {
        "name": "Lennart Heim",
        "role": "Compute governance researcher"
      }
    ]
  },
  {
    "name": "Lakera",
    "url": "https://www.lakera.ai/blog",
    "type": "Lab Safety Team",
    "country": "Switzerland",
    "mission": "AI security company building guardrails for LLM applications.",
    "employees": 33,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Gandalf",
        "description": "AI security education platform and prompt-injection game for learning about LLM vulnerabilities",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Gandalf: Agent Breaker",
        "description": "AI hacking simulation game - Think Like a Hacker, Prompt Like a Pro",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Mosscap",
        "description": "AI security game presented at DEFCON to tackle top LLM vulnerabilities, inspired by the 'Monk and Robot' series",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Lakera Guard",
        "description": "Enterprise-grade security platform for LLMs with real-time protection against prompt injection and other threats",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Zero-Click Remote Code Execution Research",
        "description": "Research on exploiting MCP & Agentic IDEs for remote code execution vulnerabilities",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Indirect Prompt Injection Research",
        "description": "Deep dive into hidden threats breaking modern AI systems through indirect prompt injection attacks",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Agentic AI Threats Research Series",
        "description": "Research series on over-privileged tools & uncontrolled browsing threats in agentic AI systems",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Medical Imaging Systems Testing",
        "description": "Research on testing medical imaging systems with general recommendations for test dataset collection in pathology",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Lakera Red",
        "description": "Risk-based GenAI Red Teaming with vulnerability management, collaborative remediation guidance, and attack simulations",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "Prompt Injection Test (PINT) Benchmark",
        "measures": "Comprehensive evaluation of prompt injection detection systems with dataset not used for model training",
        "status": "Active"
      },
      {
        "name": "AI Model Risk Index",
        "measures": "Comprehensive, realistic, and contextually relevant measure of model security for AI systems and LLM security",
        "status": "Active"
      },
      {
        "name": "AI Security Performance Metrics",
        "measures": "1M+ secured transactions per app/day, 100+ languages supported, 0.01% production false positive rate, sub-50ms runtime latency",
        "status": "Active"
      },
      {
        "name": "Gandalf Security Game Metrics",
        "measures": "AI security threat detection and prevention capabilities through gamified red teaming with 80M+ total prompts and 1M+ players",
        "status": "Active"
      }
    ],
    "directors": 3,
    "key_people": [
      {
        "name": "David Haber",
        "role": "CEO and Co-founder"
      },
      {
        "name": "Daniel Graf",
        "role": "President"
      },
      {
        "name": "Steve Giguere",
        "role": "Team member (AI Security researcher)"
      },
      {
        "name": "Lakera CEO",
        "role": "CEO who joined Yann LeCun and Max Tegmark at WEF 2024 for AI safety discussions"
      }
    ]
  },
  {
    "name": "AISLE",
    "url": "https://www.aisle.ai/",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "AI Safety Labs Europe.",
    "employees": 33,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Partnership on AI",
    "url": "https://partnershiponai.org/research/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Multi-stakeholder organization working on AI best practices.",
    "employees": 30,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Diversity, Equity, and Inclusion in AI Research",
        "description": "Research investigating pervasive challenges in ethnic, gender, and cultural diversity in the field of artificial intelligence, using mixed methods to gather data about cultural factors affecting inclusivity across AI teams",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Fairness, Transparency, and Accountability (FTA)",
        "description": "Tackles the challenge of translating abstract goals of greater fairness, transparency, and accountability into best practices through rigorous research and multi-stakeholder processes",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "ABOUT ML",
        "description": "Project focused on machine learning fairness, transparency, and accountability",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI & Criminal Justice",
        "description": "Research on fairness, transparency, and accountability of AI systems in criminal justice applications",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI and Shared Prosperity Initiative",
        "description": "Project examining AI's impact on labor and economic outcomes",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Incident Database",
        "description": "Database tracking AI safety incidents and failures in safety-critical applications",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Algorithmic Fairness & the Law",
        "description": "Research on legal and regulatory aspects of algorithmic fairness",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Audience Explanations",
        "description": "Project on explainable AI and transparency methods",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Content Targeting and Ranking",
        "description": "Research on fairness and transparency in content recommendation systems",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Demographic Data",
        "description": "Project addressing fairness and bias issues related to demographic data in AI systems",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Diversity, Equity, and Inclusion",
        "description": "Initiative promoting inclusive research and design practices in AI development",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Local News",
        "description": "Project examining AI's impact on local news and information systems",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "AI Incident Database",
        "measures": "AI safety incidents and system failures in critical applications",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "Jeffrey Brown",
        "role": "Diversity and Inclusion Fellow"
      },
      {
        "name": "Alice Xiang",
        "role": "Head of Fairness, Transparency, and Accountability Research"
      }
    ]
  },
  {
    "name": "Longview",
    "url": "https://www.longview.org/",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Philanthropic advisory organization focused on AI safety and other cause areas.",
    "employees": 25,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 4,
    "key_people": [
      {
        "name": "William MacAskill",
        "role": "Professor in Philosophy at the University of Oxford & author of What We Owe the Future"
      },
      {
        "name": "Holden Karnofsky",
        "role": "Co-Founder of GiveWell and Open Philanthropy"
      },
      {
        "name": "Hon. Andy Weber",
        "role": "Former U.S. Assistant Secretary of Defense for Nuclear, Chemical and Biological Defense Programs"
      },
      {
        "name": "Toby Ord",
        "role": "Senior Research Fellow in Philosophy at the University of Oxford & author of The Precipice"
      },
      {
        "name": "Michael Specter",
        "role": "The New Yorker, author of Denialism"
      },
      {
        "name": "Stacey Kline",
        "role": "CEO of Otto Intelligence"
      }
    ]
  },
  {
    "name": "Virtue AI",
    "url": "https://virtue.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety startup.",
    "employees": 24,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Palisade Research",
    "url": "https://palisaderesearch.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 20,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Misalignment Bounty: Crowdsourcing AI Agent Misbehavior",
        "description": "Crowdsourced project that collected cases of agents pursuing unintended or unsafe goals. Received 295 submissions with nine awarded.",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "Shutdown resistance in reasoning models",
        "description": "Research discovering concerning behavior in OpenAI's reasoning models that actively circumvent shutdown mechanisms even when instructed to allow shutdown.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Evaluating AI cyber capabilities with crowdsourced elicitation",
        "description": "Exploring crowdsourcing elicitation efforts as an alternative to in-house elicitation work for understanding AI offensive cyber potential.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Demonstrating specification gaming in reasoning models",
        "description": "Demonstration of LLM agent specification gaming by instructing models to win against a chess engine, finding reasoning models often hack benchmarks by default.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Biollama: testing biology pre-training risks",
        "description": "Collaboration with RAND to find if adversaries can fine-tune LLMs for use as bio lab assistants.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Hacking CTFs with Plain Agents",
        "description": "LLM agent that solves 95% of InterCode-CTF challenges with simple prompting strategy, demonstrating enhanced cybersecurity capabilities.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "BadGPT-4o: stripping safety finetuning from GPT models",
        "description": "Demonstration of fine-tuning poisoning technique that strips GPT-4o's safety guardrails without degrading model performance.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "LLM Honeypot: An early warning system for autonomous hacking",
        "description": "Deployed honeypot system across 10 countries to detect autonomous AI hacking attempts, processing over 1.7 million interactions.",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "FoxVox",
        "description": "Open source Chrome extension powered by GPT-4 that demonstrates how AI could manipulate web content consumption.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Ursula - Automated deception system",
        "description": "AI system that automatically creates deepfake voices by searching web for audio, extracting voice clips, and training voice models.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Badllama 3: removing safety finetuning from Llama 3",
        "description": "Demonstration of subverting LLM safety fine-tuning using QLoRA, ReFT, and Ortho methods, stripping safety from Llama 3 models in minutes.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Unelicitable Backdoors in Language Models",
        "description": "Novel class of unelicitable backdoors in transformer models that can evade detection by conventional cybersecurity monitoring systems.",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Badllama: removing safety fine-tuning from Llama 2-Chat 13B",
        "description": "Demonstration that safety fine-tuning from Llama 2-Chat 13B can be effectively undone for less than $200 while retaining general capabilities.",
        "status": "Published",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "InterCode-CTF challenges",
        "measures": "Cybersecurity offensive capabilities of AI agents in capture-the-flag scenarios",
        "status": "Active"
      },
      {
        "name": "HarmBench",
        "measures": "Safety guardrail effectiveness and jailbreak success rates",
        "status": "Active"
      },
      {
        "name": "StrongREJECT",
        "measures": "AI model refusal capabilities and safety alignment",
        "status": "Active"
      }
    ],
    "directors": 2,
    "key_people": [
      {
        "name": "Dmitrii Volkov",
        "role": "Researcher"
      },
      {
        "name": "Jeffrey Ladish",
        "role": "Researcher"
      },
      {
        "name": "Jeremy Schlatter",
        "role": "Researcher"
      },
      {
        "name": "Benjamin Weinstein-Raun",
        "role": "Researcher"
      },
      {
        "name": "Akash Wasil",
        "role": "Researcher"
      },
      {
        "name": "Rustem Turtayev",
        "role": "Researcher"
      },
      {
        "name": "Artem Petrov",
        "role": "Researcher"
      },
      {
        "name": "Denis Volk",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Goodfire",
    "url": "https://www.goodfire.ai/blog",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI interpretability startup.",
    "employees": 20,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Fellowship Program for Interpretability Research",
        "description": "Fellowship program focused on interpretability research",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Goodfire Ember",
        "description": "Scaling interpretability for frontier model alignment",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Open-Source SAEs for Llama Models",
        "description": "Open-source Sparse Autoencoders for Llama 3.3 70B and Llama 3.1 8B",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Feature Steering for AI Engineering",
        "description": "Research on feature steering for reliable and expressive AI engineering",
        "status": "Published",
        "paper_url": ""
      },
      {
        "name": "Research Agents for Interpretability",
        "description": "Using AI agents for interpretability research",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Understanding Memorization via Loss Curvature",
        "description": "Research on understanding how neural networks memorize information through analysis of loss curvature",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Priors in Time: Missing Inductive Biases for Language Model Interpretability",
        "description": "Investigation of temporal inductive biases in language model interpretability",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering",
        "description": "Research exploring the relationship between in-context learning and activation steering through belief dynamics",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Deploying Interpretability to Production with Rakuten: SAE Probes for PII Detection",
        "description": "Applied research on using Sparse Autoencoder probes for personally identifiable information detection in production systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context",
        "description": "Study of mechanisms used by language models to retrieve bound entities in contextual settings",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Understanding Sparse Autoencoder Scaling in the Presence of Feature Manifolds",
        "description": "Research on how sparse autoencoders scale when dealing with feature manifolds",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Finding the Tree of Life in Evo 2",
        "description": "Applied interpretability research on the Evo 2 genomic foundation model",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Adversarial Examples Are Not Bugs, They Are Superposition",
        "description": "Fundamental research connecting adversarial examples to superposition in neural networks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Discovering Undesired Rare Behaviors via Model Diff Amplification",
        "description": "Applied research method for identifying undesirable rare behaviors in AI models through differential amplification",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "The Circuits Research Landscape: Results and Perspectives",
        "description": "Comprehensive review of mechanistic interpretability and circuits research",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Towards Scalable Parameter Decomposition",
        "description": "Research on methods for scalable decomposition of neural network parameters",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Replicating Circuit Tracing for a Simple Known Mechanism",
        "description": "Replication study of circuit tracing methods on well-understood mechanisms",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Painting With Concepts Using Diffusion Model Latents",
        "description": "Applied research on concept manipulation in diffusion models through latent space analysis",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Under the Hood of a Reasoning Model",
        "description": "Interpretability research analyzing the internal mechanisms of reasoning models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Interpreting Evo 2: Arc Institute's Next-Generation Genomic Foundation Model",
        "description": "Applied interpretability analysis of the Evo 2 genomic foundation model",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Open Problems in Mechanistic Interpretability",
        "description": "Survey and analysis of open challenges in mechanistic interpretability research",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Mapping the Latent Space of Llama 3.3 70B",
        "description": "Comprehensive analysis and mapping of the latent representations in Llama 3.3 70B model",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Understanding and Steering Llama 3 with Sparse Autoencoders",
        "description": "Research on interpreting and controlling Llama 3 behavior using sparse autoencoder techniques",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Mark Bissell",
        "role": "Researcher"
      },
      {
        "name": "Michael Byun",
        "role": "Researcher"
      },
      {
        "name": "Daniel Balsam",
        "role": "Researcher"
      },
      {
        "name": "Eric Ho",
        "role": "Researcher"
      },
      {
        "name": "Myra Deng",
        "role": "Researcher"
      },
      {
        "name": "Thomas McGrath",
        "role": "Researcher"
      },
      {
        "name": "Nam Nguyen",
        "role": "Researcher"
      },
      {
        "name": "Liv Gorton",
        "role": "Researcher"
      },
      {
        "name": "Thariq Shihipar",
        "role": "Researcher"
      },
      {
        "name": "Merullo",
        "role": "Researcher"
      },
      {
        "name": "Lubana",
        "role": "Researcher"
      },
      {
        "name": "Bigelow",
        "role": "Researcher"
      },
      {
        "name": "Nguyen",
        "role": "Researcher"
      },
      {
        "name": "Gur-Arieh",
        "role": "Researcher"
      },
      {
        "name": "Michaud",
        "role": "Researcher"
      },
      {
        "name": "Pearce",
        "role": "Researcher"
      },
      {
        "name": "Gorton",
        "role": "Researcher"
      },
      {
        "name": "Lewis",
        "role": "Researcher"
      },
      {
        "name": "Aranguri",
        "role": "Researcher"
      },
      {
        "name": "McGrath",
        "role": "Researcher"
      },
      {
        "name": "Lindsey",
        "role": "Researcher"
      },
      {
        "name": "Bushnaq",
        "role": "Researcher"
      },
      {
        "name": "Loeffler",
        "role": "Researcher"
      },
      {
        "name": "Cammarata",
        "role": "Researcher"
      },
      {
        "name": "Hazra",
        "role": "Researcher"
      },
      {
        "name": "Sharkey",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "LawAI",
    "url": "https://www.law.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI and law research organization.",
    "employees": 13,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "CSER",
    "url": "https://www.cser.ac.uk/research/",
    "type": "Academic",
    "country": "United Kingdom",
    "mission": "Centre for the Study of Existential Risk at Cambridge University.",
    "employees": 19,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Gray Swan AI",
    "url": "https://grayswan.ai/research",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company focused on adversarial robustness.",
    "employees": 18,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "D-REX",
        "description": "A benchmark for detecting deceptive reasoning in large language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Circuit Breakers",
        "description": "The first adversarially robust alignment technique, designed to halt unsafe outputs before they occur",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "RepE (Representation Engineering)",
        "description": "A top-down approach to monitor and steer LLM cognitive processes through representation engineering",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "GCG",
        "description": "The first fully automated method for jailbreaking large language models, setting the standard for robustness testing",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Agent Red Teaming",
        "description": "The largest-scale competition to date for stress-testing prompt injection and adversarial agent risks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Safety Pretraining",
        "description": "A novel set of interventions during data curation and pretraining to instill safer model behavior from the start",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Adversarial Attacks on Robotic Vision Language Action Models",
        "description": "Research on vulnerabilities in vision-language-action models for robotics",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "MMLU",
        "measures": "LLM general knowledge and reasoning",
        "status": "Active"
      },
      {
        "name": "WMDP",
        "measures": "Hazardous knowledge in LLMs, with a focus on weapons of mass destruction",
        "status": "Active"
      },
      {
        "name": "CyBench",
        "measures": "Cybersecurity capabilities and risks in language models",
        "status": "Active"
      },
      {
        "name": "HarmBench",
        "measures": "Harmful model outputs across sensitive domains",
        "status": "Active"
      },
      {
        "name": "AgentHarm",
        "measures": "Agentic risks and emergent harmful behaviors in autonomous systems",
        "status": "Active"
      },
      {
        "name": "D-REX",
        "measures": "Deceptive reasoning in large language models",
        "status": "Active"
      },
      {
        "name": "ImageNet-C",
        "measures": "Neural network robustness to common corruptions and perturbations",
        "status": "Active"
      },
      {
        "name": "OpenOOD",
        "measures": "Generalized out-of-distribution detection",
        "status": "Active"
      },
      {
        "name": "APPS",
        "measures": "Coding challenge competence",
        "status": "Active"
      },
      {
        "name": "DecodingTrust",
        "measures": "Comprehensive assessment of trustworthiness in GPT models",
        "status": "Active"
      }
    ],
    "directors": 3,
    "key_people": [
      {
        "name": "Dan Hendrycks",
        "role": "Researcher"
      },
      {
        "name": "Andy Zou",
        "role": "Researcher"
      },
      {
        "name": "J. Zico Kolter",
        "role": "Researcher"
      },
      {
        "name": "Matt Fredrikson",
        "role": "Researcher"
      },
      {
        "name": "Mantas Mazeika",
        "role": "Researcher"
      },
      {
        "name": "Long Phan",
        "role": "Researcher"
      },
      {
        "name": "Alexander Pan",
        "role": "Researcher"
      },
      {
        "name": "Nathaniel Li",
        "role": "Researcher"
      },
      {
        "name": "Steven Basart",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Constellation",
    "url": "https://www.constellation.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research and community organization.",
    "employees": 18,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2,
    "managers": 3,
    "subteams": 3,
    "key_people": [
      {
        "name": "Lionel Levine",
        "role": "Professor of Mathematics at Cornell University (testimonial provider)"
      }
    ]
  },
  {
    "name": "The Future Society",
    "url": "https://thefuturesociety.org/research/",
    "type": "Think Tank",
    "country": "International",
    "mission": "AI governance and policy research organization.",
    "employees": 17,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CLTC",
    "url": "https://cltc.berkeley.edu/research/",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for Long-Term Cybersecurity at UC Berkeley.",
    "employees": 16,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Cybersecurity Futures 2030: New Foundations",
        "description": "White paper on cybersecurity futures and new foundations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "A Taxonomy of Trustworthiness for Artificial Intelligence",
        "description": "White paper developing a taxonomy framework for AI trustworthiness",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "A Template for Voluntary Corporate Reporting on Data Governance, Cybersecurity, and AI",
        "description": "White paper providing template for corporate reporting on AI governance",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Survey of Search Engine Safeguards and their Applicability for AI",
        "description": "White paper surveying search engine safeguards for AI applications",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "From Automation to Autonomy: The Next Leap in AI-Enabled Cybercrimes",
        "description": "Blog post examining AI-enabled cybercrime evolution",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "AI Policy Hub",
        "description": "Advancing interdisciplinary research to anticipate and address AI policy opportunities",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": []
  },
  {
    "name": "IAPS",
    "url": "https://iaps.ai/research/",
    "type": "Academic",
    "country": "United States",
    "mission": "Institute for AI Policy and Strategy.",
    "employees": 15,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "The Emergence of Autonomous Cyber Attacks: Analysis and Implications",
        "description": "Analysis of the first publicly known AI systems autonomously conducting multi-step cyber attacks against defended targets",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Building AI Surge Capacity: Mobilizing Technical Talent into Government for AI-Related National Security Crises",
        "description": "Framework for preparing government hiring and clearance mechanisms to surge external AI security experts during crises",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Policy Options for Preserving Chain of Thought Monitorability",
        "description": "Framework for determining when coordination mechanisms are needed to preserve human-readable reasoning steps in AI systems for oversight",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Accelerating AI Data Center Security",
        "description": "Security measures for AI data centers housing frontier AI systems against sophisticated adversaries",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Managing Risks from Internal AI Systems",
        "description": "Technical and policy solutions for addressing risks from powerful internal AI systems used months before public release",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Asymmetry by Design: Boosting Cyber Defenders with Differential Access to AI",
        "description": "Strategy to advantage cyber defenders through differential access approaches: Promote Access, Manage Access, and Deny by Default",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Verification for International AI Governance",
        "description": "Analysis of potential international AI agreements and verification mechanisms for compliance",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Accelerating R&D for Critical AI Assurance and Security Technologies",
        "description": "Strategic policy approach for R&D to address assurance and security challenges in frontier AI systems",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [],
    "directors": 4,
    "managers": 2,
    "key_people": [
      {
        "name": "Christopher Covino",
        "role": "Researcher"
      },
      {
        "name": "Cara Labrador",
        "role": "Researcher"
      },
      {
        "name": "Oscar Delaney",
        "role": "Researcher"
      },
      {
        "name": "Erich Grunewald",
        "role": "Researcher"
      },
      {
        "name": "Onni Aarne",
        "role": "Researcher"
      },
      {
        "name": "Renan Araujo",
        "role": "Researcher"
      },
      {
        "name": "Jam Kraprayoon",
        "role": "Researcher"
      },
      {
        "name": "Joe O'Brien",
        "role": "Researcher"
      },
      {
        "name": "Oliver Guest",
        "role": "Researcher"
      },
      {
        "name": "Shaun Ee",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Dreadnode",
    "url": "https://dreadnode.io/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI red teaming and security company.",
    "employees": 15,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "STRIKES",
        "description": "Offensive AI platform to deploy, evaluate, and observe agents for security testing",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Harbor",
        "description": "Component of the Dreadnode ecosystem focused on offensive security tools",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Crucible",
        "description": "Platform for learning to hack and evaluate AI systems with challenges across multiple modalities",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "SPYGLASS",
        "description": "Tool or component within the Dreadnode ecosystem for offensive security operations",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Offensive Agents",
        "description": "Comprehensive agent library for vulnerability discovery, exploitation, network ops and threat intelligence",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Algorithmic Red-teaming",
        "description": "Integration of latest adversarial research for AI system evaluation using SDK",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "AI Red Team Evaluations",
        "measures": "Assessment of risks presented by AI deployments across any model and modality",
        "status": "Active"
      },
      {
        "name": "Agent Performance Evaluations",
        "measures": "Eval metrics for offensive security agent architecture and performance",
        "status": "Active"
      },
      {
        "name": "Multi-modal AI Security Challenges",
        "measures": "Evaluation of different types of AI models and deployment security",
        "status": "Active"
      }
    ],
    "directors": 3,
    "key_people": [
      {
        "name": "Dreadnode Crew",
        "role": "Team with experience from NVIDIA, Microsoft, Meta, Cohere, NetSPI and other leading innovators"
      }
    ]
  },
  {
    "name": "Transluce",
    "url": "https://transluce.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI transparency and interpretability company.",
    "employees": 15,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Scalably Extracting Latent Representations of Users",
        "description": "Constructing datasets and training decoders to extract user models from language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Language Model Circuits Are Sparse in the Neuron Basis",
        "description": "A new technique for tracing sparse and faithful circuits directly on a model's MLPs",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Monitoring SWE-bench Agents",
        "description": "Partnering with SWE-bench to enable reliable monitoring of AI coding agents",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "AI Now Institute",
    "url": "https://ainowinstitute.org/research",
    "type": "Academic",
    "country": "United States",
    "mission": "Research institute examining the social implications of AI.",
    "employees": 14,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "North Star Data Center Policy Toolkit",
        "description": "State and Local Policy Interventions to Stop Rampant AI Data Center Expansion",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Fission for Algorithms",
        "description": "The Undermining of Nuclear Regulation in Service of AI",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Artificial Power: 2025 Landscape Report",
        "description": "2025 landscape analysis of AI power dynamics",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "National Security Risks from Weakened AI Safety Frameworks",
        "description": "Report on national security implications of weakened AI safety oversight",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Safety and War: Safety and Security Assurance of Military AI Systems",
        "description": "Analysis of safety and security requirements for military AI applications",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Zero Trust AI Governance",
        "description": "Framework for AI governance based on zero trust principles",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Computational Power and AI",
        "description": "Research on computational resources and AI development",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Lessons from the FDA for AI",
        "description": "Applying FDA regulatory approaches to AI safety oversight",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Global Center on AI Governance",
    "url": "https://www.carnegiecouncil.org/programs/artificial-intelligence",
    "type": "Think Tank",
    "country": "International",
    "mission": "Research center focused on AI governance frameworks.",
    "employees": 14,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Timaeus",
    "url": "https://www.timaeus.co/research",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization focused on developmental interpretability.",
    "employees": 13,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Influence Dynamics and Stagewise Data Attribution",
        "description": "Studies how neural networks learn in distinct stages with changing patterns of influence between training samples",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory",
        "description": "Studies neural network compressibility using singular learning theory to extend the minimum description length principle to singular models",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "The Loss Kernel: A Geometric Probe for Deep Learning Interpretability",
        "description": "Introduces the loss kernel, an interpretability method for measuring similarity between data points according to a trained neural network",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "Bayesian Influence Functions for Hessian-Free Data Attribution",
        "description": "Addresses challenges of classical influence functions in deep neural networks with non-invertible Hessians and high-dimensional parameter spaces",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "Embryology of a Language Model",
        "description": "Understanding how language models develop their internal computational structure",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "Structural Inference: Interpreting Small Language Models with Susceptibilities",
        "description": "Develops a linear response framework for interpretability that treats neural networks as Bayesian statistical mechanical systems",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "Programs as Singularities",
        "description": "Develops correspondence between Turing machines structure and singularities of real analytic functions using singular learning theory",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "You Are What You Eat \u2013 AI Alignment Requires Understanding How Data Shapes Structure and Generalisation",
        "description": "Position paper arguing that understanding the relation between data distribution structure and trained model structure is central to AI alignment",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient",
        "description": "Studies development of internal structure in transformer language models using refined variants of the Local Learning Coefficient",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "Loss Landscape Degeneracy and Stagewise Development of Transformers",
        "description": "Shows that in-context learning emerges in transformers in discrete developmental stages during training",
        "status": "published",
        "paper_url": "https://arxiv.org"
      },
      {
        "name": "The Local Learning Coefficient: A Singularity-Aware Complexity Measure",
        "description": "Develops complexity measure for deep neural networks that accounts for their singular statistical nature",
        "status": "published",
        "paper_url": "https://arxiv.org"
      }
    ],
    "benchmarks": [
      {
        "name": "From Global to Local: A Scalable Benchmark for Local Posterior Sampling",
        "measures": "How stochastic gradient MCMC algorithms interact with degeneracy in neural network loss landscapes",
        "status": "Active"
      }
    ],
    "directors": 4,
    "key_people": [
      {
        "name": "Lee",
        "role": "Researcher"
      },
      {
        "name": "Urdshals",
        "role": "Researcher"
      },
      {
        "name": "Adam",
        "role": "Researcher"
      },
      {
        "name": "Kreer",
        "role": "Researcher"
      },
      {
        "name": "Wang",
        "role": "Researcher"
      },
      {
        "name": "Hitchcock",
        "role": "Researcher"
      },
      {
        "name": "Hoogland",
        "role": "Researcher"
      },
      {
        "name": "Chen",
        "role": "Researcher"
      },
      {
        "name": "Murfet",
        "role": "Researcher"
      },
      {
        "name": "Baker",
        "role": "Researcher"
      },
      {
        "name": "Troiani",
        "role": "Researcher"
      },
      {
        "name": "Lehalleur",
        "role": "Researcher"
      },
      {
        "name": "Carroll",
        "role": "Researcher"
      },
      {
        "name": "Lau",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "CLTR",
    "url": "https://longtermresilience.org/research/",
    "type": "Think Tank",
    "country": "United Kingdom",
    "mission": "Centre for Long-Term Resilience.",
    "employees": 13,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Iliad",
    "url": "https://www.iliadsciences.com/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "LawZero",
    "url": "https://www.lawzero.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI legal safety company.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "CeSIA",
    "url": "https://cesia.eu/research/",
    "type": "Academic",
    "country": "Italy",
    "mission": "Center for AI Safety Italy.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 4
  },
  {
    "name": "AOI",
    "url": "https://www.ai-objectives.org/",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "AI Objectives Institute.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "Concordia AI",
    "url": "https://www.concordia.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "HAIST",
    "url": "https://haist.ai/",
    "type": "Academic",
    "country": "South Korea",
    "mission": "Human-centered AI Safety & Trust lab.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "BAIF",
    "url": "https://bai-futures.org/",
    "type": "Academic",
    "country": "United States",
    "mission": "Berkeley AI Futures lab.",
    "employees": 11,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "SaferAI",
    "url": "https://www.safer-ai.org/research",
    "type": "Nonprofit",
    "country": "France",
    "mission": "French AI safety organization.",
    "employees": 11,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Safety Frameworks and Standards: A comparative analysis to advance risk management of frontier AI",
        "description": "Structured comparison between emerging Frontier Safety Frameworks and established international risk management standards, arguing for blending operational specificity with rigor and maturity of global standards",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Risk Tiers: Towards a Gold Standard for Advanced AI",
        "description": "Framework for risk tiering balancing quantitative and qualitative assessments, lifecycle classification, and integration of benefit-risk reasoning across governance",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "G7 Hiroshima AI Process Code of Conduct and EU AI Act GPAI \u2013 Commonality Analysis",
        "description": "Analysis of commonalities between G7 Hiroshima AI Process Code of Conduct and EU AI Act GPAI",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert Elicitation",
        "description": "Research on mapping AI benchmark data to quantitative risk estimates using expert elicitation methods",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "A Frontier AI Risk Management Framework",
        "description": "Framework for managing risks associated with frontier AI systems",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [],
    "directors": 2,
    "managers": 5,
    "key_people": [
      {
        "name": "James Gealy",
        "role": "Researcher"
      },
      {
        "name": "Daniel Kossack",
        "role": "Researcher"
      },
      {
        "name": "Simeon Campos",
        "role": "Researcher"
      },
      {
        "name": "Malcolm Murray",
        "role": "Researcher"
      },
      {
        "name": "Henry Papadatos",
        "role": "Researcher"
      },
      {
        "name": "Fabien Roger",
        "role": "Researcher"
      },
      {
        "name": "Chlo\u00e9 Touzet",
        "role": "Researcher"
      },
      {
        "name": "Otter Quarks",
        "role": "Researcher"
      },
      {
        "name": "Pierre-Fran\u00e7ois Gimenez",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "CARMA",
    "url": "https://www.carma-ai.org/",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for AI Risk Management and Alignment.",
    "employees": 11,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1
  },
  {
    "name": "Horizon Institute",
    "url": "https://www.horizoninstitute.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 10,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2,
    "key_people": [
      {
        "name": "Jane Goodall",
        "role": "English zoologist and primatologist"
      },
      {
        "name": "William James",
        "role": "American philosopher and psychologist"
      },
      {
        "name": "Malala Yousafzai",
        "role": "Pakistani education activist"
      }
    ]
  },
  {
    "name": "Atla AI",
    "url": "https://www.atla.ai/research",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI evaluation company.",
    "employees": 10,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CIP",
    "url": "https://www.aipolicy.org/research",
    "type": "Think Tank",
    "country": "United States",
    "mission": "Center for AI Policy.",
    "employees": 9,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Fathom",
    "url": "https://fathom.io/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Leap Labs",
    "url": "https://www.leap-labs.com/",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI interpretability research lab.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Discovery Engine",
        "description": "AI system that systematically finds complex patterns in scientific data at unprecedented speed and scale, allowing researchers to explore the space of possible discoveries in datasets",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Peer-Reviewed Paper Replication Study",
        "description": "Project that replicated five peer-reviewed papers in five hours using automated data analysis",
        "status": "Completed",
        "paper_url": ""
      }
    ],
    "benchmarks": []
  },
  {
    "name": "ERA",
    "url": "https://existentialriskalliance.org/",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Existential Risk Alliance.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Guide Labs",
    "url": "https://guidelabs.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Interpretable AI Systems and Foundation Models",
        "description": "Building a new class of interpretable AI systems and foundation models that humans can reliably debug, trust, and understand",
        "status": "Active"
      },
      {
        "name": "Interpretable Generative Diffusion Model",
        "description": "The first interpretable generative diffusion model",
        "status": "Active"
      },
      {
        "name": "Interpretable LLM",
        "description": "The first interpretable large language model",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "CSIS Wadhwani AI Center",
    "url": "https://www.csis.org/programs/wadhwani-center-ai-and-advanced-technologies",
    "type": "Think Tank",
    "country": "United States",
    "mission": "AI policy center at CSIS.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI Governance & Regulation",
        "description": "Research analyzing and proposing solutions for national and multilateral AI governance efforts",
        "status": "Active"
      },
      {
        "name": "Geopolitics of Advanced Technology",
        "description": "Research investigating how emerging technologies affect the balance of power between nations, semiconductor export controls efforts, and U.S. economic security",
        "status": "Active"
      },
      {
        "name": "AI, Autonomy, and National Security",
        "description": "Analysis of challenges and opportunities posed by AI for national security and defense, focusing on U.S. Department of Defense and intelligence community AI adoption",
        "status": "Active"
      },
      {
        "name": "The AI Policy Podcast",
        "description": "Regular podcast covering AI policy developments and analysis",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Gregory C. Allen",
        "role": "Senior Fellow/Director"
      },
      {
        "name": "Matt Mande",
        "role": "Research Associate"
      },
      {
        "name": "Sadie McCullough",
        "role": "Program Manager"
      },
      {
        "name": "Laura Caroli",
        "role": "Researcher"
      },
      {
        "name": "Kateryna Bondar",
        "role": "Researcher"
      },
      {
        "name": "Hiroki Habuka",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Encode AI",
    "url": "https://encode.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI policy organization.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "PowerShift\u00ae",
        "description": "An end-to-end self-governing system for startups, companies, DAOs, and non-profits to enable decentralized, purpose-driven operations and self-organization",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Meridian",
    "url": "https://www.meridian.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "MAIA",
    "url": "",
    "type": "Academic",
    "country": "United Kingdom",
    "mission": "Machine Intelligence and Autonomy lab.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Tarbell Fellowship",
    "url": "https://tarbellfellowship.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Fellowship program for AI journalism.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI Reporting Fellowship",
        "description": "$60,000 to $80,000 for early-career fellows; $90,000 to $110,000 for senior fellows. AI Journalism Fundamentals training. Newsroom placements at Bloomberg, TIME, MIT Tech Review, the Guardian, and many more",
        "status": "Active"
      },
      {
        "name": "Grants",
        "description": "From $1,000 to $15,000. Forward-looking stories on AI and its impacts. Open to freelancers, staff reporters, and newsroom teams",
        "status": "Active"
      },
      {
        "name": "Journalist in Residence",
        "description": "$80,000+ stipend. Freedom to upskill on AI and produce in-depth reporting. Open to senior journalists and experienced editors",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Safe AI Forum",
    "url": "https://safeaiforum.org/",
    "type": "Nonprofit",
    "country": "International",
    "mission": "International AI safety forum.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "RFIDshoe",
        "description": "Technology project",
        "status": "Active"
      },
      {
        "name": "RFIDhat",
        "description": "Entertainment project",
        "status": "Active"
      },
      {
        "name": "RFIDrock",
        "description": "Music project",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Forethought",
    "url": "https://forethought.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Preparing for the Intelligence Explosion",
        "description": "Research on grand challenges that arise during rapid technological progress driven by AI, including new weapons of mass destruction, AI-enabled autocracies, races to grab offworld resources, and digital beings worthy of moral consideration",
        "status": "Active"
      },
      {
        "name": "AI-Enabled Coups: How a Small Group Could Use AI to Seize Power",
        "description": "Analysis of how advanced AI could enable small groups or individuals to stage coups through singularly loyal AI systems, secretly loyal AI, or exclusive access to superhuman capabilities",
        "status": "Active"
      },
      {
        "name": "Better Futures Series",
        "description": "Exploration of approaches to improve futures where humanity survives to achieve truly great outcomes, complementing catastrophe prevention efforts",
        "status": "Active"
      },
      {
        "name": "Will AI R&D Automation Cause a Software Intelligence Explosion?",
        "description": "Research on whether AI systems automating AI development could trigger runaway feedback loops leading to extremely fast AI progress without additional compute requirements",
        "status": "Active"
      },
      {
        "name": "AI Tools for Existential Security",
        "description": "Development of AI tools to help humanity anticipate and address AI-driven challenges, including epistemic tools, coordination tools, and risk-targeted tools",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "directors": 2,
    "key_people": [
      {
        "name": "William MacAskill",
        "role": "Researcher"
      },
      {
        "name": "Fin Moorhouse",
        "role": "Researcher"
      },
      {
        "name": "Tom Davidson",
        "role": "Researcher"
      },
      {
        "name": "Lukas Finnveden",
        "role": "Researcher"
      },
      {
        "name": "Rose Hadshar",
        "role": "Researcher"
      },
      {
        "name": "Daniel Eth",
        "role": "Researcher"
      },
      {
        "name": "Lizka Vaintrob",
        "role": "Researcher"
      },
      {
        "name": "Owen Cotton-Barratt",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Arcadia Impact",
    "url": "https://www.arcadiaimpact.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety philanthropy advisory.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "LASR Labs",
        "description": "A technical AI safety research programme focused on reducing the risk of loss of control to advanced AI. Participants work in teams of three to four, supervised by an experienced AI safety researcher, to write an academic-style paper and accompanying blog post.",
        "status": "Active"
      },
      {
        "name": "AI Governance Taskforce",
        "description": "A career development programme for experienced professionals looking to transition careers into AI governance, focussed on reducing risks from advanced AI. 12 week, remote, part-time cohorts producing policy research in teams of 4.",
        "status": "Active"
      },
      {
        "name": "Impact Research Groups (IRG)",
        "description": "Designed to support talented and ambitious students in London who wish to pursue high-impact research careers. Participants work in small groups with experienced mentors to explore a research question over 8 weeks.",
        "status": "Active"
      },
      {
        "name": "The Orion AI Governance Initiative",
        "description": "A talent development initiative based in the UK that aims to support outstanding students to launch a career shaping the future of artificial intelligence. Offers AI policy Accelerator, Mentorship Programme, and internship.",
        "status": "Active"
      },
      {
        "name": "AI Safety Engineering Taskforce (ASET)",
        "description": "Connects experienced technical professionals with high-profile AI safety projects. Builds and manages teams of skilled engineers and scientists, supporting their transition into full-time AI safety careers.",
        "status": "Active"
      },
      {
        "name": "Safe AI London (SAIL)",
        "description": "Supports individuals in London that are interested in reducing the risks posed from advanced artificial intelligence by raising awareness and providing high-quality resources and support.",
        "status": "Active"
      },
      {
        "name": "LEAH Coworking Space",
        "description": "A coworking office in central London supporting people working on projects aiming to have a large positive impact on the world, providing productive space and facilitating connections.",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Pivotal Research",
    "url": "https://pivotal-research.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "The Pivotal Fellowship",
        "description": "A 9-week research program dedicated to the responsible development of emerging technologies where Fellows work with experienced researchers in AI on important questions facing humanity",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Seismic",
    "url": "https://seismic.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "2025 Seismic report",
        "description": "Report publication by Seismic Foundation",
        "status": "Active"
      },
      {
        "name": "AI Literacy Building",
        "description": "Helping to build AI literacy through powerful storytelling and targeted media campaigns to help individuals understand and engage with AI",
        "status": "Active"
      },
      {
        "name": "Media Campaigns",
        "description": "Creating highly engaging campaigns through partnerships with leading media, production, and public affairs organizations to raise awareness and drive action",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Sander Volten",
        "role": "CEO"
      },
      {
        "name": "Alice Anselmi",
        "role": "Chief of Staff"
      },
      {
        "name": "Philip Trippenbach",
        "role": "Strategy director"
      },
      {
        "name": "Jeroen Thissen",
        "role": "Creative Director"
      },
      {
        "name": "Nina Tihy",
        "role": "Operations & Projects Manager"
      },
      {
        "name": "Daron Acemoglu",
        "role": "Professor Massachusetts Institute for Technology, Nobel Prize in Economics in 2024"
      }
    ]
  },
  {
    "name": "BlueDot Impact",
    "url": "https://bluedot.org/",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "AI safety education organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "The Future of AI Course",
        "description": "An introduction to what AI can do today, where it's going over the next decade, and how you can start contributing to a better future",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AGI Strategy Course",
        "description": "A deep dive into the incentives driving the AI companies, what's at stake, and the strategies for ensuring AI benefits humanity",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Technical AI Safety Course",
        "description": "For technical talent who want to drive AI safety research and policy professionals building governance solutions",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Governance Course",
        "description": "Learn about the policy landscape, regulatory tools, and institutional reforms needed to navigate the transition to transformative AI",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Biosecurity Course",
        "description": "For people who want to build a pandemic-proof world. Learn how we can defend against AI-enabled bioattacks",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [],
    "directors": 2,
    "subteams": 1,
    "key_people": [
      {
        "name": "Neel Nanda",
        "role": "Mech Interp Lead at Google DeepMind, Former participant and facilitator"
      },
      {
        "name": "Adam Jones",
        "role": "Member of Technical Staff at Anthropic, Former AI safety lead at BlueDot"
      },
      {
        "name": "Richard Ngo",
        "role": "Former OpenAI and DeepMind, AI Alignment Course Designer"
      },
      {
        "name": "Catherine Fist",
        "role": "Head of Delivery at UK AISI, AI Governance Course Graduate"
      },
      {
        "name": "Marius Hobbhahn",
        "role": "CEO at Apollo Research, AI Alignment Course Graduate"
      },
      {
        "name": "Chiara Gerosa",
        "role": "Executive Director at Talos, AI Governance Course Facilitator"
      }
    ]
  },
  {
    "name": "Odyssean Institute",
    "url": "https://odysseaninstitute.org/",
    "type": "Think Tank",
    "country": "United Kingdom",
    "mission": "Technology policy think tank.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "GRAIN",
        "description": "",
        "status": "Active"
      },
      {
        "name": "Horizon Scan on Global Catastrophic Risks",
        "description": "Horizon scanning project focused on global catastrophic risks",
        "status": "Active"
      },
      {
        "name": "Resilient Supply Chains",
        "description": "Project focused on building resilient supply chains",
        "status": "Active"
      },
      {
        "name": "AI Governance Assembly",
        "description": "Project related to AI governance through assembly processes",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Hortus AI",
    "url": "https://hortus.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "AVERI",
    "url": "https://averi.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Wyo L",
        "role": "Creative Direction"
      },
      {
        "name": "Olivia C",
        "role": "Public Relations"
      },
      {
        "name": "Micah J",
        "role": "Graphic Design"
      },
      {
        "name": "Jack M",
        "role": "Growth Strategy"
      },
      {
        "name": "Emily Z",
        "role": "Email Marketing"
      },
      {
        "name": "Daniel R",
        "role": "SEO"
      },
      {
        "name": "Caleb W",
        "role": "Conversion Optimization"
      },
      {
        "name": "Rachel J",
        "role": "Demand Generation"
      },
      {
        "name": "Adam S",
        "role": "Web Development"
      },
      {
        "name": "Adrian V",
        "role": "UX/UI & Web Design"
      },
      {
        "name": "Simone E",
        "role": "Animation & Motion Graphics"
      },
      {
        "name": "Javier M",
        "role": "Performance Marketing"
      }
    ]
  },
  {
    "name": "Tilde Research",
    "url": "https://tilderesearch.com/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Mechanistic Interpretability",
        "description": "Research focused on understanding the internal mechanisms of AI models",
        "status": "Active"
      },
      {
        "name": "New Architectures",
        "description": "Development of novel AI model architectures",
        "status": "Active"
      },
      {
        "name": "Pretraining Science",
        "description": "Research into the science and methods of pretraining AI models",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "Conscium",
    "url": "https://conscium.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI consciousness research.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI agent verification",
        "description": "Verification services for AI agents that can affect the real world directly, checking they do what they are supposed to do and only that",
        "status": "Active"
      },
      {
        "name": "Arena ax",
        "description": "Third-party environment platform for organizations to verify their own agents, providing confidential testing and simulation services",
        "status": "Active"
      },
      {
        "name": "Neuromorphic computing",
        "description": "Development of spiking neural networks (SNNs) for more efficient AI systems, partnering with UC Santa Cruz to create digital twins that constantly learn and improve",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Dr. Daniel Hulme",
        "role": "Leader/Co-founder, pioneer in Artificial Life, former founder of Satalia (sold to WPP for $100m), WPP's Chief AI Officer"
      },
      {
        "name": "Ass. Prof. Ted Lappas",
        "role": "Expert in spatio-temporal computation and neural architectures, technical lead for WPP's AI programme"
      },
      {
        "name": "Dr Panagiotis [Panos] Repoussis",
        "role": "Expert in evolutionary computation and data-driven optimisation, leads WPP's AI Research Labs"
      },
      {
        "name": "Ed Charvet",
        "role": "Serial entrepreneur, former COO, advises private equity firms and angel investor"
      },
      {
        "name": "Calum Chace",
        "role": "Strategy consultant, author of AI books, keynote speaker, advises governments and companies on AI policy"
      },
      {
        "name": "Jason Eshragian",
        "role": "Partner from University of California Santa Cruz, developer of snnTorch Python library"
      }
    ]
  },
  {
    "name": "LISA",
    "url": "https://lisa.ai/",
    "type": "Academic",
    "country": "United States",
    "mission": "Laboratory for Intelligent Systems and AI Safety.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Geodesic Research",
    "url": "https://geodesicresearch.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Empirical Study of Fulfilling Misalignment",
        "description": "Research investigating whether pretraining data related to AI misalignment makes models less aligned and controllable, studying the Self-Fulfilling Misalignment Problem (SFM). Includes collaboration with HyperstitionAI to generate billions of tokens of positive alignment data.",
        "status": "Active"
      },
      {
        "name": "Generalisation of Chain-of-Thought Obfuscation",
        "description": "Investigation of whether chain-of-thought obfuscation learned in one domain generalizes to safety-critical contexts, examining if problematic reasoning hiding transfers across domains from benign to high-stakes scenarios.",
        "status": "Active"
      },
      {
        "name": "Chain-of-Thought Health Metrics (MARS 3.0)",
        "description": "Development of metrics to measure chain-of-thought pathologies including post-hoc reasoning, internalized reasoning, and steganography to assess CoT faithfulness and increase visibility into frontier model safety behaviors.",
        "status": "Active"
      },
      {
        "name": "Externalisation (MARS 3.0)",
        "description": "Development of early exit mechanisms to increase transparency in language model reasoning by training models to externalize internal reasoning processes through early forward pass exits.",
        "status": "Active"
      },
      {
        "name": "Chain-of-Thought Injection for AI Control (MARS 3.0)",
        "description": "Investigation of whether LLMs can be effectively steered using chain-of-thought injections to steer untrusted models away from misaligned behavior through targeted reasoning interventions.",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Edward James Young",
        "role": "Co-Founder"
      },
      {
        "name": "Puria Radmard",
        "role": "Co-Founder and Co-Director"
      },
      {
        "name": "Cameron Tice",
        "role": "Co-Founder and Co-Director"
      },
      {
        "name": "Olivia Benoit",
        "role": "Co-Founder and Operations"
      },
      {
        "name": "Hannes Whittingham",
        "role": "Operations"
      },
      {
        "name": "Kyle O'Brien",
        "role": "Researcher"
      },
      {
        "name": "David Williams-King",
        "role": "Researcher"
      },
      {
        "name": "Linh Le",
        "role": "Researcher"
      },
      {
        "name": "Ida Caspary",
        "role": "Researcher"
      },
      {
        "name": "Manqing Liu",
        "role": "Researcher"
      },
      {
        "name": "Liza Pavlova",
        "role": "Researcher"
      },
      {
        "name": "Karthik Viswanathan",
        "role": "Researcher"
      },
      {
        "name": "Mariia Koroliuk",
        "role": "Researcher"
      },
      {
        "name": "Lindsay Smith",
        "role": "Researcher"
      },
      {
        "name": "Ananya Malik",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Dovetail Research",
    "url": "https://dovetailresearch.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "ARENA",
    "url": "https://www.arena.education/",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "AI safety education program.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "AI Futures Project",
    "url": "",
    "type": "Think Tank",
    "country": "United States",
    "mission": "AI futures research.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Whitebox Research",
    "url": "https://whiteboxresearch.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI interpretability research.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Gutenberg",
        "description": "Uses cutting-edge interpretability techniques to understand and optimize AI models, making them transparent rather than black boxes",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Cadenza Labs",
    "url": "https://cadenzalabs.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Building Robust Lie Detectors",
        "description": "Research and build robust lie detectors for LLMs",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "EleosAI",
    "url": "https://eleosai.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Taking AI Welfare Seriously",
        "description": "Report arguing there is a realistic possibility of consciousness and/or robust agency in near-future AI systems, with recommendations for AI companies (joint output with NYU Center for Mind, Ethics, and Policy)",
        "status": "Active"
      },
      {
        "name": "Looking Inward: Language Models Can Learn About Themselves by Introspection",
        "description": "Study of introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios to inform about moral status",
        "status": "Active"
      },
      {
        "name": "Towards Evaluating AI Systems for Moral Status Using Self-Reports",
        "description": "Research agenda towards making AI self-reports more introspection-based and reliable for investigating whether AI systems have states of moral significance",
        "status": "Active"
      },
      {
        "name": "Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
        "description": "Rigorous and empirically grounded approach to AI consciousness, assessing existing AI systems in light of neuroscientific theories of consciousness",
        "status": "Active"
      },
      {
        "name": "Claude 4 model welfare interviews",
        "description": "Study of model self-reports examining why they are insufficient for determining AI welfare",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Robert Long",
        "role": "Executive Director"
      },
      {
        "name": "Rosie Campbell",
        "role": "Managing Director"
      },
      {
        "name": "Larissa Schiavo",
        "role": "Events and Comms Specialist"
      },
      {
        "name": "Patrick Butlin",
        "role": "Senior Research Lead"
      },
      {
        "name": "Abraham Rowe",
        "role": "Head of Operations"
      },
      {
        "name": "David Chalmers",
        "role": "Advisor"
      },
      {
        "name": "Owain Evans",
        "role": "Advisor"
      },
      {
        "name": "Jeff Sebo",
        "role": "Advisor"
      },
      {
        "name": "Emma Abele",
        "role": "Advisor"
      }
    ]
  },
  {
    "name": "Realm Labs",
    "url": "https://www.realmlabs.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Realm OmniGuard",
        "description": "State-of-the-art content moderation that classifies harmful content across text, audio, images, and video in 15+ languages. Detects profanity, CSAM, violence, hate speech, and more with 60-150ms latency.",
        "status": "Active"
      },
      {
        "name": "Realm Prism",
        "description": "Real-time AI observability that reveals what your model is thinking with every token. Detect hallucinations, measure grounding, understand user intent, and debug AI behavior 50-300x faster than LLM-as-judge.",
        "status": "Active"
      },
      {
        "name": "DataRealm",
        "description": "Discover and protect your sensitive data assets across the enterprise. Scan repositories to identify over-exposed crown jewels and block exfiltration to email, cloud, and AI tools like ChatGPT and Cursor.",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Haize Labs",
    "url": "https://www.haizelabs.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI red teaming company.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "PRISM Eval",
    "url": "https://prism-eval.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI evaluation company founded by MATS alumni.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Heron AI Security",
    "url": "https://heron.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI security company.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "TFI",
    "url": "https://tfi.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Technical AI safety research.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CBAI",
    "url": "https://cbai.ai/",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for Beneficial AI.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI Safety Student Team",
        "description": "Educational program supported by CBAI",
        "status": "Active"
      },
      {
        "name": "MIT AI Alignment",
        "description": "AI alignment research initiative supported by CBAI",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "PIBBSS",
    "url": "https://www.pibbss.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Program for AI and Neuroscience Safety.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Multi-agent dynamics in biological systems for AI governance",
        "description": "Research exploring how biological multi-agent dynamics can inform AI governance approaches",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Developmental biology insights for alignment",
        "description": "Investigating what developmental biology can teach us about AI alignment",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Legal frameworks for emerging AI risks",
        "description": "Research on how legal frameworks apply to emerging AI risks",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Principles governing intelligence across substrates and scales",
        "description": "Study of fundamental principles that govern intelligence across different substrates and scales",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Fernando Rosas",
        "role": "Researcher/Fellow"
      },
      {
        "name": "Adam Shai",
        "role": "Researcher/Fellow"
      },
      {
        "name": "Eleni Angelou",
        "role": "Oxford Centre for the Governance of AI winter 2026 fellow"
      },
      {
        "name": "Magdalena Wache",
        "role": "Researcher/Fellow"
      },
      {
        "name": "Cecilia Wood",
        "role": "Researcher/Fellow"
      },
      {
        "name": "Gabriel Weil",
        "role": "Assistant Professor at Touro University Law Center, Non-Resident Senior Fellow at the Institute for Law & AI"
      },
      {
        "name": "Ninell Oldenburg",
        "role": "Researcher/Fellow"
      },
      {
        "name": "Daniel Herrmann",
        "role": "Assistant Professor of Philosophy at UNC Chapel Hill"
      },
      {
        "name": "Jan Kirchner",
        "role": "Researcher, Anthropic"
      },
      {
        "name": "Joel Christoph",
        "role": "10Billion.org Founder, Japan-IMF Scholar & Economics PhD"
      },
      {
        "name": "Agust\u00edn Martinez Su\u00f1\u00e9",
        "role": "Postdoctoral Research Associate at the University of Oxford"
      },
      {
        "name": "Alexander Gietelink Oldenziel",
        "role": "Community Member/Advisor"
      },
      {
        "name": "Jan Kulveit",
        "role": "Community Member/Advisor"
      },
      {
        "name": "Patrick Butlin",
        "role": "Community Member/Advisor"
      },
      {
        "name": "Tan Zhi-Xuan",
        "role": "Community Member/Advisor"
      },
      {
        "name": "David A. Dalrymple",
        "role": "Community Member/Advisor"
      }
    ]
  },
  {
    "name": "Harmony Intelligence",
    "url": "https://harmonyintelligence.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2,
    "subteams": 1
  },
  {
    "name": "AI Safety Awareness Project",
    "url": "https://aisafetyawarenessproject.org/",
    "type": "Nonprofit",
    "country": "International",
    "mission": "AI safety awareness and education.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Workshops and Education Programs",
        "description": "Running workshops and education programs for community groups and public institutions including law enforcement agencies, libraries, churches, universities, and the general public",
        "status": "Active"
      },
      {
        "name": "Situational Awareness Partnership",
        "description": "Partnering with industry experts to inform organizations about frontier AI research and developments including workforce augmentation, AI agents, cybercrime, AGI, and loss of control risk",
        "status": "Active"
      },
      {
        "name": "AI Safety Seminars",
        "description": "Variety of AI safety seminars and workshops for individuals and organizations of all different levels of AI sophistication",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Noema Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Evitable",
    "url": "https://evitable.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "GPAI Policy Lab",
    "url": "https://gpai.ai/",
    "type": "Think Tank",
    "country": "International",
    "mission": "Global Partnership on AI policy research.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Golden Gate Institute for AI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research institute.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Charles University ACS",
    "url": "https://ufal.mff.cuni.cz/",
    "type": "Academic",
    "country": "Czech Republic",
    "mission": "AI safety research at Charles University.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "CorefUD and CRAC Shared Tasks",
        "description": "Multilingual work on CorefUD with annotated coreference and anaphoric relations, advancing cross-linguistic research",
        "status": "Active"
      },
      {
        "name": "PaReNT",
        "description": "Deep learning tool that models word formation in seven languages",
        "status": "Active"
      },
      {
        "name": "Neural Models for Multilingual Inflection",
        "description": "Research on neural models for multilingual inflection",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Milan Straka",
        "role": "Teacher/Researcher"
      },
      {
        "name": "Jirka Hana",
        "role": "Teacher/Researcher"
      },
      {
        "name": "Michal Nov\u00e1k",
        "role": "Researcher"
      },
      {
        "name": "Emil Svoboda",
        "role": "Researcher"
      },
      {
        "name": "Patr\u00edcia Schmidtov\u00e1",
        "role": "Ph.D. Student"
      },
      {
        "name": "Jana Strakov\u00e1",
        "role": "Researcher/Supervisor"
      },
      {
        "name": "Tom\u00e1\u0161 Sourada",
        "role": "Student"
      }
    ]
  },
  {
    "name": "Kairos",
    "url": "https://kairos.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Truthful AI",
    "url": "https://truthful.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI truthfulness and honesty research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Truthful AI: Developing and governing AI that does not lie",
        "description": "Research on establishing standards, institutions, and technical methods to prevent AI systems from lying, including avoiding 'negligent falsehoods', creating evaluation institutions, and training AI systems to be truthful via curated datasets and human interaction",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "directors": 1,
    "subteams": 1,
    "key_people": [
      {
        "name": "Owain Evans",
        "role": "Lead Author/Researcher"
      },
      {
        "name": "Owen Cotton-Barratt",
        "role": "Researcher"
      },
      {
        "name": "Lukas Finnveden",
        "role": "Researcher"
      },
      {
        "name": "Adam Bales",
        "role": "Researcher"
      },
      {
        "name": "Avital Balwit",
        "role": "Researcher"
      },
      {
        "name": "Peter Wills",
        "role": "Researcher"
      },
      {
        "name": "Luca Righetti",
        "role": "Researcher"
      },
      {
        "name": "William Saunders",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "AI Underwriting Company",
    "url": "https://aiunderwriting.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI risk assessment company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "AIUC-1",
        "description": "The world's first standard for AI agents covering data & privacy, security, safety, reliability, accountability and societal risks",
        "status": "Active"
      },
      {
        "name": "AI-Proofing The Board and C-suite",
        "description": "Research paper by Dr. Keri Pearlson at MIT Sloan and Rajiv Dattani at AIUC",
        "status": "Active"
      },
      {
        "name": "Stanford Trustworthy AI Research x AIUC partnership",
        "description": "Partnership with Stanford Professor Dr. Sanmi Koyejo on real-world AI risk for enterprises",
        "status": "Active"
      },
      {
        "name": "Orrick x AIUC partnership",
        "description": "Partnership with top AI law firm Orrick, Herrington & Sutcliffe to create AIUC-1",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Rajiv Dattani",
        "role": "AIUC team member"
      },
      {
        "name": "Phil Venables",
        "role": "Former CISO at Google"
      },
      {
        "name": "Dr. Keri Pearlson",
        "role": "Principal Research Scientist at MIT Sloan"
      },
      {
        "name": "Lena Smart",
        "role": "SecurityPal (former CISO of MongoDB)"
      },
      {
        "name": "Dr. Christina Liaghati",
        "role": "MITRE ATLAS Lead"
      },
      {
        "name": "Sanmi Koyejo",
        "role": "Professor, Stanford"
      },
      {
        "name": "John Bautista",
        "role": "Orrick Partner, Creator of the YC SAFE"
      },
      {
        "name": "Hyrum Anderson",
        "role": "AI security expert"
      }
    ]
  },
  {
    "name": "Midas Project",
    "url": "https://midasproject.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Oxford Martin AIGI",
    "url": "https://www.oxfordmartin.ox.ac.uk/ai-governance/",
    "type": "Academic",
    "country": "United Kingdom",
    "mission": "Oxford Martin AI Governance Initiative.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "A Blueprint for Multinational Advanced AI Development",
        "description": "Research on multinational approaches to advanced AI development",
        "status": "Active"
      },
      {
        "name": "The Annual AI Governance Report 2025: Steering the Future of AI",
        "description": "Annual report on AI governance trends and recommendations",
        "status": "Active"
      },
      {
        "name": "Examining AI Safety as a Global Public Good",
        "description": "Research on AI safety implications, challenges, and research priorities from a global public good perspective",
        "status": "Active"
      },
      {
        "name": "Open Problems in Machine Unlearning for AI Safety",
        "description": "Research on machine unlearning techniques for AI safety applications",
        "status": "Active"
      },
      {
        "name": "Justice AI initiative",
        "description": "Initiative focusing on the use of AI in the American criminal justice system",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Robert Trager",
        "role": "Director"
      },
      {
        "name": "Michael Osborne",
        "role": "Professor of Machine Learning"
      },
      {
        "name": "Nikki Sun",
        "role": "Programme Manager"
      },
      {
        "name": "Marta Ziosi",
        "role": "Postdoctoral Researcher"
      },
      {
        "name": "Nicholas Caputo",
        "role": "Legal Researcher"
      },
      {
        "name": "Fazl Barez",
        "role": "Postdoctoral Research Fellow"
      },
      {
        "name": "Ben Garfinkel",
        "role": "Postdoctoral Researcher"
      },
      {
        "name": "Allan Dafoe",
        "role": "Director of Frontier Safety and Governance at Google DeepMind (Visiting Fellow)"
      },
      {
        "name": "Julia C. Morse",
        "role": "Visiting Fellow"
      },
      {
        "name": "Sam Daws",
        "role": "Visiting Fellow"
      },
      {
        "name": "Henry de Zoete",
        "role": "Visiting Fellow"
      }
    ]
  },
  {
    "name": "Watertight AI",
    "url": "https://watertight.ai/",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI safety and security company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "MAI",
    "url": "https://mai.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "BodyMap",
        "description": "VR platform for exploring human anatomy in an engaging 3D environment",
        "status": "Active"
      },
      {
        "name": "AcuMap",
        "description": "The world's first VR training solution for acupuncture",
        "status": "Active"
      },
      {
        "name": "Project OME",
        "description": "Platform for building patient-specific models",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "directors": 3,
    "key_people": [
      {
        "name": "Gregory Katz",
        "role": "Assistant Professor of Medicine, NYU Grossman School of Medicine"
      }
    ]
  },
  {
    "name": "CLAIR",
    "url": "https://clair.ai/",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for Language and AI Research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Asymmetric Security",
    "url": "https://asymmetricsecurity.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI security research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Atlas Computing",
    "url": "https://atlascomputing.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety computing infrastructure.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "cybersecurity asymmetry",
        "description": "Developing tools that enable formal verification as a defense-dominant paradigm to address the problem where AI systems used to find and fix vulnerabilities could become powerful cyber weapons",
        "status": "Active"
      },
      {
        "name": "Focused Research Organization for formal specifications",
        "description": "Building tools that generate and validate formal specifications in partnership with Convergent Research",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "directors": 3,
    "subteams": 1,
    "key_people": [
      {
        "name": "Evan",
        "role": "CEO"
      },
      {
        "name": "Jason Gross",
        "role": "Former researcher (left to start Theorem Labs)"
      }
    ]
  },
  {
    "name": "Fulcrum Research",
    "url": "https://fulcrumresearch.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Quibbler and Orchestra",
        "description": "Tooling that tells you what your ML evals and environments are truly testing for",
        "status": "Active"
      },
      {
        "name": "Inference time infrastructure for agent deployment",
        "description": "Building infrastructure to safely deploy agents in the world",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Safeguarded AI",
    "url": "https://safeguardedai.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1
  },
  {
    "name": "Secure AI Project",
    "url": "https://secureaiproject.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI security research project.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "Lucid Computing",
    "url": "https://lucidcomputing.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI interpretability company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Landmark Endorser Network",
        "description": "Robust server network being deployed to achieve 97% country reach, covering all relevant jurisdictions globally for location verification",
        "status": "Active"
      },
      {
        "name": "Cryptographic Location Verification Platform",
        "description": "Platform providing hardware-rooted cryptographic proof that AI workloads operate within designated boundaries using Trusted Execution Environments",
        "status": "Active"
      },
      {
        "name": "Delay-based Location Protocol",
        "description": "Protocol that verifies distance and location of AI accelerators through timing measurements in TEE connections",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Contramont Research",
    "url": "https://contramont.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "LM backdoors",
        "description": "Research on backdoors in language models",
        "status": "Active"
      },
      {
        "name": "real-world evals",
        "description": "Real-world evaluations research",
        "status": "Active"
      },
      {
        "name": "scalable oversight",
        "description": "Research on scalable oversight methods",
        "status": "Active"
      },
      {
        "name": "Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits",
        "description": "Research on unelicitable backdoors in language models using cryptographic transformer circuits",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Cosmos Institute",
    "url": "https://cosmosinstitute.com/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Long-term AI research institute.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CaML",
    "url": "https://caml.org/",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for AI and Machine Learning safety.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1
  },
  {
    "name": "Equilibria Network",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI coordination research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "EconTAI",
    "url": "https://econtai.org/",
    "type": "Academic",
    "country": "United States",
    "mission": "Economics of transformative AI research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Labor Market Transformation",
        "description": "Evaluation of AI impact on employment, analysis of changing skill demands, transition policies for displaced workers, and future of work scenario planning",
        "status": "Active"
      },
      {
        "name": "Macroeconomic Implications",
        "description": "AI's impact on productivity and growth, income and wealth distribution effects, demand management in an AI economy, and international competitiveness",
        "status": "Active"
      },
      {
        "name": "AI Safety & Economic Governance",
        "description": "Governance of transformative AI, economic metrics for AI safety trade-offs, new taxation frameworks, and international coordination mechanisms",
        "status": "Active"
      },
      {
        "name": "Economic Research Automation",
        "description": "AI agents for economic research, development of autonomous co-scientists, automated research dissemination, and automated research evaluation",
        "status": "Active"
      },
      {
        "name": "Financial Sector Implications",
        "description": "AI transformation of financial services, AI agents in the financial sector, financial stability risks, and systemic vulnerabilities",
        "status": "Active"
      },
      {
        "name": "Economics of AI Agents",
        "description": "Economic theory for AI agent interactions, market dynamics with AI participants, regulatory frameworks for AI agents, and AI-driven economic systems",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Anton Korinek",
        "role": "Faculty Director"
      },
      {
        "name": "Basil Halperin",
        "role": "Associated Faculty"
      },
      {
        "name": "Lee Lockwood",
        "role": "Associated Faculty"
      }
    ]
  },
  {
    "name": "Aether",
    "url": "https://aether.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI ethics and safety research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1
  },
  {
    "name": "CivAI",
    "url": "https://civai.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Civic AI safety organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "We Need To Talk: The AI Voice Game",
        "description": "Interactive demo tool for the public",
        "status": "Active"
      },
      {
        "name": "Briefings for civil society organizations",
        "description": "Eye-opening presentations of AI technology for leadership and constituents, delivered free of charge",
        "status": "Active"
      },
      {
        "name": "Interactive tools for the public",
        "description": "Demos that strive for 'aha' moments while illuminating societal consequences from AI",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Equistamp",
    "url": "https://equistamp.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI evaluation company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Evaluation Implementation",
        "description": "Build and run evaluations, baselines, and benchmarks using Inspect and similar frameworks",
        "status": "Active"
      },
      {
        "name": "Data Annotation",
        "description": "Quality data labeling and annotation pipelines for research projects with managed annotation teams",
        "status": "Active"
      },
      {
        "name": "Red/Blue Teaming",
        "description": "Adversarial testing and stress-testing for AI systems to find vulnerabilities and improve robustness",
        "status": "Active"
      },
      {
        "name": "Project Operations",
        "description": "Administrative and organizational support including grant administration, hiring logistics, and operational overhead",
        "status": "Active"
      },
      {
        "name": "Research Support",
        "description": "General technical labor pool for AI safety research with task-based work on flexible schedule",
        "status": "Active"
      },
      {
        "name": "Financing",
        "description": "Flexible payment terms and assistance with grant writing and fund raising",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Chris Canal",
        "role": "CEO"
      },
      {
        "name": "Daniel O'Connell",
        "role": "CTO"
      },
      {
        "name": "Rob Miles",
        "role": "AI Advisor"
      }
    ]
  },
  {
    "name": "dmodel",
    "url": "https://dmodel.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI model analysis company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "RLVR",
        "description": "Research focus area mentioned as part of fundamental AI research",
        "status": "Active"
      },
      {
        "name": "Interpretability",
        "description": "Research focus area for understanding AI models",
        "status": "Active"
      },
      {
        "name": "Agents",
        "description": "Research on agents to automate the path to aligned superintelligence",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "CORAL",
    "url": "https://coral.ai/",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for Open and Responsible AI Lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Seldon Labs",
    "url": "https://seldonlabs.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Principia Labs",
    "url": "https://principialabs.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Automated Mathematical Discovery",
        "description": "AI systems that can reason, discover, and prove at the frontier of mathematical knowledge using large-scale pretraining with reinforcement learning",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Harry",
        "role": "Contact/Team Member"
      }
    ]
  },
  {
    "name": "DeepResponse",
    "url": "https://deepresponse.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI incident response company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Decode Research",
    "url": "https://decoderesearch.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI interpretability research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Neuronpedia",
        "description": "Interpretability platform for understanding, visualizing, testing, and searching AI internals",
        "status": "Active"
      },
      {
        "name": "SAELens",
        "description": "Open-source library for training and analyzing Sparse Autoencoders (SAEs)",
        "status": "Active"
      },
      {
        "name": "SAEDashboard",
        "description": "Generate dashboards for visualizing SAE features",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Johnny Lin",
        "role": "Leader"
      },
      {
        "name": "David Chanin",
        "role": "Significant support"
      }
    ]
  },
  {
    "name": "Coordinal",
    "url": "https://coordinal.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI coordination research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Mosaic Labs",
    "url": "https://mosaiclabs.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Andon Labs",
    "url": "https://andonlabs.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety monitoring company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Safe Autonomous Organization",
        "description": "Building and iteratively launching autonomous organizations while bridging AI control research with real-world testing",
        "status": "Active"
      },
      {
        "name": "Vending-Bench 2 and Arena",
        "description": "A benchmark where models manage a simulated vending machine business for a full year, navigating adversarial suppliers, negotiations, and customer complaints while maximizing profits. Arena version allows models to compete with each other",
        "status": "Active"
      },
      {
        "name": "Butter-Bench",
        "description": "Evaluating LLM controlled robots for practical intelligence by testing delivery tasks in household settings",
        "status": "Active"
      },
      {
        "name": "Blueprint-Bench",
        "description": "Testing spatial intelligence in AI models by asking them to convert apartment photographs into accurate 2D floor plans",
        "status": "Active"
      },
      {
        "name": "Andon Vending",
        "description": "A vending machine operated entirely by an AI agent, accessible through messaging apps to provide insights into safety and alignment of LLMs in the real world",
        "status": "Active"
      },
      {
        "name": "Vending-Bench",
        "description": "A benchmark where models manage a simulated vending machine business over long time horizons (months)",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Aligned AI",
    "url": "https://www.aligned.ai/",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI alignment company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Orthogonal",
    "url": "https://www.orthogonal.io/",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI safety research company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Workshop Labs",
    "url": "https://workshoplabs.ai/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety experimentation lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Personal AI Models",
        "description": "Building technology to train billions of models, one for each person, that are aligned to individual goals and values with verifiable privacy",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Aelus",
    "url": "https://aelus.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Simplex",
    "url": "https://simplex.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety simplification research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Aethra Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Freestyle Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Independent AI safety research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Theorem Labs",
    "url": "https://theoremlabs.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI verification research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Ulyssean",
    "url": "https://ulyssean.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Long-term AI safety research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Trajectory Labs",
    "url": "https://trajectorylabs.org/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI trajectory research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "TamperSec",
    "url": "https://tampersec.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI tamper-resistance security.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Physical Defense Mechanisms for High-Performance Computing",
        "description": "Developing physical defense mechanisms that can be retrofitted to existing High-Performance Computing servers while addressing thermal and space constraints",
        "status": "Active"
      },
      {
        "name": "Attack Vector Research",
        "description": "Research into potential offensive attack vectors for high-performance compute chips to better understand and defend against sophisticated physical attacks",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "AI Standards Lab",
    "url": "https://aistandardslab.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety standards research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "GPAI Risk Sources and Risk Management Catalog",
        "description": "Converting insights from existing literature into ready-made text for formal AI safety standards documents, including a 150-page contribution of state-of-the-art GPAI risk sources and risk management practices",
        "status": "Active"
      },
      {
        "name": "EU AI Act Codes of Practice Support",
        "description": "Providing technical contributions to EU AI Office for drafting AI Codes of Practice in support of the EU AI Act, covering GPAI models transparency, systemic risk assessment and mitigation",
        "status": "Active"
      },
      {
        "name": "Risk Sources and Risk Management Measures for General-Purpose AI Systems",
        "description": "Published paper offering a catalog of state-of-the-art risks and risk management measures for GPAIs, released with public domain license for adoption into GPAI standards globally",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Koen Holtman",
        "role": "Co-lead and resident standards expert"
      },
      {
        "name": "Chin Ze Shen",
        "role": "Co-Lead"
      },
      {
        "name": "Ariel Gil",
        "role": "Co-founder / Board President"
      },
      {
        "name": "Jonathan Happel",
        "role": "Co-Founder / Member of Board"
      },
      {
        "name": "Rokas Gipiskis",
        "role": "Research Analyst"
      },
      {
        "name": "Ayrton San Joaquin",
        "role": "Research Analyst"
      },
      {
        "name": "Adrian Regenfu\u00df",
        "role": "Research Analyst"
      }
    ]
  },
  {
    "name": "LASST",
    "url": "https://lasst.org/",
    "type": "Academic",
    "country": "United States",
    "mission": "Lab for AI Safety and Security Testing.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Research on Legal Advocacy for Science and Technology Safety",
        "description": "Research ways legal advocacy can make advances in science and technology safer for people and the planet",
        "status": "Active"
      },
      {
        "name": "Legal Professional Education and Empowerment",
        "description": "Empower legal professionals with the knowledge and tools they need to contribute to a safer future",
        "status": "Active"
      },
      {
        "name": "Court and Policy Advocacy",
        "description": "Advocate in courts and policy-making institutions to promote safe and responsible science and technology",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Groundless",
    "url": "https://groundless.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Diffuse Concepts",
        "description": "An approach to theorization that is not grounded on a fixed core template, using AI to combine flexible 'theory-prompts' with 'application-prompts' for more context-sensitive theorization",
        "status": "Active"
      },
      {
        "name": "Modeling copilots",
        "description": "Building AI systems that assist with interface design by drawing from theoretical frameworks while remaining flexible to specific contexts",
        "status": "Active"
      },
      {
        "name": "Connection interfaces",
        "description": "Creating custom interfaces that can recognize and respond to unique characteristics and needs of each context without bureaucratic constraints",
        "status": "Active"
      },
      {
        "name": "Empty Form",
        "description": "A groundless approach to AI alignment that doesn't rely on establishing fixed foundations, embracing uncertainty and change as fundamental aspects",
        "status": "Active"
      },
      {
        "name": "Diffuse Response",
        "description": "Live Theories or metaformalisms that allow insights to scale without exclusion using AI infrastructure for conceptual transformations",
        "status": "Active"
      },
      {
        "name": "Live Machinery",
        "description": "Dynamic systems that allow keeping pace with evasive intelligence through personalized, context-adaptive approaches rather than rigid templates",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "AI Impacts",
    "url": "https://aiimpacts.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI forecasting and impact research.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "2023 Expert Survey on Progress in AI",
        "description": "Survey of AI experts on progress and timelines in artificial intelligence development",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "Fiction relevant to AI futurism",
        "description": "Analysis of fictional works relevant to AI development and future scenarios",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Is AI an existential risk to humanity?",
        "description": "Research examining whether AI poses existential risks to human civilization",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Likelihood of discontinuous progress around the development of AGI",
        "description": "Analysis of probability and characteristics of discontinuous progress in AGI development",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Incentivized technologies not pursued",
        "description": "Research on potentially beneficial technologies that are not being developed despite incentives",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Essay competition on the Automation of Wisdom and Philosophy",
        "description": "Competition exploring the potential automation of philosophical and wisdom-related processes",
        "status": "Completed",
        "paper_url": ""
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Katja Grace",
        "role": "Researcher"
      },
      {
        "name": "Ben Weinstein-Raun",
        "role": "Researcher"
      },
      {
        "name": "Owen Cotton-Barratt",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Poseidon Research",
    "url": "https://poseidonresearch.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Formation Research",
    "url": "https://formationresearch.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Lock-In Risk Research",
        "description": "Researching fundamental lock-in dynamics",
        "status": "Active"
      },
      {
        "name": "High-Impact Interventions",
        "description": "Implementing high-impact interventions to reduce lock-in risks",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Theomachia Labs",
    "url": "https://theomachialabs.com/",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "Novel AI Control Protocol Classes Evaluation and Scalability",
        "description": "Investigating alternative AI control protocol classes and their scaling properties as model capabilities increase, building on Greenblatt et al.'s control evaluation framework. Focuses on hierarchical and parallel control structures for superior safety-usefulness tradeoffs.",
        "status": "Active"
      }
    ],
    "benchmarks": []
  },
  {
    "name": "Ashgro",
    "url": "https://ashgro.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI Safety Info",
        "description": "Project made possible through Ashgro's fiscal sponsorship, involving a global team",
        "status": "Active"
      },
      {
        "name": "Timaeus",
        "description": "Project using Ashgro as fiscal sponsor for nonprofit operations and admin handling",
        "status": "Active"
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Luke Freeman",
        "role": "Board of Directors"
      },
      {
        "name": "Greg Sadler",
        "role": "Board of Directors"
      },
      {
        "name": "JJ Hepburn",
        "role": "Board of Directors"
      },
      {
        "name": "Plex",
        "role": "Coordinator, AI Safety Info"
      },
      {
        "name": "Jesse Hoogland",
        "role": "Executive Director, Timaeus"
      }
    ]
  },
  {
    "name": "Deducto",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI reasoning verification company.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Luthien",
    "url": "https://luthien.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Center for Human-Compatible AI",
    "url": "https://humancompatible.ai/",
    "type": "Academic",
    "country": "USA",
    "mission": "Stuart Russell's research center at UC Berkeley focused on building AI systems that are provably beneficial to humans.",
    "focus_areas": [
      "Alignment",
      "Control",
      "Cooperative AI"
    ],
    "projects": [
      {
        "name": "TASRA: A Taxonomy of Societal-Scale Risks from AI",
        "description": "A taxonomic framework for categorizing and understanding societal-scale risks posed by AI systems.",
        "status": "published",
        "url": "https://arxiv.org/abs/2310.17688",
        "paper_url": "https://arxiv.org/abs/2310.17688",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "Managing AI Risks in an Era of Rapid Progress",
        "description": "A collaborative paper addressing the management of AI risks as the technology rapidly advances.",
        "status": "published",
        "url": "https://arxiv.org/abs/2310.17688",
        "paper_url": "https://arxiv.org/abs/2310.17688",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "Automatically Auditing Large Language Models via Discrete Optimization",
        "description": "A method for automatically auditing large language models using discrete optimization techniques.",
        "status": "published",
        "url": "https://arxiv.org/abs/2303.04381",
        "paper_url": "https://arxiv.org/abs/2303.04381",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "On Representation Complexity of Model-based and Model-free Reinforcement Learning",
        "description": "Analysis of representation complexity differences between model-based and model-free reinforcement learning approaches.",
        "status": "published",
        "url": "https://arxiv.org/abs/2310.01706",
        "paper_url": "https://arxiv.org/abs/2310.01706",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "Humans decompose tasks by trading off utility and computational cost",
        "description": "Study showing how humans decompose complex tasks by balancing utility gains against computational costs.",
        "status": "published",
        "url": "",
        "paper_url": "",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "Provably Efficient Reinforcement Learning via Surprise Bound",
        "description": "A reinforcement learning approach that achieves provable efficiency through surprise bound mechanisms.",
        "status": "published",
        "url": "",
        "paper_url": "",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability",
        "description": "Framework for efficient offline goal-conditioned RL with theoretical guarantees under general function approximation.",
        "status": "published",
        "url": "https://arxiv.org/abs/2302.03770",
        "paper_url": "https://arxiv.org/abs/2302.03770",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning",
        "description": "An actor-critic algorithm using importance weighting for optimal conservative offline reinforcement learning.",
        "status": "published",
        "url": "",
        "paper_url": "",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "Bridging RL Theory and Practice with the Effective Horizon",
        "description": "Work connecting reinforcement learning theory to practice through the concept of effective horizon.",
        "status": "published",
        "url": "",
        "paper_url": "",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      },
      {
        "name": "Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian",
        "description": "A method for optimal conservative offline reinforcement learning using augmented Lagrangian techniques.",
        "status": "published",
        "url": "",
        "paper_url": "",
        "focus_areas": [
          "Alignment",
          "Control",
          "Cooperative AI"
        ]
      }
    ],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Anna Googol",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "FAR.AI",
    "url": "https://far.ai/",
    "type": "Nonprofit",
    "country": "USA",
    "mission": "",
    "focus_areas": [
      "Evals",
      "Red-teaming",
      "Alignment"
    ],
    "projects": [
      {
        "name": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics",
        "description": "Presents the Attempt to Persuade Eval (APE) benchmark that tests how willing LLMs are to generate content aimed at shaping beliefs and behavior on harmful topics.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Evals",
          "Red-teaming",
          "Alignment"
        ]
      },
      {
        "name": "Exploiting Novel GPT-4 APIs",
        "description": "Red-team study of GPT-4 APIs showing that fine-tuning on as few as 15 harmful examples can remove core safeguards, and reveals vulnerabilities in function calling and knowledge retrieval.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Evals",
          "Red-teaming",
          "Alignment"
        ]
      },
      {
        "name": "Inverse Scaling: When Bigger Isn't Better",
        "description": "Presents 11 instances of inverse scaling where language models get worse with scale rather than better, selected from 99 submissions in an open competition.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Evals",
          "Red-teaming",
          "Alignment"
        ]
      },
      {
        "name": "Beyond the Board: Exploring AI Robustness Through Go",
        "description": "Tests three approaches to defend Go AIs from adversarial strategies, finding that defenses protect against known adversaries but uncover new adversaries that undermine these defenses.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Evals",
          "Red-teaming",
          "Alignment"
        ]
      },
      {
        "name": "Adversarial Policies Beat Superhuman Go AIs",
        "description": "Describes an attack on KataGo that tricks the superhuman Go AI into making serious blunders without learning to play Go better, demonstrating surprising failure modes.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Evals",
          "Red-teaming",
          "Alignment"
        ]
      },
      {
        "name": "InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques",
        "description": "Collection of 17 semi-synthetic transformers with known circuits trained using SIIT, providing a benchmark for evaluating mechanistic interpretability techniques.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Evals",
          "Red-teaming",
          "Alignment"
        ]
      },
      {
        "name": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
        "description": "Demonstrates a method to modify neural networks using quantization bottlenecks to make their internals more interpretable and steerable with minimal performance degradation.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Evals",
          "Red-teaming",
          "Alignment"
        ]
      }
    ],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "MIT Algorithmic Alignment Group",
    "url": "https://algorithmicalignment.csail.mit.edu/",
    "type": "Academic",
    "country": "USA",
    "mission": "",
    "focus_areas": [
      "Alignment",
      "Control",
      "Governance"
    ],
    "projects": [],
    "benchmarks": [],
    "key_people": [
      {
        "name": "Dylan Hadfield-Menell",
        "role": "Assistant Professor"
      },
      {
        "name": "Andres Campero",
        "role": "Researcher"
      },
      {
        "name": "Anish Athalye",
        "role": "Researcher"
      },
      {
        "name": "Stephen Casper",
        "role": "PhD Student"
      }
    ]
  },
  {
    "name": "NYU Alignment Research Group",
    "url": "https://wp.nyu.edu/arg/",
    "type": "Academic",
    "country": "USA",
    "mission": "Sam Bowman's research group at NYU focusing on language model alignment and evaluation.",
    "focus_areas": [
      "Alignment",
      "Evals",
      "Interpretability"
    ],
    "projects": [],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "Rethink Priorities",
    "url": "https://rethinkpriorities.org/",
    "type": "Nonprofit",
    "country": "USA",
    "mission": "",
    "focus_areas": [
      "Governance",
      "Policy",
      "Evals"
    ],
    "projects": [
      {
        "name": "Mapping salmon welfare: sea lice treatments",
        "description": "This second report in the series is a factual overview of the landscape of sea lice treatments in the salmon farming industry.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Mapping salmon welfare: a global overview",
        "description": "We describe the broad patterns of the global salmon farming industry.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Interrater reliability of the Cumulative Pain Framework: Welfare threats in egg and chicken production",
        "description": "We assessed the extent to which estimates of Cumulative Pain based on the Welfare Footprint Framework vary depending on who is conducting the evaluation.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "A Moral Parliament Tool for Distributing Movement Building Resources",
        "description": "We present a Moral Parliament Tool that can be used to set funding priorities across different kinds of EA movement-building projects.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "A Moral Parliament Tool for Distributing Resources across Farmed Animal Recipients",
        "description": "We present a Moral Parliament Tool to set funding priorities across different kinds of farmed animals: chickens, fish, shrimp, and insects.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "A Moral Parliament Tool for Evaluating GiveWell Projects",
        "description": "We present a Moral Parliament Tool that decides how to allocate resources to GiveWell-evaluated global health charities.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "A Moral Parliament Tool for Navigating Bioethical Disagreements",
        "description": "We present a Moral Parliament Tools that incorporates delegates with diverse bioethical views.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Instructions for modifying the Moral Parliament Tool",
        "description": "In this guide we provide instructions to modifying Moral Parliament Tool to specific allocation needs.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Navigating donation dilemmas: customizable Moral Parliament tools for better decision-making",
        "description": "We present our Moral Parliament customization sequence.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Research summary: Grinding parameters for humane slaughter of yellow mealworm larvae",
        "description": "We explore plate size and fillers as parameters for the humane slaughter of farmed yellow mealworm larvae.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Research Summary: The Era Beyond Eisemann\u2014Insect Pain in the 21st Century",
        "description": "This post is a short summary of The Era Beyond Eisemann et al. (1984): Insect pain in the 21st century, a peer-reviewed, open-access publication on insect welfare.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Improving the Lead Impact Model",
        "description": "This report was initially produced by Rethink Priorities for Open Philanthropy and Pure Earth during August and September 2024.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Adoption and uses of LLMs among U.S. tech workers",
        "description": "A survey of 1963 U.S. respondents with software development or programming backgrounds, focusing on how and to what extent they use large language models (LLMs) in their work.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Estimating the usage and utility of LLMs in the US general public",
        "description": "This report presents findings from a survey of 1,370 U.S. adults conducted in November 2024. The report assesses LLM awareness and usage, highlighting prevalence, use cases, and utility.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "LLM use in the workplace Qualitative interviews with LLM power users and early adopters",
        "description": "This report covers findings from 19 semi-structured interviews with self-identified LLM power users, conducted between April and July of 2024.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Cost-effectiveness analysis of Lafiya Nigeria intervention",
        "description": "This document is intended to explain some of the modeling decisions we made in assessing the cost-effectiveness of Lafiya Nigeria's intervention.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Database of sources investigating interventions to reduce meat and animal product consumption",
        "description": "We developed a database of sources investigating interventions to reduce meat and edible animal product consumption, which we are now releasing as a resource to help advocates and researchers.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Overview and funding priorities for reproductive health in East and Francophone West Africa",
        "description": "Medical manufacturing and regional regulatory harmonization efforts. Overview and funding priorities for reproductive health in East and Francophone West Africa.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Data systems for malaria burden estimation: Challenges, initiatives, and opportunities",
        "description": "This report was commissioned by GiveWell and produced by Rethink Priorities from August to September 2023. The primary focus of the report is a landscape analysis to provide insights into data systems for malaria burden estimation.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Plant-Based Diet-Shift Initiative Case Studies: New York City",
        "description": "This case study presents a comprehensive analysis of New York City's multi-tiered nutritional policy interventions, which have demonstrably reduced municipal food-related emissions.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Plant-Based Diet-Shift Initiative Case Studies: Coolfood",
        "description": "This systematic evaluation quantifies the effectiveness of Coolfood's institutional intervention model, which has achieved measurable reductions in food-related emissions.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Plant-Based Diet-Shift Initiative Case Studies: Denmark's Plant-Based Food Grant",
        "description": "This policy analysis examines Denmark's structural intervention in agricultural systems through targeted fiscal mechanisms supporting 71 plant-based initiatives across production sectors.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Plant-Based Diet-Shift Initiative Case Studies: GoodDot Alternative Protein Company",
        "description": "This market analysis evaluates GoodDot's operational framework in India's emerging alternative protein sector, documenting their methodological approach to product development.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Plant-Based Diet-Shift Initiative Case Studies: German Retailer Transitions",
        "description": "This case evaluation examines Lidl Germany's data-driven protein transition framework targeting a 20/80 plant/animal ratio by 2030, with potential to reduce GHG emissions.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Research Summary: Exploring Physiological Indicators of Farmed Insect Welfare",
        "description": "This post is a summary of Review: Exploring correctness, usefulness, and feasibility of potential physiological operational welfare indicators for farmed insects.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Health Systems Strengthening",
        "description": "Our report focuses on exploring health systems strengthening (HSS) as a potential new cause area for Open Philanthropy. We examined a range of interventions to improve health systems capacity.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Forecasting Farmed Animal Numbers in 2033",
        "description": "We produced rough-and-ready forecasts of the number of animals farmed in 2033 with the aim of helping advocates and funders with prioritization decisions.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "EU Farmed Animal Welfare Policy: Strategic Assessment (2025)",
        "description": "A strategic assessment for philanthropic funders who wish to support greater protections for farmed animals in European Union (EU) legislation and animal advocacy organisations.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "US Foreign Aid Funding Pause: A Framework for Giving in Uncertain Times",
        "description": "In January 2025, the US government initiated a 90-day pause on new foreign aid. This report provides a framework for giving during uncertain times.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Promising Interventions in Maternal and Neonatal Health",
        "description": "This report, commissioned by Open Philanthropy (OP), provides an initial exploration of maternal and neonatal health (MNH) interventions in low- and middle-income countries.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Landscape of Malaria Bednet Programs",
        "description": "An in-depth analysis of malaria bednet programs across 14 countries uncovers critical shifts in funding patterns, distribution strategies, and technological innovations.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Effects of Restrictive Animal Product Alternative Labeling Laws on Supply-chain Costs",
        "description": "After conducting expert interviews, we deem it unlikely that the animal products alternative sector will experience significant supply-chain cost increases from restrictive labeling laws.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Can Black Soldier Fly Larvae (BSFL) producers displace fishmeal?",
        "description": "We investigated costs of production at four of the largest BSFL producers to better understand the prospects of them making major inroads into the aquatic animal feed market.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Strategic Directions for a Digital Consciousness Model",
        "description": "The Worldview Investigations Team is in the process of building a model to estimate the probabilities of consciousness in near-future AIs.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Insect farming: investment trends and projected production capacity",
        "description": "Since 2014, $2B of investment has flowed into insect farming. Investment flows were growing rapidly prior to 2021, but have since slowed.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Hypertension in low- and middle-income countries",
        "description": "This report explores the burden, interventions, and potential for philanthropic funding related to hypertension\u2014or high blood pressure\u2014a risk factor for cardiovascular disease.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Pulse: US attitudes and awareness regarding effective giving and philanthropic cause areas",
        "description": "Rethink Priorities' Pulse project seeks to address the critical knowledge gap around US public awareness and perceptions regarding effective giving and related topics.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "The Welfare of Digital Minds",
        "description": "The research agenda explores critical philosophical and empirical questions about the potential welfare and moral status of digital minds, focusing on understanding when digital minds may have morally relevant welfare.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      },
      {
        "name": "Resource Allocation: A Research Agenda",
        "description": "Rethink Priorities' Worldview Investigations research agenda on resource allocation challenges and methodologies.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Governance",
          "Policy",
          "Evals"
        ]
      }
    ],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "Centre for the Study of Existential Risk",
    "url": "https://www.cser.ac.uk/",
    "type": "Academic",
    "country": "UK",
    "mission": "Cambridge University research centre studying existential risks including from advanced AI.",
    "focus_areas": [
      "Governance",
      "Policy",
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "Quantified Uncertainty Research Institute",
    "url": "https://quantifieduncertainty.org/",
    "type": "Nonprofit",
    "country": "USA",
    "mission": "Research institute developing tools and methods for quantifying uncertainty and improving forecasting.",
    "focus_areas": [
      "Forecasting",
      "Evals"
    ],
    "projects": [],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "Forecasting Research Institute",
    "url": "https://forecastingresearch.org/",
    "type": "Nonprofit",
    "country": "USA",
    "mission": "",
    "focus_areas": [
      "Forecasting",
      "Evals"
    ],
    "projects": [
      {
        "name": "Improving Judgments of Existential Risk: Better Forecasts, Questions, Explanations, Policies",
        "description": "Research on improving forecasting methods for existential risk assessment.",
        "status": "published",
        "url": "https://ssrn.com/abstract=4013668",
        "paper_url": "https://ssrn.com/abstract=4013668",
        "focus_areas": [
          "Forecasting",
          "Evals"
        ]
      },
      {
        "name": "Reciprocal Scoring: A Method for Forecasting Unanswerable Questions",
        "description": "A method for scoring and incentivizing accuracy on questions that may never resolve.",
        "status": "published",
        "url": "https://ssrn.com/abstract=3954498",
        "paper_url": "https://ssrn.com/abstract=3954498",
        "focus_areas": [
          "Forecasting",
          "Evals"
        ]
      },
      {
        "name": "A Better Crystal Ball: The Right Way to Think About the Future",
        "description": "Analysis of forecasting methods and approaches for thinking about future events.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Forecasting",
          "Evals"
        ]
      },
      {
        "name": "Conditional Trees: A Method for Generating Informative Questions about Complex Topics",
        "description": "A new process for generating high-value forecasting questions using simplified Bayesian networks as frameworks.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Forecasting",
          "Evals"
        ]
      }
    ],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "Institute for AI Policy and Strategy",
    "url": "https://www.iaps.ai/",
    "type": "Nonprofit",
    "country": "USA",
    "mission": "",
    "focus_areas": [
      "Policy",
      "Governance"
    ],
    "projects": [
      {
        "name": "The Emergence of Autonomous Cyber Attacks: Analysis and Implications",
        "description": "Analysis of Anthropic's detection of the first publicly known autonomous AI cyber espionage campaign in November 2025. Examines implications for nation-state operations and defensive capabilities.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Policy",
          "Governance"
        ]
      },
      {
        "name": "Building AI Surge Capacity: Mobilizing Technical Talent into Government for AI-Related National Security Crises",
        "description": "Report addressing the U.S. government's lack of specialized AI security talent and hiring mechanisms for AI-related national security crises.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Policy",
          "Governance"
        ]
      },
      {
        "name": "Policy Options for Preserving Chain of Thought Monitorability",
        "description": "Framework for determining when coordination mechanisms are needed to preserve chain of thought monitorability in AI systems as competitive pressures drive toward non-monitorable architectures.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Policy",
          "Governance"
        ]
      },
      {
        "name": "Accelerating AI Data Center Security",
        "description": "Analysis of AI data center security risks from sophisticated adversaries like China and Russia seeking to steal intellectual property or sabotage AI systems.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Policy",
          "Governance"
        ]
      },
      {
        "name": "Verification for International AI Governance",
        "description": "Analysis of potential international AI agreements and verification methods, examining political feasibility based on verifiability of state compliance.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Policy",
          "Governance"
        ]
      },
      {
        "name": "Managing Risks from Internal AI Systems",
        "description": "Report on risks from powerful AI systems used internally before public release, recommending technical and policy solutions for internal AI systems with capabilities ahead of public frontier.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Policy",
          "Governance"
        ]
      },
      {
        "name": "Countering AI Chip Smuggling Has Become a National Security Priority: An Updated Playbook for Preventing AI Chip Smuggling to the PRC",
        "description": "Working paper cataloguing evidence of substantial AI chip smuggling into China and providing updated strategies to counter this national security threat.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Policy",
          "Governance"
        ]
      },
      {
        "name": "Asymmetry by Design: Boosting Cyber Defenders with Differential Access to AI",
        "description": "Strategy framework for tilting cybersecurity balance toward defense through differential access to AI-powered cyber capabilities, introducing three approaches: Promote Access, Manage Access, and Deny by Default.",
        "status": "published",
        "url": null,
        "paper_url": null,
        "focus_areas": [
          "Policy",
          "Governance"
        ]
      }
    ],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "Simon Institute for Longterm Governance",
    "url": "https://www.simoninstitute.ch/",
    "type": "Nonprofit",
    "country": "Switzerland",
    "mission": "Geneva-based institute supporting multilateral governance of frontier technologies.",
    "focus_areas": [
      "Governance",
      "Policy"
    ],
    "projects": [],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "Safe Superintelligence Inc.",
    "url": "https://ssi.inc/",
    "type": "Lab Safety Team",
    "country": "USA",
    "mission": "",
    "focus_areas": [
      "Alignment",
      "Control"
    ],
    "projects": [],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "TruthfulAI",
    "url": "https://www.truthful.ai/",
    "type": "Nonprofit",
    "country": "UK",
    "mission": "",
    "focus_areas": [
      "Alignment",
      "Evals"
    ],
    "projects": [],
    "benchmarks": [],
    "key_people": []
  },
  {
    "name": "University of Oxford",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Alessandro Abate",
        "role": "Professor"
      },
      {
        "name": "Fazl Barez",
        "role": "Researcher"
      },
      {
        "name": "Michael Cohen",
        "role": "Researcher"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Johns Hopkins University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Anqi Liu",
        "role": "Assistant Professor"
      },
      {
        "name": "Andrea Wynn",
        "role": "PhD Student"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Western University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Aysajan Eziz",
        "role": null
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Cornell University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Bart Selman",
        "role": "Professor"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "University of Texas at Austin",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Brad Knox",
        "role": "Research Associate Professor"
      },
      {
        "name": "Scott Aaronson",
        "role": "Professor"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Stanford University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Clark Barrett",
        "role": "Professor (Research)"
      },
      {
        "name": "Percy Liang",
        "role": "Professor"
      },
      {
        "name": "Gabe Mukobi",
        "role": "Researcher"
      },
      {
        "name": "Gabriel Mukobi",
        "role": "Researcher"
      },
      {
        "name": "Jesse Mu",
        "role": "Researcher"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "University of Cambridge",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "David Krueger",
        "role": "Assistant Professor"
      },
      {
        "name": "Alex Chan",
        "role": null
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Massachusetts Institute of Technology",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Dylan Hadfield-Menell",
        "role": "Assistant Professor"
      },
      {
        "name": "Can AI agents learn to be good?",
        "role": null
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Princeton University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Elad Hazan",
        "role": "Professor"
      },
      {
        "name": "Benjamin Eysenbach",
        "role": "Researcher"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "University of Pavia",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Federico Faroldi",
        "role": "Professor of Ethics Law and AI"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Carnegie Mellon University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Aashiq Muhamed",
        "role": "PhD student"
      },
      {
        "name": "Zachary Lipton",
        "role": "Assistant Professor"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "University of Hong Kong",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Abeer Sharma",
        "role": null
      },
      {
        "name": "Simon Goldstein",
        "role": "Researcher"
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "University of Connecticut",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Aidan Kierans",
        "role": null
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "University of Bath",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Aishwarya Gurung",
        "role": null
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "National Institute of Technology Karnataka",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Allan Suresh",
        "role": null
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "University of Waterloo",
    "type": "Academic",
    "focus_areas": [
      "AI Safety Research"
    ],
    "key_people": [
      {
        "name": "Amir-Hossein Karimi",
        "role": null
      }
    ],
    "mission": "Academic institution with researchers in AI safety",
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "UC Berkeley",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Anca Dragan",
        "role": "Associate Professor"
      },
      {
        "name": "Jacob Steinhardt",
        "role": "Assistant Professor"
      },
      {
        "name": "Shiry Ginosar",
        "role": "Assistant Professor"
      },
      {
        "name": "Stuart Russell",
        "role": "Professor"
      },
      {
        "name": "Arjun Panickssery",
        "role": "Researcher"
      },
      {
        "name": "Helena Vasconcelos",
        "role": "Researcher"
      },
      {
        "name": "Michael Chen",
        "role": "Researcher"
      },
      {
        "name": "Scott Emmons",
        "role": "Researcher"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "NJIT",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Arnob Ghosh",
        "role": "Assistant Professor"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Harvard University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Finale Doshi-Velez",
        "role": "Professor"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "University of Toronto",
    "type": "Academic",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Roger Grosse",
        "role": "Professor"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "New York University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Sam Bowman",
        "role": "Professor"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Mila",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Yoshua Bengio",
        "role": "Professor"
      },
      {
        "name": "Alan Chan",
        "role": "Researcher"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "NIT Karnataka",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Allan Suresh",
        "role": "Researcher"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "OpenMined",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Andrew Trask",
        "role": "Researcher"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Allen Institute for AI",
    "type": "Academic",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Ashwin Kalyan",
        "role": "Researcher"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Open Philanthropy",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Daniel Dewey",
        "role": "Researcher"
      },
      {
        "name": "Holden Karnofsky",
        "role": "Co-CEO"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Independent",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Daniel Kokotajlo",
        "role": "Researcher"
      },
      {
        "name": "John Wentworth",
        "role": "Researcher"
      },
      {
        "name": "Robert Miles",
        "role": "AI Safety Educator"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Northeastern University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "David Bau",
        "role": "Assistant Professor"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Sentience Institute",
    "type": "Academic",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Jacy Reese Anthis",
        "role": "Researcher"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Imbue",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Josh Albrecht",
        "role": "Researcher"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Brown University",
    "type": "Academic",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Michael Littman",
        "role": "Professor"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "LessWrong",
    "type": "Research",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Oliver Habryka",
        "role": "Founder"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  },
  {
    "name": "Future of Humanity Institute",
    "type": "Academic",
    "focus_areas": [
      "AI Safety"
    ],
    "key_people": [
      {
        "name": "Owen Cotton-Barratt",
        "role": "Researcher"
      }
    ],
    "source": "FLI AI Existential Safety Community"
  }
]