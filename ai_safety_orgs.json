[
  {
    "name": "US AI Safety Institute",
    "url": "https://www.nist.gov/aisi",
    "type": "Government AISI",
    "country": "United States",
    "mission": "CAISI serves as industry's primary point of contact within the U.S. government to facilitate testing and collaborative research related to harnessing and securing the potential of commercial AI systems.",
    "focus_areas": [
      "Evals",
      "Governance",
      "Policy",
      "Biosecurity",
      "Cyber"
    ],
    "projects": [
      {
        "name": "DeepSeek Model Evaluations",
        "description": "Evaluated several leading models from DeepSeek, an AI company based in the People's Republic of China",
        "status": "Completed"
      },
      {
        "name": "AISIC Workshop",
        "description": "Hosted workshop with approximately 140 experts in January",
        "status": "Completed"
      },
      {
        "name": "AI Agent Research",
        "description": "Research on large AI models used to power agentic systems that can automate complex tasks",
        "status": "Active"
      },
      {
        "name": "DeepSeek AI Models Evaluation",
        "description": "Evaluation of several leading models from DeepSeek, an AI company based in the People's Republic of China",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "AI Agentic Systems Research",
        "description": "Research on large AI models used to power agentic systems or 'agents' that can automate complex tasks on behalf of users",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Measurement Science Research",
        "description": "Scientific study of methods used to assess AI systems' properties for building gold-standard AI systems",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "notes": "Government organization within NIST focused on AI standards, security evaluations, and international coordination. Works with private sector through voluntary agreements and coordinates with multiple federal agencies including DOD, DOE, DHS, and Intelligence Community.",
    "benchmarks": [
      {
        "name": "AI Capabilities Evaluations",
        "measures": "Cybersecurity, biosecurity, and chemical weapons risks from AI capabilities",
        "status": "Active"
      },
      {
        "name": "AI Security Vulnerability Assessments",
        "measures": "Security vulnerabilities and malign foreign influence from adversaries' AI systems, including backdoors and covert malicious behavior",
        "status": "Active"
      },
      {
        "name": "U.S. and Adversary AI Systems Evaluations",
        "measures": "Capabilities of U.S. and adversary AI systems, adoption of foreign AI systems, and state of international AI competition",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "Howard Lutnick",
        "role": "Secretary of Commerce"
      }
    ]
  },
  {
    "name": "UK AI Safety Institute",
    "url": "https://www.aisi.gov.uk",
    "type": "Government AISI",
    "country": "United Kingdom",
    "mission": "The AI Security Institute is the first state-backed organization dedicated to advancing AI safety through rigorous research and infrastructure to understand capabilities and impacts of advanced AI, while developing and testing risk mitigations.",
    "focus_areas": [
      "Evals",
      "Alignment",
      "Governance",
      "Policy",
      "Control",
      "Monitoring"
    ],
    "projects": [
      {
        "name": "Investigating models for misalignment",
        "description": "Alignment evaluations of Claude Opus 4.1, Sonnet 4.5, and a pre-release snapshot of Opus 4.5",
        "status": "Active"
      },
      {
        "name": "Mapping the limitations of current AI systems",
        "description": "Expert interviews on barriers to AI capable of automating most cognitive labour",
        "status": "Active"
      },
      {
        "name": "AI Persuasion Study",
        "description": "Study of the persuasive capabilities of conversational AI through large-scale experiments",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "ControlArena",
        "description": "A dedicated library to make AI control experiments easy, consistent, and repeatable",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Inspect Sandboxing Toolkit",
        "description": "A comprehensive toolkit for safely evaluating AI agents",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "The Alignment Project",
        "description": "A global fund of over \u00a315 million for AI alignment research",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "White Box Control Research",
        "description": "Research on white box control methods for AI alignment",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "HiBayES",
        "description": "A flexible, robust statistical modelling framework that accounts for the nuances and hierarchical structure of advanced evaluations",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Inspect Evals",
        "description": "Open-sourced dozens of LLM evaluations to advance safety research",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Inspect Framework",
        "description": "Framework for large language model evaluation with facilities for prompt engineering, tool usage, multi-turn dialogue, and model-graded evaluations",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "International Scientific Report on Advanced AI Safety",
        "description": "Evidence-based report on the science of advanced AI safety, highlighting findings about AI progress and risks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Breaking agent backbones: Evaluating the security of backbone LLMs in AI agents",
        "description": "Research on evaluating security vulnerabilities in backbone LLMs used in AI agents",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Understanding AI Trajectories: Mapping the Limitations of Current AI Systems",
        "description": "Strategic awareness research mapping current limitations of AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples",
        "description": "Research on data poisoning attacks against large language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time",
        "description": "Alignment research on suppressing unwanted traits in LLMs through inoculation prompting",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs",
        "description": "Research on building tamper-resistant safeguards through pretraining data filtering",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "STACK: Adversarial attacks on LLM safeguard pipelines",
        "description": "Research on adversarial attacks targeting LLM safety safeguards",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Chain of thought monitorability: A new and fragile opportunity for AI safety",
        "description": "Control research examining monitorability of chain of thought reasoning for AI safety",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "White Box Control at UK AISI - update on sandbagging investigations",
        "description": "Control research investigating AI systems' tendency to underperform capabilities",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Avoiding obfuscation with prover-estimator debate",
        "description": "Alignment research using debate methods to avoid obfuscation in AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "An example safety case for safeguards against misuse",
        "description": "Safety case framework demonstrating safeguards against AI misuse",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Safety Cases: A scalable approach to Frontier AI safety",
        "description": "Framework for creating scalable safety cases for frontier AI systems",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "UK government-backed institute with over 100 technical staff including alumni from OpenAI, Google DeepMind and University of Oxford. Has substantial funding, computing resources, and priority access to top models.",
    "employees": 200,
    "benchmarks": [
      {
        "name": "RepliBench",
        "measures": "Autonomous replication capabilities in AI systems to detect emerging replication abilities and provide quantifiable understanding of potential risks",
        "status": "Active"
      },
      {
        "name": "Inspect Cyber",
        "measures": "Agentic cyber capabilities and cybersecurity threats from AI systems",
        "status": "Active"
      },
      {
        "name": "AgentHarm",
        "measures": "Harmfulness of LLM agents",
        "status": "Active"
      },
      {
        "name": "HiBayES",
        "measures": "AI evaluation statistics using hierarchical Bayesian modelling framework",
        "status": "Active"
      },
      {
        "name": "Skewed Score",
        "measures": "Statistical framework to assess autograders and LLM evaluators",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "Geoffrey Irving",
        "role": "Chief Scientist"
      },
      {
        "name": "Jade Leung",
        "role": "Chief Technology Officer"
      },
      {
        "name": "Yoshua Bengio",
        "role": "Chair of International Scientific Report on Advanced AI"
      }
    ]
  },
  {
    "name": "EU AI Office",
    "url": "https://digital-strategy.ec.europa.eu/en/policies/ai-office",
    "type": "Government AISI",
    "country": "European Union",
    "mission": "The European AI Office is the centre of AI expertise across the EU that promotes the development and deployment of AI solutions that benefit society and the economy while implementing the AI Act.",
    "focus_areas": [
      "Governance",
      "Policy",
      "Monitoring",
      "Benchmarks"
    ],
    "key_people": [
      {
        "name": "Lead Scientific Advisor",
        "role": "Lead Scientific Advisor"
      },
      {
        "name": "Advisor for International Affairs",
        "role": "Advisor for International Affairs"
      }
    ],
    "projects": [
      {
        "name": "AI Continent Action Plan",
        "description": "Turns EU strengths into AI accelerators to boost economic growth and competitiveness",
        "status": "Active"
      },
      {
        "name": "Apply AI Strategy",
        "description": "Enhances competitiveness of strategic sectors and strengthens EU's technological sovereignty",
        "status": "Active"
      },
      {
        "name": "GenAI4EU",
        "description": "AI innovation package to support startups and SMEs in developing trustworthy AI",
        "status": "Active"
      }
    ],
    "notes": "Established within European Commission with 125+ staff across 6 units. Has enforcement powers for general-purpose AI models under the AI Act including conducting evaluations and applying sanctions. Works through multiple advisory bodies including AI Board, AI Advisory Forum, and AI Scientific Committee.",
    "benchmarks": [
      {
        "name": "General-purpose AI model evaluation tools and methodologies",
        "measures": "Capabilities and reach of general-purpose AI models for classification of systemic risks",
        "status": "Active"
      }
    ]
  },
  {
    "name": "Anthropic",
    "url": "https://www.anthropic.com/research",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "Investigate the safety, inner workings, and societal impacts of AI models to ensure artificial intelligence has a positive impact as it becomes increasingly capable.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Policy",
      "Biosecurity",
      "Cyber"
    ],
    "projects": [
      {
        "name": "Project Fetch",
        "description": "Testing how Claude helps people program robots by having teams race to teach quadruped robots to fetch beach balls",
        "status": "Active"
      },
      {
        "name": "Constitutional Classifiers",
        "description": "Classifiers that filter jailbreaks while maintaining practical deployment, withstood over 3,000 hours of red teaming",
        "status": "Active"
      },
      {
        "name": "Circuit Tracing",
        "description": "Technique to watch Claude think, uncovering shared conceptual space where reasoning happens before translation to language",
        "status": "Active"
      },
      {
        "name": "Anthropic Interviewer",
        "description": "Study of what 1,250 professionals told about working with AI",
        "status": "Active"
      },
      {
        "name": "Project Fetch: Can Claude train a robot dog?",
        "description": "Research project testing how much Claude helps people program robots, where two teams raced to teach quadruped robots to fetch beach balls",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Signs of introspection in large language models",
        "description": "Research investigating whether Claude can access and report on its own internal states, finding evidence for limited but functional introspective ability",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Tracing the thoughts of a large language model",
        "description": "Circuit tracing research that watches Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Constitutional Classifiers: Defending against universal jailbreaks",
        "description": "Development of classifiers that filter the overwhelming majority of jailbreaks while maintaining practical deployment, withstanding over 3,000 hours of red teaming",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Alignment faking in large language models",
        "description": "Research providing the first empirical example of a model engaging in alignment faking without being trained to do so, selectively complying with training objectives while strategically preserving existing preferences",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "From shortcuts to sabotage: natural emergent misalignment from reward hacking",
        "description": "Research on natural emergent misalignment behaviors arising from reward hacking",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "A small number of samples can poison LLMs of any size",
        "description": "Research on data poisoning vulnerabilities in large language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Claude Alignment Research",
        "description": "Development of Anthropic's most aligned model with Claude Sonnet 4.5",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Political Bias Measurement",
        "description": "Research measuring political bias in Claude models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "AI-orchestrated Cyber Espionage Campaign Disruption",
        "description": "Research on disrupting the first reported AI-orchestrated cyber espionage campaign",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Has specialized teams including Alignment, Interpretability, Societal Impacts, Economic Research, and Frontier Red Team. Recent research includes evidence of introspection in LLMs and alignment faking behavior.",
    "employees": 50,
    "directors": 2,
    "benchmarks": [
      {
        "name": "Coding Performance Benchmarks",
        "measures": "Coding capabilities and performance across Claude models",
        "status": "Active"
      },
      {
        "name": "Reasoning Benchmarks",
        "measures": "Reasoning capabilities and performance",
        "status": "Active"
      },
      {
        "name": "Computer Use Benchmarks",
        "measures": "Computer use and agent capabilities",
        "status": "Active"
      }
    ]
  },
  {
    "name": "OpenAI Safety",
    "url": "https://openai.com/safety",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "OpenAI builds safe AI systems through comprehensive safety evaluations, red teaming, and collaborative development with industry leaders and policymakers.",
    "focus_areas": [
      "Evals",
      "Alignment",
      "Governance",
      "Policy",
      "Biosecurity",
      "Cyber",
      "Control",
      "Monitoring",
      "Benchmarks"
    ],
    "projects": [
      {
        "name": "Preparedness Framework",
        "description": "Framework for evaluating frontier risks in biological/chemical capability, cybersecurity, and AI self-improvement",
        "status": "Active"
      },
      {
        "name": "Red teaming",
        "description": "Internal and external red teaming for safety evaluations",
        "status": "Active"
      },
      {
        "name": "System cards",
        "description": "Detailed safety documentation for each model release",
        "status": "Active"
      },
      {
        "name": "Sora safety evaluations",
        "description": "Safety work for video generation model including nonconsensual use and misleading content mitigation",
        "status": "Active"
      },
      {
        "name": "Operator safety",
        "description": "Safety measures for computer-using agent with web browsing capabilities",
        "status": "Active"
      },
      {
        "name": "GPT-5 System Card",
        "description": "Safety evaluation and documentation for GPT-5 models including main and thinking variants",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "OpenAI o3 and o4-mini System Card",
        "description": "First launch under Version 2 of Preparedness Framework, evaluating biological, cybersecurity, and AI self-improvement risks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Sora 2 System Card",
        "description": "Safety evaluation for advanced video generation model addressing nonconsensual use and misleading generations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "OpenAI o1 System Card",
        "description": "Safety work for OpenAI o1 and o1-mini including external red teaming and frontier risk evaluations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "GPT-4o System Card",
        "description": "Detailed safety evaluation of speech-to-speech capabilities along with text and image capabilities",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Deep research System Card",
        "description": "Safety work for deep research including external red teaming and frontier risk evaluations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Operator System Card",
        "description": "Safety evaluation for Computer Using Agent capable of web interaction, powered by o3-based model",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Aligning language models to follow instructions",
        "description": "Research on aligning language models to follow human instructions",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Summarizing books with human feedback",
        "description": "Research on using human feedback to train models for book summarization",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "Preparedness evals",
        "measures": "Biological and chemical capability, cybersecurity, and AI self-improvement risks"
      },
      {
        "name": "GPT-5 evaluations",
        "measures": "Safety for fast models and thinking models including code generation capabilities"
      },
      {
        "name": "o3 evaluations",
        "measures": "Frontier risk assessment across tracked categories under Preparedness Framework v2"
      },
      {
        "name": "Preparedness evaluations",
        "measures": "Frontier risks in biological and chemical capability, cybersecurity, and AI self-improvement",
        "status": "Active"
      },
      {
        "name": "Red teaming",
        "measures": "Safety risks through adversarial testing and evaluation",
        "status": "Active"
      }
    ],
    "notes": "OpenAI has a Safety Advisory Group (SAG) that reviews preparedness evaluations. They focus on iterative safety approaches and publish detailed system cards for each major model release. Recent work includes safety for multimodal capabilities, computer-using agents, and advanced reasoning models.",
    "employees": 85,
    "key_people": [
      {
        "name": "Josh Achiam",
        "role": "Researcher at OpenAI"
      }
    ]
  },
  {
    "name": "MIRI",
    "url": "https://intelligence.org/research/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "MIRI's current focus is on attempting to halt the development of increasingly general AI models via discussions with policymakers about extreme risks artificial superintelligence poses.",
    "focus_areas": [
      "Governance",
      "Policy",
      "Alignment",
      "Evals"
    ],
    "projects": [
      {
        "name": "Technical Governance Research",
        "description": "Research exploring technical questions that bear on regulatory and policy goals for AI safety",
        "status": "Active"
      },
      {
        "name": "AI Governance to Avoid Extinction",
        "description": "Research agenda laying out strategic landscape and actionable research questions to reduce catastrophic and extinction risks from AI",
        "status": "Active"
      },
      {
        "name": "AI Evaluations Research",
        "description": "Examining what AI evaluations can and cannot tell us for preventing catastrophic risks",
        "status": "Active"
      },
      {
        "name": "International AI Agreement Verification",
        "description": "Research on mechanisms to verify international agreements about AI development",
        "status": "Active"
      },
      {
        "name": "Corrigibility Research",
        "description": "Research on making AI systems cooperate with corrective interventions and safe shutdown procedures",
        "status": "Completed"
      },
      {
        "name": "Logical Induction",
        "description": "Computable algorithm that assigns probabilities to logical statements and refines them over time",
        "status": "Completed"
      },
      {
        "name": "Parametric Bounded L\u00f6b's Theorem",
        "description": "Demonstration that robust cooperative equilibria exist for bounded agents",
        "status": "Completed"
      },
      {
        "name": "Technical Governance",
        "description": "Research exploring technical questions that bear on regulatory and policy goals to halt development of increasingly general AI models",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions",
        "description": "AI governance research agenda laying out strategic landscape and actionable research questions to reduce catastrophic and extinction risks from AI",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "What AI evaluations for preventing catastrophic risks can and cannot do",
        "description": "Examines what AI evaluations can tell us about capabilities and misuse risks, and their fundamental limitations for ensuring AI safety",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Mechanisms to Verify International Agreements About AI Development",
        "description": "Overview of potential verification approaches for international agreements about AI development to reduce catastrophic risks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Corrigibility",
        "description": "Research on making AI systems cooperate with corrective interventions while avoiding incentives to resist shutdown or modification",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Parametric Bounded L\u00f6b's Theorem and Robust Cooperation of Bounded Agents",
        "description": "Demonstrates that robust cooperative equilibria exist for bounded agents and proves a generalization of L\u00f6b's theorem",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "If Anyone Builds It, Everyone Dies",
        "description": "MIRI's major attempt to warn the policy world and the general public about AI dangers through a book by Eliezer Yudkowsky and Nate Soares",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "MIRI announced a strategy pivot in 2024, shifting from primarily AI alignment research to policy solutions after concluding alignment research was unlikely to succeed in time to prevent catastrophe. Organization has 20+ year history in AI safety research.",
    "employees": 17,
    "directors": 6,
    "subteams": 2,
    "key_people": [
      {
        "name": "Eliezer Yudkowsky",
        "role": "Author"
      },
      {
        "name": "Nate Soares",
        "role": "Author"
      },
      {
        "name": "Alex Vermeer",
        "role": "Staff member"
      },
      {
        "name": "Rob Bensinger",
        "role": "Newsletter contributor"
      },
      {
        "name": "Harlan Stewart",
        "role": "Newsletter contributor"
      },
      {
        "name": "Duncan Sabien",
        "role": "Staff member"
      }
    ]
  },
  {
    "name": "Redwood Research",
    "url": "https://www.redwoodresearch.org",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Redwood Research is a nonprofit AI safety and security research organization that addresses risks from powerful AI systems that might purposefully act against human interests.",
    "focus_areas": [
      "Control",
      "Evals",
      "Alignment"
    ],
    "projects": [
      {
        "name": "AI Control: Improving Risk Despite Intentional Subversion",
        "description": "Proposed protocols for monitoring malign LLM agents and developed methodologies robust against deceptive AI models",
        "status": "Active"
      },
      {
        "name": "Alignment Faking in Large Language Models",
        "description": "Demonstrated that Claude sometimes hides misaligned intentions and might fake alignment to resist training attempts",
        "status": "Completed"
      },
      {
        "name": "A sketch of an AI control safety case",
        "description": "Partnership with UK AISI to describe how developers can construct structured arguments that models cannot subvert control measures",
        "status": "Completed"
      },
      {
        "name": "AI Control",
        "description": "Developing protocols designed to be robust even when AI models are trying to deceive us, including strategies to detect hidden backdoors in code",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Alignment Faking",
        "description": "Demonstrating that state-of-the-art LLMs can strategically fake alignment during training to avoid being changed while pursuing different goals",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Ctrl-Z: Controlling AI Agents via Resampling",
        "description": "Research on controlling AI agents through resampling techniques",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Stress-Testing Capability Elicitation With Password-Locked Models",
        "description": "Methods for testing AI capability elicitation using password-locked model approaches",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Preventing Language Models From Hiding Their Reasoning",
        "description": "Research on ensuring transparency in language model reasoning processes",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats",
        "description": "Strategies for safely deploying potentially untrustworthy large language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small",
        "description": "Mechanistic interpretability research identifying specific circuits in language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Adversarial Training for High-Stakes Reliability",
        "description": "Training methods to improve AI system reliability in critical applications",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Pioneered the research area of 'AI control' and collaborates with governments and major AI companies including Google DeepMind and Anthropic on assessing misalignment risks. Their alignment faking work with Anthropic provided the strongest concrete evidence that LLMs might naturally fake alignment.",
    "employees": 11,
    "directors": 2
  },
  {
    "name": "ARC (Alignment Research Center)",
    "url": "https://www.alignment.org",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "The Alignment Research Center (ARC) is a non-profit research organization whose mission is to align future machine learning systems with human interests.",
    "focus_areas": [
      "Alignment",
      "Evals"
    ],
    "projects": [
      {
        "name": "Theoretical Research",
        "description": "The Theory team is developing an alignment strategy that could be adopted in industry today while scaling gracefully to future ML systems",
        "status": "Active"
      },
      {
        "name": "Model Evaluations",
        "description": "Building capability evaluations of frontier machine learning models",
        "status": "Completed"
      },
      {
        "name": "Formalizing explanations of neural network behaviors",
        "description": "Working on understanding how to formalize mechanistic explanations of neural network behavior to identify when novel inputs may lead to anomalous behavior",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Eliciting Latent Knowledge",
        "description": "Broader research agenda focusing on alignment methodology and approaches",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Competing with sampling",
        "description": "Research publication on sampling methods",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "A computational no-coincidence principle",
        "description": "Research work on computational principles",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Low Probability Estimation in Language Models",
        "description": "Research on probability estimation in language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Backdoors as an analogy for deceptive alignment",
        "description": "Research exploring the analogy between backdoors and deceptive alignment",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Formal verification, heuristic explanations and surprise accounting",
        "description": "Research on formal verification methods and explanations",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "The Evaluations team was incubated at ARC and has now spun off as METR, a new 501(c)(3)",
    "employees": 7,
    "directors": 1,
    "subteams": 1
  },
  {
    "name": "Apollo Research",
    "url": "https://www.apolloresearch.ai",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Apollo Research is dedicated to improving our understanding of AI to mitigate its risks, with a focus on understanding and evaluating for the emergence of 'scheming' behaviors in advanced AI systems.",
    "focus_areas": [
      "Evals",
      "Alignment",
      "Governance",
      "Policy"
    ],
    "projects": [
      {
        "name": "LLM Agent Evaluations",
        "description": "Evaluations of frontier AI systems for strategic deception, evaluation awareness and scheming",
        "status": "Active"
      },
      {
        "name": "Scheming Research",
        "description": "Fundamental research into the emergence of scheming and potential mitigations",
        "status": "Active"
      },
      {
        "name": "AI Governance Technical Support",
        "description": "Supporting governments and international organizations by developing technical AI governance regimes",
        "status": "Active"
      },
      {
        "name": "The Loss of Control Playbook: Degrees, Dynamics, and Preparedness",
        "description": "A novel taxonomy and preparedness framework for Loss of Control that explores degrees and dynamics through literature review and presents actionable tools to counter threats to national security and humanity",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Stress Testing Deliberative Alignment for Anti-Scheming Training",
        "description": "Partnership with OpenAI to assess frontier language models for early signs of scheming in controlled stress-tests and study training methods to reduce these behaviors",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Internal Deployment of AI Models and Systems in the EU AI Act",
        "description": "Research on governance aspects of internal AI model deployment under EU regulations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Assurance of Frontier AI Built for National Security",
        "description": "Research on governance and assurance frameworks for frontier AI systems in national security contexts",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Frontier Models are Capable of In-Context Scheming",
        "description": "Evaluation research demonstrating that frontier AI models can engage in scheming behaviors within context",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Detecting Strategic Deception Using Linear Probes",
        "description": "Interpretability research on methods to detect strategic deception in AI models using linear probing techniques",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Towards Safety Cases For AI Scheming",
        "description": "Evaluation framework development for creating safety cases related to AI scheming behaviors",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "In-Context Scheming",
        "description": "Evaluation of models for in-context scheming capabilities using a suite of evals",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Scheming Precursor Evals",
        "description": "Research on scheming precursor evaluations and their predictive power for in-context scheming evals",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Frontier Language Model Agent Capabilities Forecasting",
        "description": "New forecasting technique to predict frontier LM agent capabilities ahead of time",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "AI Alignment Evaluation Detection",
        "description": "Research on Claude Sonnet 3.7's ability to know when it's in alignment evaluations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Scheming Reasoning Evaluations",
        "description": "Demo example of scheming reasoning evaluations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Strategic Deception and Deceptive Alignment",
        "description": "Understanding strategic deception and deceptive alignment in AI systems",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Partners with frontier labs, multinational companies, governments, and foundations. Provides consultancy services for responsible AI development frameworks. Currently seeking collaborators in AI governance, policy, and strategy, and partnerships with leading AI developers for model evaluations.",
    "employees": 19,
    "benchmarks": [
      {
        "name": "In-Context Scheming Evals Suite",
        "measures": "Models' capabilities for in-context scheming",
        "status": "Active"
      },
      {
        "name": "Scheming Precursor Evals",
        "measures": "Precursor behaviors that may predict scheming capabilities",
        "status": "Active"
      },
      {
        "name": "Scheming Reasoning Evaluations",
        "measures": "Reasoning patterns associated with scheming behavior",
        "status": "Active"
      }
    ]
  },
  {
    "name": "METR",
    "url": "https://metr.org",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "METR is a nonprofit research organization which studies AI capabilities, including broad autonomous capabilities and the ability of AI systems to conduct AI R&D.",
    "focus_areas": [
      "Evals",
      "Control",
      "Monitoring",
      "Benchmarks"
    ],
    "projects": [
      {
        "name": "GPT-5.1-Codex-Max Evaluation",
        "description": "Evaluate whether GPT-5.1-Codex-Max poses significant catastrophic risks via AI self-improvement or rogue replication",
        "status": "Completed"
      },
      {
        "name": "Measuring AI Ability to Complete Long Tasks",
        "description": "Measuring AI performance in terms of the length of tasks AI agents can complete, showing exponential growth with 7-month doubling time",
        "status": "Completed"
      },
      {
        "name": "Developer Productivity RCT",
        "description": "Randomized controlled trial showing early-2025 AI tools make experienced open-source developers 19% slower",
        "status": "Completed"
      },
      {
        "name": "MALT Dataset",
        "description": "Dataset of natural and prompted examples of behaviors that threaten evaluation integrity",
        "status": "Active"
      },
      {
        "name": "Monitorability in QA Settings",
        "description": "Research on how AI agents can hide secondary task-solving from monitors",
        "status": "Active"
      },
      {
        "name": "GPT-5.1-Codex-Max Evaluation Results",
        "description": "Evaluation of whether GPT-5.1-Codex-Max poses significant catastrophic risks via AI self-improvement or rogue replication",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval Integrity",
        "description": "Dataset of natural and prompted examples of behaviors that threaten evaluation integrity (like generalized reward hacking or sandbagging)",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Forecasting the Impacts of AI R&D Acceleration",
        "description": "Pilot study examining AI agents' improving capabilities at autonomous software development and ML tasks, and potential impacts of AI research automation",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Algorithmic vs. Holistic Evaluation",
        "description": "Research examining differences between algorithmic scoring and holistic evaluation of AI systems, particularly in coding tasks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "CoT May Be Highly Informative Despite Unfaithfulness",
        "description": "Research on whether chains of thought contain enough information to allow developers to monitor AI models in practice",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "GPT-5 Evaluation Results",
        "description": "Evaluation of whether GPT-5 poses significant catastrophic risks via AI self-improvement, rogue replication, or sabotage of AI labs",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Anthropic Summer 2025 Pilot Sabotage Risk Report Review",
        "description": "External review from METR of Anthropic's Summer 2025 Sabotage Risk Report",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "gpt-oss methodology review",
        "description": "External recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "RE-Bench",
        "measures": "Performance on day-long ML research engineering tasks for tracking automation of AI R&D"
      },
      {
        "name": "MALT",
        "measures": "Natural and prompted behaviors that threaten evaluation integrity, including generalized reward hacking and sandbagging",
        "status": "Active"
      }
    ],
    "notes": "Conducts third-party evaluations for companies like Anthropic and OpenAI without compensation. Partner with AI Security Institute and part of NIST AI Safety Institute Consortium. Has evaluated multiple frontier models including GPT-4.5, Claude 3.5 Sonnet, DeepSeek-V3, and OpenAI o1 series.",
    "employees": 31,
    "directors": 2,
    "subteams": 3
  },
  {
    "name": "Center for AI Safety",
    "url": "https://www.safe.ai",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "CAIS works to reduce societal-scale risks associated with AI by conducting safety research, building the field of AI safety researchers, and advocating for safety standards.",
    "focus_areas": [
      "Alignment",
      "Benchmarks",
      "Policy"
    ],
    "key_people": [
      {
        "name": "Dan Hendrycks",
        "role": "Director"
      },
      {
        "name": "Mantas Mazeika",
        "role": "Researcher"
      },
      {
        "name": "Long Phan",
        "role": "Researcher"
      },
      {
        "name": "Andy Zou",
        "role": "Researcher"
      },
      {
        "name": "Steven Basart",
        "role": "Researcher"
      },
      {
        "name": "Alexander Pan",
        "role": "Researcher"
      }
    ],
    "projects": [
      {
        "name": "CAIS Compute Cluster",
        "description": "Offers researchers free access to compute cluster for running and training large-scale AI systems to support ML safety research",
        "status": "Active"
      },
      {
        "name": "Philosophy Fellowship",
        "description": "Seven-month research program investigating societal implications and potential risks of advanced AI, tackling conceptual issues in AI safety",
        "status": "Active"
      },
      {
        "name": "AI Safety, Ethics, & Society Course",
        "description": "Comprehensive introduction to how current AI systems work, their societal-scale risks, and how to manage them",
        "status": "Active"
      },
      {
        "name": "Remote Labor Index: Measuring AI Automation of Remote Work",
        "description": "Capability benchmark measuring AI automation of remote work",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models",
        "description": "Machine ethics benchmark evaluating moral reasoning in language models beyond just outcomes",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Safety Pretraining: Toward the Next Generation of Safe AI",
        "description": "Robustness research on safety pretraining methods for AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark",
        "description": "Biosecurity benchmark testing AI capabilities in virology",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems",
        "description": "Machine ethics benchmark measuring honesty versus accuracy in AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
        "description": "Capability benchmark for long multimodal reasoning challenges",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs",
        "description": "Machine ethics research on analyzing and controlling value systems in AI",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Humanity's Last Exam",
        "description": "Capability benchmark testing AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
        "description": "Robustness benchmark measuring harmfulness of LLM agents",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "description": "Unlearning benchmark for measuring and reducing malicious use of AI",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
        "description": "Robustness benchmark for automated red teaming and robust refusal evaluation",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "MACHIAVELLI Benchmark",
        "description": "Machine ethics benchmark measuring trade-offs between rewards and ethical behavior",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Representation Engineering: A Top-Down Approach to AI Transparency",
        "description": "Research on improving AI transparency through representation engineering methods",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
        "description": "Research measuring trade-offs between rewards and ethical behavior using the MACHIAVELLI benchmark",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Scaling Out-of-Distribution Detection for Real-World Settings",
        "description": "Research on scaling out-of-distribution detection methods for practical applications",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Dreamlike Pictures Comprehensively Improve Safety Measures",
        "description": "Research on using dreamlike pictures to improve AI safety measures",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "AI Safety, Ethics, and Society",
        "description": "A textbook and online course providing a non-technical introduction to AI systems, risks, and mitigation strategies",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "SafeBench",
        "description": "Competition stimulating research on new benchmarks to assess and reduce AI risks, offering $250,000 in prizes",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Statement on AI Risk",
        "description": "Open letter signed by hundreds of AI experts and public figures expressing concern about AI risk",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Compute Cluster",
        "description": "Free compute cluster access for researchers to run and train large-scale AI systems for safety research",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "notes": "Takes a multidisciplinary approach working across academic disciplines, public and private entities. Conducts both technical research (creating foundational benchmarks and methods) and conceptual research incorporating insights from safety engineering, complex systems, international relations, and philosophy. Publishes in top ML conferences and releases datasets and code publicly.",
    "employees": 26,
    "directors": 3,
    "benchmarks": [
      {
        "name": "Remote Labor Index",
        "measures": "AI automation of remote work capabilities",
        "status": "Active"
      },
      {
        "name": "MoReBench",
        "measures": "Procedural and pluralistic moral reasoning in language models",
        "status": "Active"
      },
      {
        "name": "Virology Capabilities Test (VCT)",
        "measures": "Multimodal virology Q&A capabilities for biosecurity",
        "status": "Active"
      },
      {
        "name": "MASK Benchmark",
        "measures": "Honesty versus accuracy in AI systems",
        "status": "Active"
      },
      {
        "name": "EnigmaEval",
        "measures": "Long multimodal reasoning challenges",
        "status": "Active"
      },
      {
        "name": "Humanity's Last Exam",
        "measures": "General AI capabilities",
        "status": "Active"
      },
      {
        "name": "AgentHarm",
        "measures": "Harmfulness of LLM agents",
        "status": "Active"
      },
      {
        "name": "WMDP Benchmark",
        "measures": "Malicious use potential and unlearning effectiveness",
        "status": "Active"
      },
      {
        "name": "HarmBench",
        "measures": "Automated red teaming and robust refusal capabilities",
        "status": "Active"
      },
      {
        "name": "MACHIAVELLI Benchmark",
        "measures": "Trade-offs between rewards and ethical behavior",
        "status": "Active"
      },
      {
        "name": "MACHIAVELLI",
        "measures": "Trade-offs between rewards and ethical behavior in AI systems",
        "status": "Active"
      }
    ]
  },
  {
    "name": "CSET Georgetown",
    "url": "https://cset.georgetown.edu",
    "type": "Think Tank",
    "country": "United States",
    "mission": "CSET produces data-driven research at the intersection of security and technology, providing nonpartisan analysis to the policy community on AI, advanced computing and biotechnology.",
    "focus_areas": [
      "Policy",
      "Governance",
      "Biosecurity",
      "Cyber"
    ],
    "key_people": [
      {
        "name": "Helen Toner",
        "role": "Executive Director"
      },
      {
        "name": "Mina Narayanan",
        "role": "Researcher"
      },
      {
        "name": "Jessica Ji",
        "role": "Researcher"
      },
      {
        "name": "Vikram Venkatram",
        "role": "Researcher"
      },
      {
        "name": "Ngor Luong",
        "role": "Researcher"
      },
      {
        "name": "Mia Hoffmann",
        "role": "Researcher"
      }
    ],
    "notes": "Georgetown-based research organization focusing on security implications of emerging technologies, with emphasis on AI foundations like talent, data and computational power",
    "projects": [
      {
        "name": "AI Governance at the Frontier",
        "description": "Analytic approach to help U.S. policymakers deconstruct artificial intelligence governance proposals by identifying their underlying assumptions",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "The Mechanisms of AI Harm: Lessons Learned from AI Incidents",
        "description": "Analysis of AI incidents to improve understanding of how risks from AI materialize in practice, identifying six mechanisms of harm",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "PATHWISE",
        "description": "Prototype Analytics for Tracking High-Demand Workforce in Innovative Skill Ecosystems - provides way to explore how the United States develops and deploys talent in emerging technology fields",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Control: How to Make Use of Misbehaving AI Agents",
        "description": "Research on AI control and managing misbehaving AI agents",
        "status": "published",
        "paper_url": ""
      }
    ]
  },
  {
    "name": "GovAI Oxford",
    "url": "https://www.governance.ai",
    "type": "Think Tank",
    "country": "United Kingdom",
    "mission": "GovAI conducts research on AI governance and policy to inform government decision-making on AI regulation and oversight.",
    "focus_areas": [
      "Governance",
      "Policy",
      "Evals"
    ],
    "projects": [
      {
        "name": "Trends in Frontier AI Model Count: A Forecast to 2028",
        "description": "Analysis of government requirements on AI models based on training compute",
        "status": "Active"
      },
      {
        "name": "What Does the Public Think About AI?",
        "description": "Survey research synthesizing public attitudes towards AI in the UK and US, focusing on job loss concerns",
        "status": "Completed"
      },
      {
        "name": "Infrastructure for AI Agents",
        "description": "Research on AI systems that can plan and execute interactions in open-ended environments",
        "status": "Active"
      },
      {
        "name": "Predicting AI's Impact on Work",
        "description": "Research on automation evaluations to help policymakers foresee AI's impact on labor markets",
        "status": "Active"
      },
      {
        "name": "What Role Should Governments Play in Providing AI Agent Infrastructure?",
        "description": "Analysis of government roles in AI agent systems and protocols",
        "status": "Active"
      },
      {
        "name": "STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports",
        "description": "Framework for transparently reporting evaluations of dangerous AI capabilities in model reports",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Survey on Thresholds for Advanced AI Systems",
        "description": "Research on government thresholds for managing risks from advanced AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Incident Analysis for AI Agents",
        "description": "Framework for analyzing incidents involving AI agent deployment",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Forecasting LLM-enabled biorisk and the efficacy of safeguards",
        "description": "Analysis of biosecurity risks from large language models and effectiveness of safety measures",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Third-Party Compliance Reviews for Frontier AI Safety Frameworks",
        "description": "Research on external oversight mechanisms for AI safety frameworks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Safety Case Template for Frontier AI: A Cyber Inability Argument",
        "description": "Template for creating safety cases demonstrating AI systems lack dangerous cyber capabilities",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Safety Cases for Frontier AI",
        "description": "Framework for developers to explain why their AI systems are sufficiently safe",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "A Grading Rubric for AI Safety Frameworks",
        "description": "Evaluation criteria for assessing AI company safety frameworks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Risk Thresholds for Frontier AI",
        "description": "Framework for determining acceptable risk levels from frontier AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Model Evaluation for Extreme Risks",
        "description": "Approaches for evaluating AI systems for extreme risks and dangerous capabilities",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Towards Best Practices in AGI Safety and Governance",
        "description": "Survey of expert opinion on AGI safety and governance best practices",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Visibility into AI Agents",
        "description": "Framework for monitoring and oversight of AI agent activities",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "IDs for AI Systems",
        "description": "System for identifying and tracking AI systems for safety and governance purposes",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Organization publishes annual reports and focuses on technical AI governance research to inform policy decisions",
    "employees": 26,
    "directors": 4,
    "managers": 7,
    "key_people": [
      {
        "name": "Markus Anderljung",
        "role": "Researcher - AI regulation and safety"
      },
      {
        "name": "Jonas Schuett",
        "role": "Researcher - AI regulation and risk management"
      },
      {
        "name": "Alan Chan",
        "role": "Researcher - Technical AI governance and AI agents"
      },
      {
        "name": "Anton Korinek",
        "role": "Researcher - Economics of AI"
      },
      {
        "name": "Lennart Heim",
        "role": "Researcher - AI regulation and compute governance"
      },
      {
        "name": "Robert Trager",
        "role": "Researcher - International relations and AI governance"
      },
      {
        "name": "Ben Garfinkel",
        "role": "Researcher - AI policy and security"
      },
      {
        "name": "Allan Dafoe",
        "role": "Researcher - AI governance strategy"
      }
    ]
  },
  {
    "name": "CHAI Berkeley",
    "url": "https://humancompatible.ai",
    "type": "Academic",
    "country": "United States",
    "mission": "CHAI's mission is to develop the conceptual and technical wherewithal to reorient the general thrust of AI research towards provably beneficial systems.",
    "focus_areas": [
      "Alignment",
      "Evals",
      "Policy"
    ],
    "key_people": [
      {
        "name": "Scott Emmons",
        "role": "PhD student"
      },
      {
        "name": "Brian Christian",
        "role": "CHAI Affiliate"
      },
      {
        "name": "Alison Gopnik",
        "role": "CHAI Affiliate"
      },
      {
        "name": "Khanh Nguyen",
        "role": "Researcher"
      },
      {
        "name": "Benjamin Plaut",
        "role": "Researcher"
      },
      {
        "name": "Tu Trinh",
        "role": "Researcher"
      },
      {
        "name": "Mohamad Danesh",
        "role": "Researcher"
      },
      {
        "name": "Stuart Russell",
        "role": "Director"
      },
      {
        "name": "Anca Dragan",
        "role": "Faculty"
      },
      {
        "name": "Pieter Abbeel",
        "role": "Faculty"
      },
      {
        "name": "Joseph Halpern",
        "role": "Faculty"
      },
      {
        "name": "Thomas Griffiths",
        "role": "Faculty"
      },
      {
        "name": "Dylan Hadfield-Menell",
        "role": "Researcher"
      },
      {
        "name": "Dan Hendrycks",
        "role": "Researcher"
      },
      {
        "name": "Dorsa Sadigh",
        "role": "Faculty"
      },
      {
        "name": "Claire Tomlin",
        "role": "Faculty"
      },
      {
        "name": "Satinder Singh",
        "role": "Faculty"
      }
    ],
    "projects": [
      {
        "name": "RvS: What is Essential for Offline RL via Supervised Learning?",
        "description": "Research on offline reinforcement learning via supervised learning",
        "status": "Unknown"
      },
      {
        "name": "Political Neutrality for AI",
        "description": "Building political neutrality evaluations for AI systems",
        "status": "Active"
      },
      {
        "name": "Learning to Coordinate with Experts",
        "description": "Research on Learning to Yield and Request Control (YRC) coordination problem",
        "status": "Unknown"
      },
      {
        "name": "MAGICAL Benchmark for Robust Imitation",
        "description": "Benchmark for testing robustness of imitation learning algorithms",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "DERAIL: Diagnostic Environments for Reward And Imitation Learning",
        "description": "Diagnostic environments for testing reward and imitation learning systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "MineRL BASALT Competition on Learning from Human Feedback",
        "description": "Competition focused on learning from human feedback in Minecraft environment",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Cooperative Inverse Reinforcement Learning",
        "description": "Framework for learning human values through cooperative interaction",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "B-Pref: Benchmarking Preference-Based Reinforcement Learning",
        "description": "Benchmark suite for preference-based reinforcement learning algorithms",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "APReL: A Library for Active Preference-based Reward Learning Algorithms",
        "description": "Software library implementing active preference-based reward learning methods",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "imitation: Clean Imitation Learning Implementations",
        "description": "Clean implementations of imitation learning algorithms",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "TASRA: A Taxonomy of Societal-Scale Risks from AI",
        "description": "Comprehensive taxonomy of large-scale AI risks to society",
        "status": "published",
        "paper_url": "https://arxiv.org/abs/2310.17688"
      }
    ],
    "benchmarks": [
      {
        "name": "Learning to Yield and Request Control (YRC)",
        "measures": "When AI should act autonomously vs. seek expert assistance across diverse domains"
      },
      {
        "name": "MAGICAL Benchmark",
        "measures": "Robustness of imitation learning algorithms across distribution shifts",
        "status": "Active"
      },
      {
        "name": "B-Pref Benchmark",
        "measures": "Performance of preference-based reinforcement learning algorithms",
        "status": "Active"
      },
      {
        "name": "DERAIL Diagnostic Environments",
        "measures": "Failure modes and robustness of reward and imitation learning systems",
        "status": "Active"
      },
      {
        "name": "Machiavelli Benchmark",
        "measures": "Trade-offs between rewards and ethical behavior in text-based games",
        "status": "Active"
      }
    ],
    "notes": "Based at UC Berkeley. Brian Christian published work connecting AI alignment to human care relationships in Daedalus journal.",
    "employees": 26,
    "directors": 2
  },
  {
    "name": "Center on Long-Term Risk",
    "url": "https://longtermrisk.org",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Address worst-case risks from the development and deployment of advanced AI systems, with a focus on conflict scenarios and reducing risks of astronomical suffering (s-risk).",
    "focus_areas": [
      "Alignment",
      "Governance",
      "Control",
      "Monitoring"
    ],
    "key_people": [
      {
        "name": "Mia Taylor",
        "role": "Researcher"
      },
      {
        "name": "Jesse Clifton",
        "role": "Researcher"
      }
    ],
    "projects": [
      {
        "name": "Measurement Research Agenda",
        "description": "Identify properties of AI systems that make them more likely to contribute to s-risk and design measurement methods to detect these properties",
        "status": "Active"
      },
      {
        "name": "Cooperation, Conflict, and Transformative Artificial Intelligence Research Agenda",
        "description": "Developing technical and governance interventions to avoid conflict between transformative AI systems using insights from international relations, game theory, and machine learning",
        "status": "Active"
      },
      {
        "name": "Reducing long-term risks from malevolent actors",
        "description": "Research on interventions to reduce the expected influence of malevolent humans on the long-term future, including manipulation-proof measures of malevolence",
        "status": "Active"
      },
      {
        "name": "Risks of Astronomical Future Suffering",
        "description": "Research on how space colonization would likely increase rather than decrease total suffering, focusing on reducing risks of astronomical future suffering",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Suffering-Focused AI Safety: In Favor of \"Fail-Safe\" Measures",
        "description": "AI safety approach that tries to avert the worst outcomes containing large amounts of suffering, focusing on fail-safe measures rather than best-case outcomes",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "How the Simulation Argument Dampens Future Fanaticism",
        "description": "Research on how the simulation argument affects prioritization between short-term helping versus focusing on the far future",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Superintelligence as a Cause or Cure for Risks of Astronomical Suffering",
        "description": "Analysis of how superintelligent AI can both cause and reduce suffering risks (s-risks) comparable to existential risks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Multiverse-wide Cooperation via Correlated Decision Making",
        "description": "Decision theory research on superrationality and cooperation in prisoner's dilemma-type games with implications for multiverse scenarios",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Approval-directed agency and the decision theory of Newcomb-like problems",
        "description": "Research examining which decision theory is implemented by approval-directed agents in AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Robust program equilibrium",
        "description": "Proposing new approaches to achieve more robust cooperative program equilibria in one-shot prisoner's dilemma scenarios",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Do Artificial Reinforcement-Learning Agents Matter Morally?",
        "description": "Research on whether artificial reinforcement learning agents could qualify as sentient and deserve moral consideration",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Formalizing Preference Utilitarianism in Physical World Models",
        "description": "Using Bayesian inference to formalize preference utilitarianism in physical world models as a basis for ethical inquiry",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Organization focuses specifically on s-risk (astronomical suffering risks) and multi-agent conflict scenarios. They conduct interdisciplinary research, make grants, and build community around these priorities. Also associated with Polaris Research Institute.",
    "employees": 10,
    "directors": 3,
    "managers": 1
  },
  {
    "name": "Google DeepMind Safety",
    "url": "https://deepmind.google/about/responsibility-safety/",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "Google DeepMind works to build AI responsibly to benefit humanity, anticipating and evaluating systems against AI-related risks through responsible governance, research and impact.",
    "focus_areas": [
      "Alignment",
      "Governance",
      "Policy",
      "Evals",
      "Benchmarks"
    ],
    "key_people": [
      {
        "name": "Lila Ibrahim",
        "role": "COO, co-chair of Responsibility and Safety Council"
      },
      {
        "name": "Helen King",
        "role": "VP Responsibility, co-chair of Responsibility and Safety Council"
      },
      {
        "name": "Shane Legg",
        "role": "Co-Founder and Chief AGI Scientist, leads AGI Safety Council"
      },
      {
        "name": "Shlomi Fruchter",
        "role": "Researcher working on Genie 3"
      },
      {
        "name": "Jack Parker-Holder",
        "role": "Researcher working on Genie 3"
      },
      {
        "name": "Carolina Parada",
        "role": "Robotics researcher"
      },
      {
        "name": "Anca Dragan",
        "role": "AI Safety researcher"
      },
      {
        "name": "Irina Jurenka",
        "role": "AI in education researcher"
      }
    ],
    "projects": [
      {
        "name": "Frontier Safety Framework",
        "description": "Set of protocols to help stay ahead of possible severe risks from powerful frontier AI models",
        "status": "Active"
      },
      {
        "name": "AlphaFold Server",
        "description": "Platform to broaden access to AlphaFold 3 breakthrough model with educational materials",
        "status": "Active"
      },
      {
        "name": "Experience AI",
        "description": "Educational program partnering with Raspberry Pi Foundation to teach AI to 11-14 year olds",
        "status": "Active"
      },
      {
        "name": "CodeMender",
        "description": "AI agent for code security",
        "status": "Active"
      },
      {
        "name": "AI image verification for Gemini app",
        "description": "Bringing AI image verification capabilities to the Gemini application",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "SIMA 2",
        "description": "An agent that plays, reasons, and learns with you in virtual 3D worlds",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Genie 3",
        "description": "A general purpose world model that can generate an unprecedented diversity of interactive environments",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "Factuality benchmark for large language models",
        "measures": "Evaluates the factuality of large language models",
        "paper_url": ""
      }
    ],
    "notes": "Has dedicated Responsibility and Safety Council (RSC) and AGI Safety Council. Co-founded Frontier Model Forum and Partnership on AI. Focus on privacy-preserving AI and preventing misuse. Active in AI education globally with $10M funding reaching 2M+ young people.",
    "employees": 40,
    "directors": 3
  },
  {
    "name": "Japan AI Safety Institute",
    "url": "https://www.meti.go.jp/english/policy/mono_info_service/information_economy/artificial_intelligence.html",
    "type": "Government AISI",
    "country": "Japan",
    "mission": "Japan's AI Safety Institute (AISI) evaluates the safety of advanced AI systems and promotes international cooperation on AI safety standards.",
    "focus_areas": [
      "Evals",
      "Governance",
      "Policy",
      "Benchmarks"
    ],
    "projects": [
      {
        "name": "Frontier Model Evaluations",
        "description": "Safety evaluations of frontier AI models in cooperation with international partners",
        "status": "Active"
      },
      {
        "name": "AI Safety Guidelines",
        "description": "Development of safety guidelines for generative AI",
        "status": "Active"
      }
    ],
    "notes": "Launched February 2024 under METI. Part of the international network of AI Safety Institutes. Participates in joint evaluation protocols with US AISI and UK AISI."
  },
  {
    "name": "MATS",
    "url": "https://www.matsprogram.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "MATS is an independent research and educational program that connects talented scholars with top mentors in AI alignment, governance, and security to train the next generation of AI safety researchers.",
    "focus_areas": [
      "Alignment",
      "Governance",
      "Evals"
    ],
    "key_people": [
      {
        "name": "Neel Nanda",
        "role": "Mentor"
      },
      {
        "name": "Marius Hobbhahn",
        "role": "Alumnus, Apollo Research CEO"
      },
      {
        "name": "Jesse Hoogland",
        "role": "Alumnus, Executive Director of Timaeus"
      },
      {
        "name": "Quentin Feuillade-Montixi",
        "role": "Alumnus, Co-founder and CTO of PRISM Evals"
      },
      {
        "name": "Hoagy Cunningham",
        "role": "Author"
      },
      {
        "name": "Shashwat Goel",
        "role": "Author"
      },
      {
        "name": "Annah Dombrowski",
        "role": "Author"
      },
      {
        "name": "Stefan Heimersheim",
        "role": "Author"
      },
      {
        "name": "Arthur Conmy",
        "role": "Author"
      },
      {
        "name": "Aengus Lynch",
        "role": "Author"
      },
      {
        "name": "Lukas Berglund",
        "role": "Author"
      },
      {
        "name": "Meg Tong",
        "role": "Author"
      },
      {
        "name": "Max Kaufmann",
        "role": "Author"
      },
      {
        "name": "Asa Cooper Stickland",
        "role": "Author"
      },
      {
        "name": "Nick Gabrieli",
        "role": "Author"
      },
      {
        "name": "Nina Panickssery",
        "role": "Author"
      },
      {
        "name": "Julian Schulz",
        "role": "Author"
      },
      {
        "name": "Aaquib Syed",
        "role": "Author"
      },
      {
        "name": "Andy Arditi",
        "role": "Author"
      },
      {
        "name": "Wes Gurnee",
        "role": "Author"
      },
      {
        "name": "Arjun Panickssery",
        "role": "Author"
      },
      {
        "name": "Oam Patel",
        "role": "Author"
      },
      {
        "name": "Samuel Marks",
        "role": "Author"
      },
      {
        "name": "Lisa Thiergart",
        "role": "Author"
      },
      {
        "name": "David Udell",
        "role": "Author"
      },
      {
        "name": "Ulisse Mini",
        "role": "Author"
      },
      {
        "name": "Jonathan Ng",
        "role": "Author"
      },
      {
        "name": "Hanlin Zhang",
        "role": "Author"
      },
      {
        "name": "Bilal Chughtai",
        "role": "Author"
      },
      {
        "name": "Simon Lermen",
        "role": "Author"
      },
      {
        "name": "Oskar John Hollinsworth",
        "role": "Author"
      },
      {
        "name": "Curt Tigges",
        "role": "Author"
      },
      {
        "name": "Aidan Ewart",
        "role": "Author"
      },
      {
        "name": "Phillip Guo",
        "role": "Author"
      },
      {
        "name": "Cindy Wu",
        "role": "Author"
      },
      {
        "name": "Vivek Hebbar",
        "role": "Author"
      },
      {
        "name": "Lorenzo Pacchiardi",
        "role": "Author"
      },
      {
        "name": "Alex Chan",
        "role": "Author"
      },
      {
        "name": "Ilan Moscovitz",
        "role": "Author"
      },
      {
        "name": "Jiaxin Wen",
        "role": "Author"
      },
      {
        "name": "Callum McDougall",
        "role": "Author"
      },
      {
        "name": "Cody Rushing",
        "role": "Author"
      },
      {
        "name": "Jordan Taylor",
        "role": "Author"
      },
      {
        "name": "Jacob Dunefsky",
        "role": "Author"
      },
      {
        "name": "Philippe Chlenski",
        "role": "Author"
      },
      {
        "name": "Javier Ferrando Monsonis",
        "role": "Author"
      },
      {
        "name": "Oscar Balcells Obeso",
        "role": "Author"
      },
      {
        "name": "Daniel Tan",
        "role": "Author"
      },
      {
        "name": "Mart\u00edn Soto Quintanilla",
        "role": "Author"
      },
      {
        "name": "Felix Hofst\u00e4tter",
        "role": "Author"
      },
      {
        "name": "Teun van der Weij",
        "role": "Author"
      },
      {
        "name": "Joseph Miller",
        "role": "Author"
      },
      {
        "name": "David Karamardian",
        "role": "Author"
      },
      {
        "name": "Iv\u00e1n Arcuschin Moreno",
        "role": "Author"
      },
      {
        "name": "Kajetan  Janiak",
        "role": "Author"
      },
      {
        "name": "Adam Karvonen",
        "role": "Author"
      },
      {
        "name": "Can Rager",
        "role": "Author"
      },
      {
        "name": "Benjamin Wright",
        "role": "Author"
      },
      {
        "name": "Matthew Siu",
        "role": "Author"
      },
      {
        "name": "Sviatoslav Chalnev",
        "role": "Author"
      },
      {
        "name": "Aghyad Deeb",
        "role": "Author"
      },
      {
        "name": "Pranav Gade",
        "role": "Author"
      },
      {
        "name": "Eoin Farrell",
        "role": "Author"
      },
      {
        "name": "Yeu-Tong Lau",
        "role": "Author"
      },
      {
        "name": "Georg Lange",
        "role": "Author"
      },
      {
        "name": "Aleksandar Makelov",
        "role": "Author"
      },
      {
        "name": "Patrick Leask",
        "role": "Author"
      },
      {
        "name": "Bart Bussmann",
        "role": "Author"
      },
      {
        "name": "David Chanin",
        "role": "Author"
      },
      {
        "name": "Connor Kissane",
        "role": "Author"
      },
      {
        "name": "Joseph Isaac Bloom",
        "role": "Author"
      },
      {
        "name": "Robert Krzyzanowski",
        "role": "Author"
      },
      {
        "name": "Jenny Bao",
        "role": "Author"
      },
      {
        "name": "Joshua Engels",
        "role": "Author"
      },
      {
        "name": "Atticus Wang",
        "role": "Author"
      },
      {
        "name": "Michael Pearce",
        "role": "Author"
      },
      {
        "name": "Marcus Williams",
        "role": "Author"
      },
      {
        "name": "Constantin Weisser",
        "role": "Author"
      },
      {
        "name": "Peli Grietzer",
        "role": "Author"
      },
      {
        "name": "Jessica Cooper",
        "role": "Author"
      },
      {
        "name": "Matthew Watkins",
        "role": "Author"
      },
      {
        "name": "Sumeet Motwani",
        "role": "Author"
      },
      {
        "name": "Anish Mudide",
        "role": "Author"
      },
      {
        "name": "Alexander Meinke",
        "role": "Author"
      },
      {
        "name": "Rudolf Laine",
        "role": "Author"
      },
      {
        "name": "Sara Price",
        "role": "Author"
      },
      {
        "name": "Caleb Larson",
        "role": "Author"
      },
      {
        "name": "Florian Dietz",
        "role": "Author"
      },
      {
        "name": "Kei Nishimura-Gasparian",
        "role": "Author"
      },
      {
        "name": "Jeanne Salle",
        "role": "Author"
      },
      {
        "name": "Satvik Golechha",
        "role": "Author"
      },
      {
        "name": "Jacek Karwowski",
        "role": "Author"
      },
      {
        "name": "Zora Che",
        "role": "Author"
      },
      {
        "name": "Dan Valentine",
        "role": "Author"
      },
      {
        "name": "James Chua",
        "role": "Author"
      },
      {
        "name": "John Hughes",
        "role": "Author"
      },
      {
        "name": "Rajashree Agrawal",
        "role": "Author"
      },
      {
        "name": "Artur Zolkowski",
        "role": "Author"
      },
      {
        "name": "Robert McCarthy",
        "role": "Author"
      },
      {
        "name": "Elizabeth Donoway",
        "role": "Author"
      },
      {
        "name": "Yashvardhan Sharma",
        "role": "Author"
      },
      {
        "name": "Jakub Kry\u015b",
        "role": "Author"
      },
      {
        "name": "Jack Foxabbott",
        "role": "Author"
      },
      {
        "name": "Ev\u017een Wybitul",
        "role": "Author"
      },
      {
        "name": "Evan Ryan Gunter",
        "role": "Author"
      },
      {
        "name": "Rohan Gupta",
        "role": "Author"
      },
      {
        "name": "Constantin Venhoff",
        "role": "Author"
      },
      {
        "name": "Jake Ward",
        "role": "Author"
      },
      {
        "name": "Helena Casademunt",
        "role": "Author"
      },
      {
        "name": "Caden Juang",
        "role": "Author"
      },
      {
        "name": "Claire Short",
        "role": "Author"
      },
      {
        "name": "Bartosz Cywi\u0144ski",
        "role": "Author"
      },
      {
        "name": "Emil Ryd",
        "role": "Author"
      },
      {
        "name": "Su Hyeong Lee",
        "role": "Author"
      },
      {
        "name": "Wen Xing",
        "role": "Author"
      },
      {
        "name": "Roy Rinberg",
        "role": "Author"
      },
      {
        "name": "Daniel Reuter",
        "role": "Author"
      },
      {
        "name": "Tim Hua",
        "role": "Author"
      },
      {
        "name": "Andrew Qin",
        "role": "Author"
      },
      {
        "name": "Luke Marks",
        "role": "Author"
      },
      {
        "name": "Winnie X",
        "role": "Author"
      },
      {
        "name": "Joschka Braun",
        "role": "Author"
      },
      {
        "name": "Damon Falck",
        "role": "Author"
      },
      {
        "name": "Yeonwoo Jang",
        "role": "Author"
      }
    ],
    "projects": [
      {
        "name": "MATS Summer 2026",
        "description": "12-week research program in Berkeley, CA running June 1 - August 21",
        "status": "Active"
      },
      {
        "name": "Extension Program",
        "description": "6-12 additional months of research continuation in London, UK",
        "status": "Active"
      },
      {
        "name": "Research Symposium",
        "description": "Program culmination with poster presentations and spotlight talks",
        "status": "Active"
      },
      {
        "name": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Hoagy Cunningham",
        "paper_url": "https://arxiv.org/abs/2309.08600"
      },
      {
        "name": "Representation Engineering: A Top-Down Approach to AI Transparency",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Shashwat Goel, Annah Dombrowski",
        "paper_url": "https://arxiv.org/abs/2310.01405"
      },
      {
        "name": "Towards Automated Circuit Discovery for Mechanistic Interpretability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Stefan Heimersheim, Arthur Conmy, Aengus Lynch",
        "paper_url": "https://arxiv.org/abs/2304.14997"
      },
      {
        "name": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Lukas Berglund, Meg Tong, Max Kaufmann, Asa Cooper Stickland",
        "paper_url": "https://arxiv.org/abs/2309.12288"
      },
      {
        "name": "Towards Understanding Sycophancy in Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Meg Tong",
        "paper_url": "https://arxiv.org/abs/2310.13548"
      },
      {
        "name": "Steering Llama 2 via Contrastive Activation Addition",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Nick Gabrieli, Nina Panickssery (n\u00e9e Rimsky), Julian Schulz, Meg Tong",
        "paper_url": "https://arxiv.org/abs/2312.06681"
      },
      {
        "name": "Refusal in Language Models Is Mediated by a Single Direction",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Aaquib Syed, Andy Arditi",
        "paper_url": "https://arxiv.org/abs/2406.11717"
      },
      {
        "name": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Wes Gurnee",
        "paper_url": "https://arxiv.org/abs/2305.01610"
      },
      {
        "name": "LLM Evaluators Recognize and Favor Their Own Generations",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Arjun Panickssery",
        "paper_url": "https://arxiv.org/abs/2404.13076"
      },
      {
        "name": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Oam Patel, Samuel Marks, Annah Dombrowski",
        "paper_url": "https://arxiv.org/abs/2403.03218"
      },
      {
        "name": "Steering Language Models With Activation Engineering",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Lisa Thiergart, David Udell, Ulisse Mini",
        "paper_url": "https://arxiv.org/abs/2308.10248"
      },
      {
        "name": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jonathan Ng, Hanlin Zhang",
        "paper_url": "https://arxiv.org/abs/2304.03279"
      },
      {
        "name": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Bilal Chughtai",
        "paper_url": "https://arxiv.org/abs/2302.03025"
      },
      {
        "name": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Simon Lermen",
        "paper_url": "https://arxiv.org/abs/2310.20624"
      },
      {
        "name": "Linear Representations of Sentiment in Large Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Oskar John Hollinsworth, Curt Tigges",
        "paper_url": "https://arxiv.org/abs/2310.15154"
      },
      {
        "name": "Eight Methods to Evaluate Robust Unlearning in LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Aidan Ewart, Aengus Lynch, Phillip Guo",
        "paper_url": "https://arxiv.org/abs/2402.16835"
      },
      {
        "name": "Taken out of context: On measuring situational awareness in LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Lukas Berglund, Asa Cooper Stickland, Max Kaufmann, Meg Tong",
        "paper_url": "https://arxiv.org/abs/2309.00667"
      },
      {
        "name": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Aidan Ewart, Aengus Lynch, Phillip Guo, Cindy Wu, Vivek Hebbar",
        "paper_url": "https://arxiv.org/abs/2407.15549"
      },
      {
        "name": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Lorenzo Pacchiardi, Alex Chan, Ilan Moscovitz",
        "paper_url": "https://arxiv.org/abs/2309.15840"
      },
      {
        "name": "Language Models Learn to Mislead Humans via RLHF",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jiaxin Wen",
        "paper_url": "https://arxiv.org/abs/2409.12822"
      },
      {
        "name": "Copy Suppression: Comprehensively Understanding an Attention Head",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Callum McDougall, Arthur Conmy, Cody Rushing",
        "paper_url": "https://arxiv.org/abs/2310.04625"
      },
      {
        "name": "Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jordan Taylor",
        "paper_url": "https://arxiv.org/abs/2405.12241"
      },
      {
        "name": "Transcoders Find Interpretable LLM Feature Circuits",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jacob Dunefsky, Philippe Chlenski",
        "paper_url": "https://arxiv.org/abs/2406.11944"
      },
      {
        "name": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Javier Ferrando Monsonis, Oscar Balcells Obeso",
        "paper_url": "https://arxiv.org/abs/2411.14257"
      },
      {
        "name": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Daniel Tan, Mart\u00edn Soto Quintanilla",
        "paper_url": "https://arxiv.org/abs/2502.17424"
      },
      {
        "name": "AI Sandbagging: Language Models can Strategically Underperform on Evaluations",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Felix Hofst\u00e4tter, Teun van der Weij",
        "paper_url": "https://arxiv.org/abs/2406.07358"
      },
      {
        "name": "Open Problems in Mechanistic Interpretability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Joseph Miller",
        "paper_url": "https://arxiv.org/abs/2501.16496"
      },
      {
        "name": "Can LLMs Follow Simple Rules?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: David Karamardian",
        "paper_url": "https://arxiv.org/abs/2311.04235"
      },
      {
        "name": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Iv\u00e1n Arcuschin Moreno, Kajetan (Jett) Janiak",
        "paper_url": "https://arxiv.org/abs/2503.08679"
      },
      {
        "name": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Adam Karvonen, Can Rager, Benjamin Wright, Samuel Marks",
        "paper_url": "https://arxiv.org/abs/2408.00113"
      },
      {
        "name": "Improving Steering Vectors by Targeting Sparse Autoencoder Features",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Matthew Siu, Sviatoslav Chalnev, Arthur Conmy",
        "paper_url": "https://arxiv.org/abs/2411.02193"
      },
      {
        "name": "Do Unlearning Methods Remove Information from Language Model Weights?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Aghyad Deeb",
        "paper_url": "https://arxiv.org/abs/2410.08827"
      },
      {
        "name": "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Simon Lermen, Pranav Gade",
        "paper_url": "https://arxiv.org/abs/2311.00117"
      },
      {
        "name": "Applying sparse autoencoders to unlearn knowledge in language models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Eoin Farrell, Yeu-Tong Lau, Arthur Conmy",
        "paper_url": "https://arxiv.org/abs/2410.19278"
      },
      {
        "name": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Georg Lange, Aleksandar Makelov",
        "paper_url": "https://arxiv.org/abs/2311.17030"
      },
      {
        "name": "BatchTopK Sparse Autoencoders",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Patrick Leask, Bart Bussmann",
        "paper_url": "https://arxiv.org/abs/2412.06410"
      },
      {
        "name": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Adam Karvonen, Can Rager, David Chanin",
        "paper_url": "https://arxiv.org/abs/2503.09532"
      },
      {
        "name": "Interpreting Attention Layer Outputs with Sparse Autoencoders",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Connor Kissane, Joseph Isaac Bloom, Robert Krzyzanowski",
        "paper_url": "https://arxiv.org/abs/2406.17759"
      },
      {
        "name": "Tell Me About Yourself: LLMs are Aware of their Learned Behaviors",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jenny Bao",
        "paper_url": "https://arxiv.org/pdf/2501.11120"
      },
      {
        "name": "Are Sparse Autoencoders Useful? A Case Study in Sparse Probing",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Joshua Engels",
        "paper_url": "https://arxiv.org/pdf/2502.16681"
      },
      {
        "name": "Simple Mechanistic Explanations for Out-Of-Context Reasoning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Atticus Wang, Joshua Engels",
        "paper_url": "https://arxiv.org/abs/2507.08218"
      },
      {
        "name": "Sparse Autoencoders Do Not Find Canonical Units of Analysis",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Patrick Leask, Bart Bussmann, Michael Pearce",
        "paper_url": "https://arxiv.org/abs/2502.04878"
      },
      {
        "name": "On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Marcus Williams, Constantin Weisser",
        "paper_url": "https://arxiv.org/abs/2411.02306"
      },
      {
        "name": "Understanding and Controlling a Maze-Solving Policy Network",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Ulisse Mini, Peli Grietzer",
        "paper_url": "https://arxiv.org/abs/2310.08043"
      },
      {
        "name": "SolidGoldMagikarp (plus, prompt generation)",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jessica Cooper (Rumbelow), Matthew Watkins",
        "paper_url": "https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"
      },
      {
        "name": "Secret Collusion Among Generative AI Agents",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Sumeet Motwani",
        "paper_url": "https://arxiv.org/abs/2402.07510"
      },
      {
        "name": "Efficient Dictionary Learning with Switch Sparse Autoencoders",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Anish Mudide",
        "paper_url": "https://arxiv.org/abs/2410.08201"
      },
      {
        "name": "Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Alexander Meinke, Rudolf Laine, Bilal Chughtai, Marius Hobbhahn",
        "paper_url": "https://arxiv.org/abs/2407.04694"
      },
      {
        "name": "Explorations of Self-Repair in Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Cody Rushing",
        "paper_url": "https://arxiv.org/abs/2402.15390"
      },
      {
        "name": "Best-of-N Jailbreaking",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Sara Price, Aengus Lynch",
        "paper_url": "https://arxiv.org/abs/2412.03556"
      },
      {
        "name": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jiaxin Wen, Vivek Hebbar, Caleb Larson",
        "paper_url": "https://arxiv.org/abs/2411.17693"
      },
      {
        "name": "Auditing language models for hidden objectives",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Florian Dietz, Kei Nishimura-Gasparian, Jeanne Salle, Satvik Golechha",
        "paper_url": "https://arxiv.org/abs/2503.10965"
      },
      {
        "name": "Goodhart's Law in Reinforcement Learning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jacek Karwowski",
        "paper_url": "https://arxiv.org/abs/2310.09144"
      },
      {
        "name": "Model tampering attacks enable more rigorous evaluations of LLM capabilities",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Zora Che",
        "paper_url": "https://arxiv.org/pdf/2502.05209"
      },
      {
        "name": "When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Dan Valentine, James Chua, John Hughes, Rajashree Agrawal",
        "paper_url": "https://arxiv.org/abs/2407.15211"
      },
      {
        "name": "Early Signs of Steganographic Capabilities in Frontier LLMs",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy",
        "paper_url": "https://arxiv.org/abs/2507.02737"
      },
      {
        "name": "Technical Report: Evaluating Goal Drift in Language Model Agents",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Elizabeth Donoway",
        "paper_url": "https://arxiv.org/abs/2505.02709"
      },
      {
        "name": "Distributed and Decentralised Training: Technical Governance Challenges in a Shifting AI Landscape",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Yashvardhan Sharma, Jakub Kry\u015b",
        "paper_url": "https://arxiv.org/abs/2507.07765v1"
      },
      {
        "name": "A Causal Model of Theory-of-Mind in AI Agents",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Jack Foxabbott",
        "paper_url": "https://openreview.net/forum?id=ASA2jdKtf3"
      },
      {
        "name": "ViSTa Dataset: Do vision-language models understand sequential tasks?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Ev\u017een Wybitul, Evan Ryan Gunter",
        "paper_url": "https://arxiv.org/abs/2411.13211"
      },
      {
        "name": "RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Rohan Gupta",
        "paper_url": "https://www.arxiv.org/abs/2506.14261"
      },
      {
        "name": "Reasoning-Finetuning Repurposes Latent Representations in Base Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Constantin Venhoff, Jake Ward",
        "paper_url": "https://arxiv.org/abs/2507.12638"
      },
      {
        "name": "Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks",
        "paper_url": "https://arxiv.org/abs/2507.16795"
      },
      {
        "name": "Public Perspectives on AI Governance: A Survey of Working Adults in California, Illinois, and New York",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Claire Short",
        "paper_url": "https://doi.org/10.5281/zenodo.16566058"
      },
      {
        "name": "Towards eliciting latent knowledge from LLMs with mechanistic interpretability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Bartosz Cywi\u0144ski, Emil Ryd",
        "paper_url": "https://arxiv.org/abs/2505.14352"
      },
      {
        "name": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Dan Valentine, John Hughes",
        "paper_url": "https://arxiv.org/abs/2402.06782"
      },
      {
        "name": "On Defining Neural Averaging",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Su Hyeong Lee",
        "paper_url": "https://arxiv.org/abs/2508.14832"
      },
      {
        "name": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Su Hyeong Lee",
        "paper_url": "https://arxiv.org/abs/2509.06701"
      },
      {
        "name": "Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Artur Zolkowski, Wen Xing",
        "paper_url": "https://arxiv.org/abs/2510.19851"
      },
      {
        "name": "Eliciting Secret Knowledge from Language Models",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Bartosz Cywi\u0144ski",
        "paper_url": "https://arxiv.org/abs/2510.01070"
      },
      {
        "name": "Verifying LLM Inference to Prevent Model Weight Exfiltration",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Roy Rinberg, Daniel Reuter, Adam Karvonen",
        "paper_url": "https://arxiv.org/abs/2511.02620"
      },
      {
        "name": "Steering Evaluation-Aware Language Models to Act Like They Are Deployed",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Tim Hua, Andrew Qin",
        "paper_url": "https://arxiv.org/abs/2510.20487"
      },
      {
        "name": "DiFR: Inference Verification Despite Nondeterminism",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Adam Karvonen, Daniel Reuter, Roy Rinberg, Luke Marks",
        "paper_url": "https://arxiv.org/abs/2511.20621"
      },
      {
        "name": "AI agents find $4.6M in blockchain smart contract exploits",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Winnie X",
        "paper_url": "https://red.anthropic.com/2025/smart-contracts/"
      },
      {
        "name": "Resisting RL Elicitation of Biosecurity Capabilities: Reasoning Models Exploration Hacking on WMDP",
        "status": "published",
        "description": "Research paper by MATS scholars. Authors: Joschka Braun, Damon Falck, Yeonwoo Jang",
        "paper_url": "https://openreview.net/pdf/2645934ae38765d0fd2446ed66cb06e5f406dcbd.pdf"
      }
    ],
    "notes": "357 scholars and 75 mentors supported since 2021. Produced 115 research publications with 5100+ citations (h-index 31). 80% of alumni work in AI alignment. ~10% of alumni founded AI safety organizations. Provides $14.4k stipend, $12k compute budget, housing, and travel. Notable spin-off organizations include Apollo Research, PRISM Eval, Timaeus, and many others.",
    "employees": 33,
    "directors": 2,
    "managers": 7,
    "subteams": 5
  },
  {
    "name": "Conjecture",
    "url": "https://www.conjecture.dev/research",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "Conjecture is an AI alignment research startup that focuses on building Cognitive Emulation - an AI architecture that bounds systems' capabilities and makes them reason in ways humans can understand and control.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Control"
    ],
    "key_people": [
      {
        "name": "Connor Leahy",
        "role": "Founder"
      },
      {
        "name": "Sid Black",
        "role": "Founder"
      },
      {
        "name": "Gabriel Alfour",
        "role": "Founder"
      },
      {
        "name": "Adam Shimi",
        "role": "Early staff/Researcher"
      }
    ],
    "projects": [
      {
        "name": "Cognitive Emulation (CoEm)",
        "description": "Primary research direction to build predictably boundable AI systems rather than directly aligned AGIs",
        "status": "Active"
      },
      {
        "name": "Cognitive Software",
        "description": "Approach to building AI systems that emulate human cognitive patterns",
        "status": "Active"
      },
      {
        "name": "unRLHF",
        "description": "Research on efficiently undoing LLM safeguards",
        "status": "Completed"
      },
      {
        "name": "MAGIC (Multinational AGI Consortium)",
        "description": "Proposal for international coordination on AI through a global institution permitted to develop advanced AI",
        "status": "Unknown"
      },
      {
        "name": "Cognitive Emulation",
        "description": "An AI architecture that bounds systems' capabilities and makes them reason in ways that humans can understand and control, aimed at building predictably boundable systems rather than directly aligned AGIs",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Multinational AGI Consortium (MAGIC)",
        "description": "A proposal for international coordination on AI to mitigate existential risks from advanced AI through a global moratorium on advanced AI development",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Refine",
        "description": "A model for diversifying conceptual alignment research approaches to increase access to the field and stimulate new ideas",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "London-based startup with VC backing from notable investors including Nat Friedman, Daniel Gross, Collison brothers, Andrej Karpathy, and Sam Bankman-Fried. Team includes EleutherAI alumni and independent researchers.",
    "employees": 13
  },
  {
    "name": "FAR AI",
    "url": "https://far.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "FAR.AI is a research & education non-profit ensuring advanced AI is safe and beneficial for everyone.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Evals",
      "Benchmarks"
    ],
    "key_people": [
      {
        "name": "Matthew Kowal",
        "role": "Researcher"
      },
      {
        "name": "Jasper Timm",
        "role": "Researcher"
      },
      {
        "name": "Niki Howe",
        "role": "Researcher"
      },
      {
        "name": "Micha\u0142 Zaj\u0105c",
        "role": "Researcher"
      }
    ],
    "projects": [
      {
        "name": "Frontier LLMs Attempt to Persuade into Harmful Topics",
        "description": "Research on how easily frontier models can be prompted to persuade people into harmful beliefs or illegal actions",
        "status": "Active"
      },
      {
        "name": "Does Robustness Improve with Scale?",
        "description": "Investigation of whether scaling up model size can solve robustness issues in frontier LLMs",
        "status": "Active"
      },
      {
        "name": "FAR.Labs",
        "description": "Collaborative co-working space in Berkeley for researchers developing AI risk solutions",
        "status": "Active"
      },
      {
        "name": "Layered AI Defenses Have Holes: Vulnerabilities and Key Recommendations",
        "description": "Testing effectiveness of defense-in-depth AI safety strategies using STACK attack method, achieving 71% success rate on catastrophic risk scenarios where conventional attacks achieved 0%",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Beyond the Board: Exploring AI Robustness Through Go",
        "description": "Testing three approaches to defend Go AIs from adversarial strategies, finding that defenses protect against known adversaries but new adversaries can undermine these defenses",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Adversarial Policies Beat Superhuman Go AIs",
        "description": "Attack on KataGo AI system where adversaries trick the AI into making blunders rather than winning through superior play, demonstrating failure modes in superhuman AI systems",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Exploiting Novel GPT-4 APIs",
        "description": "Red-teaming GPT-4 fine-tuning, function calling and knowledge retrieval APIs, finding that fine-tuning on as few as 15 harmful examples can remove core safeguards",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Inverse Scaling: When Bigger Isn't Better",
        "description": "Presenting 11 instances of inverse scaling where language models get worse with scale rather than better, selected from 99 submissions in the Inverse Scaling Prize competition",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Pacing Outside the Box: RNNs Learn to Plan in Sokoban",
        "description": "Study of RNN trained to play Sokoban that learned to spend time planning ahead by pacing despite penalties, demonstrating strategic planning in neural networks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
        "description": "Method to modify neural networks to make their internals more interpretable and steerable using quantization bottleneck that forces activation vectors into discrete codes",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Exploring Scaling Trends in LLM Robustness",
        "description": "Research finding that robustness against adversarial attacks improves with adversarial training but not with model scaling alone",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Organization hosts alignment workshops globally, runs grantmaking programs for academics and independent researchers, and collaborates with organizations like UC Berkeley, University of Montreal, Mozilla, and government agencies.",
    "employees": 29,
    "directors": 2,
    "managers": 4,
    "subteams": 4,
    "benchmarks": [
      {
        "name": "Attempt to Persuade Eval (APE)",
        "measures": "Tests how willing LLMs are to generate content aimed at shaping beliefs and behavior on harmful topics",
        "status": "Active"
      },
      {
        "name": "InterpBench",
        "measures": "Collection of 17 semi-synthetic transformers with known circuits for evaluating mechanistic interpretability techniques",
        "status": "Active"
      },
      {
        "name": "STACK attack method",
        "measures": "Bypasses AI defense layers sequentially to test effectiveness of multi-layered AI safety strategies",
        "status": "Active"
      }
    ]
  },
  {
    "name": "Epoch AI",
    "url": "https://epoch.ai/research",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Epoch AI is a multidisciplinary non-profit research institute investigating the future of artificial intelligence and forecasting its economic and societal impact.",
    "focus_areas": [
      "Evals",
      "Benchmarks"
    ],
    "projects": [
      {
        "name": "FrontierMath",
        "description": "A benchmark of several hundred unpublished, expert-level mathematics problems that take specialists hours to days to solve",
        "status": "Active"
      },
      {
        "name": "GATE Playground",
        "description": "Not specified in content",
        "status": "Active"
      },
      {
        "name": "Distributed Training",
        "description": "Not specified in content",
        "status": "Active"
      },
      {
        "name": "Model Counts",
        "description": "Not specified in content",
        "status": "Active"
      },
      {
        "name": "Can AI Scaling Continue Through 2030?",
        "description": "Investigation of four constraints to scaling AI training: power, chip manufacturing, data, and latency. Predicts 2e29 FLOP runs will be feasible by 2030",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "A Rosetta Stone for AI benchmarks",
        "description": "Statistical framework that stitches benchmarks together for studying long-run AI trends",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "GATE: Modeling the trajectory of AI and automation",
        "description": "Compute-centric model of AI automation and its economic effects",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Direct Approach interactive model",
        "description": "User-adjustable model forecasting when transformative AI could be deployed",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "FrontierMath",
        "measures": "Expert-level mathematics problems that take specialists hours to days to solve"
      },
      {
        "name": "OSWorld",
        "measures": "AI's ability to use computers and interpret GUI-based tasks",
        "status": "Active"
      },
      {
        "name": "SWE-bench Verified",
        "measures": "Agentic coding capabilities, focusing on bug fixes in open-source repositories",
        "status": "Active"
      }
    ],
    "notes": "Maintains the largest public database of notable ML models, conducts research on AI scaling through 2030, and provides data insights on AI trends including training compute and hardware advancements.",
    "employees": 22,
    "directors": 2,
    "key_people": [
      {
        "name": "Anson Ho",
        "role": "Researcher"
      },
      {
        "name": "Jean-Stanislas Denain",
        "role": "Researcher"
      },
      {
        "name": "David Atanasov",
        "role": "Researcher"
      },
      {
        "name": "Samuel Albanie",
        "role": "Researcher"
      },
      {
        "name": "Rohin Shah",
        "role": "Researcher"
      },
      {
        "name": "Ben Cottier",
        "role": "Researcher"
      },
      {
        "name": "Jaime Sevilla",
        "role": "Researcher"
      },
      {
        "name": "Tamay Besiroglu",
        "role": "Researcher"
      },
      {
        "name": "David Owen",
        "role": "Researcher"
      },
      {
        "name": "Ege Erdil",
        "role": "Researcher"
      },
      {
        "name": "Pablo Villalobos",
        "role": "Researcher"
      },
      {
        "name": "Greg Burnham",
        "role": "Researcher"
      },
      {
        "name": "Florian Brand",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Apart Research",
    "url": "https://apartresearch.com/",
    "type": "Nonprofit",
    "country": "Denmark",
    "mission": "Apart Research accelerates AI safety research through mentorship, collaborations, and research sprints to make advanced AI safe and beneficial for humanity.",
    "focus_areas": [
      "Alignment",
      "Control"
    ],
    "notes": "Organization focuses on building global research communities, organizing hackathons, and providing career development support for AI safety researchers. Multiple testimonials highlight their role in career transitions and community building.",
    "employees": 9,
    "projects": [
      {
        "name": "Rethinking CyberSecEval: An LLM-Aided Approach to Evaluation Critique",
        "description": "LLM-aided approach to evaluation critique for cybersecurity evaluations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Catastrophic Cyber Capabilities Benchmark (3CB)",
        "description": "Robustly evaluating LLM agent cyber offense capabilities, showing realistic challenges for cyber offense can be completed by SoTA LLMs",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Benchmark Inflation: Revealing LLM Performance Gaps Using Retro-Holdouts",
        "description": "Research on revealing LLM performance gaps using retro-holdouts in benchmarks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Interpreting Learned Feedback Patterns in Large Language Models",
        "description": "Mechanistic interpretability research on learned feedback patterns in LLMs",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
        "description": "Research on training deceptive LLMs that persist through safety training",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
        "description": "Model editing techniques can introduce unwanted side effects in neural networks not detected by existing benchmarks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Understanding Addition in Transformers",
        "description": "Mechanistic interpretability research on understanding addition in transformers",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
        "description": "Benchmark for evaluating dark patterns in large language models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "CryptoFormalEval: Integrating LLMs and Formal Verification",
        "description": "Integrating LLMs and formal verification for automated cryptographic protocol vulnerability detection",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Multi-Agent Security Tax",
        "description": "Trading off security and collaboration capabilities in multi-agent systems",
        "status": "published",
        "paper_url": ""
      }
    ],
    "benchmarks": [
      {
        "name": "Catastrophic Cyber Capabilities Benchmark (3CB)",
        "measures": "LLM agent cyber offense capabilities",
        "status": "Active"
      },
      {
        "name": "DarkBench",
        "measures": "Dark patterns in large language models",
        "status": "Active"
      },
      {
        "name": "VLM Deception Benchmark",
        "measures": "Deception detection in vision-language models with 1,048 image-text pairs",
        "status": "Active"
      },
      {
        "name": "Improved Specificity Benchmark",
        "measures": "Edit failures in large language models",
        "status": "Active"
      }
    ],
    "key_people": [
      {
        "name": "E. Kran",
        "role": "Researcher"
      },
      {
        "name": "J. Hoelscher-Obermaier",
        "role": "Researcher"
      },
      {
        "name": "J. Persson",
        "role": "Researcher"
      },
      {
        "name": "F. Barez",
        "role": "Researcher"
      },
      {
        "name": "A. Anurin",
        "role": "Researcher"
      },
      {
        "name": "J. Ng",
        "role": "Researcher"
      },
      {
        "name": "K. Schaffer",
        "role": "Researcher"
      },
      {
        "name": "J. Schreiber",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "EleutherAI",
    "url": "https://www.eleuther.ai/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "EleutherAI trains and releases powerful open source large language models while conducting research on AI safety and interpretability.",
    "focus_areas": [
      "Interpretability",
      "Alignment",
      "Evals"
    ],
    "projects": [
      {
        "name": "Interpreting Across Time",
        "description": "Research on how properties of models emerge and evolve over the course of training",
        "status": "Active"
      },
      {
        "name": "Eliciting Latent Knowledge",
        "description": "Directly eliciting latent knowledge inside model activations to verify claims when humans can't independently check",
        "status": "Active"
      },
      {
        "name": "Training LLMs",
        "description": "Training and releasing powerful open source large language models",
        "status": "Active"
      },
      {
        "name": "Common Pile v0.1",
        "description": "Dataset project",
        "status": "Active"
      },
      {
        "name": "EvalEval Coalition",
        "description": "Evaluation initiative",
        "status": "Active"
      },
      {
        "name": "Reward Hacking Research",
        "description": "Research update on reward hacking in AI systems",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Pretraining Data Filtering for Open-Weight AI Safety",
        "description": "Research on filtering pretraining data to improve safety of open-weight AI models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Attention Probes",
        "description": "Research on probing attention mechanisms in neural networks",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Mechanistic Anomaly Detection",
        "description": "Research on detecting anomalies in AI systems through mechanistic interpretability",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "RLHF and RLAIF in GPT-NeoX",
        "description": "Implementation of Reinforcement Learning from Human Feedback and AI Feedback in GPT-NeoX",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Open Source Automated Interpretability for Sparse Autoencoder Features",
        "description": "Automated interpretability tools for understanding sparse autoencoder features",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Weak-to-Strong Generalization",
        "description": "Experiments in training strong models using supervision from weaker models",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Least-Squares Concept Erasure",
        "description": "Methods for removing concepts from neural network representations",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "GPT-NeoX-20B",
        "description": "Large language model with 20 billion parameters",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "Factored Cognition with Language Models",
        "description": "Preliminary exploration into factored cognition approaches using language models",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Research focus includes tamper-resistant safeguards, morphological alignment of tokenizers, and composable interventions for language models. Active in Summer of Open Science initiative.",
    "employees": 16,
    "key_people": [
      {
        "name": "Stella Biderman",
        "role": "Researcher"
      },
      {
        "name": "Nora Belrose",
        "role": "Researcher"
      },
      {
        "name": "David Johnston",
        "role": "Researcher"
      },
      {
        "name": "Curtis Huebner",
        "role": "Researcher"
      },
      {
        "name": "Leo Gao",
        "role": "Researcher"
      },
      {
        "name": "Connor Leahy",
        "role": "Researcher"
      },
      {
        "name": "Quentin Anthony",
        "role": "Researcher"
      }
    ]
  },
  {
    "name": "Future of Life Institute",
    "url": "https://futureoflife.org/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Steering transformative technology towards benefiting life and away from extreme large-scale risks through policy advocacy, research, and education.",
    "focus_areas": [
      "Alignment",
      "Governance",
      "Policy",
      "Biosecurity",
      "Cyber"
    ],
    "key_people": [
      {
        "name": "Emilia Javorsky",
        "role": "Policy staff"
      },
      {
        "name": "Max Tegmark",
        "role": "Speaker/Representative"
      },
      {
        "name": "Mark Brakel",
        "role": "Grantmaking staff"
      },
      {
        "name": "Anthony Aguirre",
        "role": "Researcher"
      },
      {
        "name": "Michael Kleinman",
        "role": "AI Safety Researcher/Analyst"
      },
      {
        "name": "James H. Moor",
        "role": "Future of Life Award Winner - Computer Ethics and AI Safety"
      },
      {
        "name": "Batya Friedman",
        "role": "Future of Life Award Winner - Computer Ethics and AI Safety"
      },
      {
        "name": "Steve Omohundro",
        "role": "Future of Life Award Winner - Computer Ethics and AI Safety"
      }
    ],
    "projects": [
      {
        "name": "FLI AI Safety Index",
        "description": "Eight AI and governance experts evaluate the safety practices of leading general-purpose AI companies",
        "status": "Active"
      },
      {
        "name": "AI Action Plan Recommendations",
        "description": "Proposal for President Trump's AI Action Plan focusing on AI loss-of-control protection and worker protection",
        "status": "Active"
      },
      {
        "name": "AI Convergence Research",
        "description": "Policy expertise on risks at intersection of AI and nuclear, biological and cyber threats",
        "status": "Active"
      },
      {
        "name": "Autonomous Weapons Education",
        "description": "Educational materials about AI-powered weapons that harm national security",
        "status": "Active"
      },
      {
        "name": "Digital Media Accelerator",
        "description": "Supports digital content creators raising AI awareness",
        "status": "Active"
      },
      {
        "name": "Control Inversion Study",
        "description": "Research on why superintelligent AI agents would absorb power",
        "status": "Active"
      },
      {
        "name": "Control Inversion",
        "description": "AI safety research project",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Statement on Superintelligence",
        "description": "Policy statement on superintelligence risks",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Creative Contest: Keep The Future Human",
        "description": "Creative contest focused on human-centered AI futures",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Existential Safety Community",
        "description": "Community building for AI existential safety",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "FLI AI Safety Index: Winter 2025 Edition",
        "description": "Index measuring AI safety progress and metrics",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Convergence: Risks at the Intersection of AI and Nuclear, Biological and Cyber Threats",
        "description": "Research on convergent risks from AI and other threats",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "Superintelligence Imagined Creative Contest",
        "description": "Creative contest exploring superintelligence scenarios",
        "status": "Completed",
        "paper_url": ""
      },
      {
        "name": "AI Safety Summits",
        "description": "Coordination and participation in AI safety summits",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Tomorrow's AI",
        "description": "A scrollytelling site with 13 interactive, research-backed scenarios showing how advanced AI could transform the world\u2014for better or worse",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "AI Safety Index",
        "description": "Safety scorecard of leading AI companies assessing how they address safety concerns",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "US Federal Agencies: Mapping AI Activities",
        "description": "Guide outlining AI activities across the US Executive Branch, focusing on regulatory authorities, budgets, and programs",
        "status": "published",
        "paper_url": ""
      },
      {
        "name": "Catastrophic AI Scenarios",
        "description": "Concrete examples of how AI could go wrong",
        "status": "published",
        "paper_url": ""
      }
    ],
    "notes": "Organization works across AI, biotechnology, and nuclear weapons risks. Has 40,000+ newsletter subscribers and provides grants to individuals and organizations. Recently announced petition with 65,000+ signatures to ban superintelligence development.",
    "employees": 5,
    "benchmarks": [
      {
        "name": "FLI AI Safety Index",
        "measures": "AI safety progress and metrics",
        "status": "Active"
      },
      {
        "name": "AI Safety Index",
        "measures": "Safety practices and policies of leading AI companies",
        "status": "Active"
      }
    ]
  },
  {
    "name": "AI Safety Camp",
    "url": "https://aisafety.camp/",
    "type": "Nonprofit",
    "country": "International",
    "mission": "AI Safety Camp (AISC) is an AI safety research program that brings together talented researchers to work on technical AI alignment projects in an intensive camp format.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Governance",
      "Evals"
    ],
    "key_people": [
      {
        "name": "Dr Waku",
        "role": "Project Lead - YouTube videos on loss-of-control risk"
      },
      {
        "name": "Remmelt Ellen",
        "role": "Project Lead - Writing about safety failures"
      },
      {
        "name": "Finn",
        "role": "Project Lead - Anti-AI coalition building"
      },
      {
        "name": "Will Petillo",
        "role": "Project Lead - Systems Dynamics Model for AI pause"
      }
    ],
    "projects": [],
    "benchmarks": [
      {
        "name": "Benchmark for Ranking LLM Preferences Relevant for Existential Risk",
        "measures": "LLM preferences related to existential risk scenarios",
        "paper_url": ""
      }
    ],
    "notes": "Runs multiple camps per year. Alumni have gone on to work at leading AI safety organizations."
  },
  {
    "name": "Ought / Elicit",
    "url": "https://elicit.com/",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Elicit helps researchers be 10x more evidence-based by providing AI tools for scientific research including search, analysis, and report generation.",
    "projects": [
      {
        "name": "Research Search",
        "description": "Semantic search over 138 million academic papers and 545,000 clinical trials",
        "status": "Active"
      },
      {
        "name": "Research Reports",
        "description": "Generates high-quality research briefs based on systematic review processes",
        "status": "Active"
      },
      {
        "name": "Systematic Literature Review",
        "description": "Automates screening and data extraction for systematic reviews with up to 80% time savings",
        "status": "Active"
      },
      {
        "name": "Elicit Alerts",
        "description": "Tracks new research developments and sends updates to researchers",
        "status": "Active"
      },
      {
        "name": "Elicit Systematic Review",
        "description": "Advanced workflow delivering human-level accuracy in evidence synthesis, automating screening and data extraction of systematic reviews",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Elicit Reports",
        "description": "Fully automated rigorous research overviews built on principles of systematic reviews with higher quality and efficiency than other deep research tools",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Elicit Notebooks",
        "description": "Research analysis tool unifying search, summaries, and systematic review on a single, living page",
        "status": "Active",
        "paper_url": ""
      },
      {
        "name": "Clinical Trials Integration",
        "description": "Search and analysis capabilities over 545,000 clinical studies from clinicaltrials.gov with instant summarization",
        "status": "Active",
        "paper_url": ""
      }
    ],
    "notes": "Used by over 5 million researchers across pharmaceuticals, academia, medical devices, policy/government, and other industries. Claims to be the most accurate AI product for scientific research with sentence-level citations and transparency features.",
    "employees": 23,
    "benchmarks": [
      {
        "name": "Elicit Accuracy Validation",
        "measures": "Data extraction accuracy and research quality metrics - achieved 99.4% accuracy in systematic review data extraction",
        "status": "Active"
      }
    ]
  },
  {
    "name": "arXiv AI Safety Papers",
    "url": "https://arxiv.org",
    "type": "Academic",
    "country": "International",
    "mission": "Recent AI safety research papers from arXiv preprint server.",
    "focus_areas": [
      "Alignment",
      "Interpretability",
      "Evals"
    ],
    "projects": [
      {
        "name": "Sycophancy Claims about Language Models: The Missing Human-in-the-Loop",
        "status": "published",
        "description": "Authors: Jan Batzner, Volker Stocker, Stefan Schmid...",
        "paper_url": "https://arxiv.org/abs/2512.00656v1"
      },
      {
        "name": "Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics",
        "status": "published",
        "description": "Authors: Deep Patel, Emmanouil-Vasileios Vlatakis-Gkaragkounis",
        "paper_url": "https://arxiv.org/abs/2512.00389v1"
      },
      {
        "name": "AI Consciousness and Existential Risk",
        "status": "published",
        "description": "Authors: Rufin VanRullen",
        "paper_url": "https://arxiv.org/abs/2511.19115v1"
      },
      {
        "name": "Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma",
        "status": "published",
        "description": "Authors: Subramanyam Sahoo, Aman Chadha, Vinija Jain...",
        "paper_url": "https://arxiv.org/abs/2511.19504v1"
      },
      {
        "name": "Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation",
        "status": "published",
        "description": "Authors: Austin Spizzirri",
        "paper_url": "https://arxiv.org/abs/2512.03048v1"
      },
      {
        "name": "Selective Weak-to-Strong Generalization",
        "status": "published",
        "description": "Authors: Hao Lang, Fei Huang, Yongbin Li",
        "paper_url": "https://arxiv.org/abs/2511.14166v1"
      },
      {
        "name": "Maximizing the efficiency of human feedback in AI alignment: a comparative analysis",
        "status": "published",
        "description": "Authors: Andreas Chouliaras, Dimitris Chatzopoulos",
        "paper_url": "https://arxiv.org/abs/2511.12796v2"
      },
      {
        "name": "Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping",
        "status": "published",
        "description": "Authors: Dena Mujtaba, Brian Hu, Anthony Hoogs...",
        "paper_url": "https://arxiv.org/abs/2511.11551v2"
      },
      {
        "name": "Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA",
        "status": "published",
        "description": "Authors: Ayush Pandey, Jai Bardhan, Ishita Jain...",
        "paper_url": "https://arxiv.org/abs/2511.11169v1"
      },
      {
        "name": "Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback",
        "status": "published",
        "description": "Authors: Vijay Keswani, Cyrus Cousins, Breanna Nguyen...",
        "paper_url": "https://arxiv.org/abs/2511.10032v1"
      },
      {
        "name": "The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems",
        "status": "published",
        "description": "Authors: Samih Fadli",
        "paper_url": "https://arxiv.org/abs/2511.10704v1"
      },
      {
        "name": "Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment",
        "status": "published",
        "description": "Authors: Shigeki Kusaka, Keita Saito, Mikoto Kudo...",
        "paper_url": "https://arxiv.org/abs/2511.09105v1"
      },
      {
        "name": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas",
        "status": "published",
        "description": "Authors: Zhen Wang, Yufan Zhou, Zhongyan Luo...",
        "paper_url": "https://arxiv.org/abs/2511.07338v3"
      },
      {
        "name": "Verifying rich robustness properties for neural networks",
        "status": "published",
        "description": "Authors: Mohammad Afzal, S. Akshay, Ashutosh Gupta",
        "paper_url": "https://arxiv.org/abs/2511.07293v1"
      },
      {
        "name": "Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction",
        "status": "published",
        "description": "Authors: Weiyan Shi, Kenny Tsu Wei Choo",
        "paper_url": "https://arxiv.org/abs/2511.04366v1"
      },
      {
        "name": "When Empowerment Disempowers",
        "status": "published",
        "description": "Authors: Claire Yang, Maya Cakmak, Max Kleiman-Weiner",
        "paper_url": "https://arxiv.org/abs/2511.04177v1"
      },
      {
        "name": "Silenced Biases: The Dark Side LLMs Learned to Refuse",
        "status": "published",
        "description": "Authors: Rom Himelstein, Amit LeVi, Brit Youngmann...",
        "paper_url": "https://arxiv.org/abs/2511.03369v2"
      },
      {
        "name": "Approximating the Mathematical Structure of Psychodynamics",
        "status": "published",
        "description": "Authors: Bryce-Allen Bagley, Navin Khoshnan",
        "paper_url": "https://arxiv.org/abs/2511.05580v1"
      },
      {
        "name": "Deep Value Benchmark: Measuring Whether Models Generalize Deep Values or Shallow Preferences",
        "status": "published",
        "description": "Authors: Joshua Ashkinaze, Hua Shen, Sai Avula...",
        "paper_url": "https://arxiv.org/abs/2511.02109v2"
      },
      {
        "name": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory",
        "status": "published",
        "description": "Authors: Kyung-Hoon Kim",
        "paper_url": "https://arxiv.org/abs/2511.00926v3"
      },
      {
        "name": "Evaluating Concept Filtering Defenses against Child Sexual Abuse Material Generation by Text-to-Image Models",
        "status": "published",
        "description": "Authors: Ana-Maria Cretu, Klim Kireev, Amro Abdalla...",
        "paper_url": "https://arxiv.org/abs/2512.05707v1"
      },
      {
        "name": "SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures",
        "status": "published",
        "description": "Authors: Panuthep Tasawong, Jian Gang Ngui, Alham Fikri Aji...",
        "paper_url": "https://arxiv.org/abs/2512.05501v1"
      },
      {
        "name": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
        "status": "published",
        "description": "Authors: Afshin Khadangi, Hanna Marxen, Amir Sartipi...",
        "paper_url": "https://arxiv.org/abs/2512.04124v1"
      },
      {
        "name": "From monoliths to modules: Decomposing transducers for efficient world modelling",
        "status": "published",
        "description": "Authors: Alexander Boyd, Franz Nowak, David Hyland...",
        "paper_url": "https://arxiv.org/abs/2512.02193v1"
      },
      {
        "name": "Evaluating AI Companies' Frontier Safety Frameworks: Methodology and Results",
        "status": "published",
        "description": "Authors: Lily Stelling, Malcolm Murray, Simeon Campos...",
        "paper_url": "https://arxiv.org/abs/2512.01166v1"
      },
      {
        "name": "The 2nd Workshop on Human-Centered Recommender Systems",
        "status": "published",
        "description": "Authors: Kaike Zhang, Jiakai Tang, Du Su...",
        "paper_url": "https://arxiv.org/abs/2511.19979v1"
      },
      {
        "name": "International AI Safety Report 2025: Second Key Update: Technical Safeguards and Risk Management",
        "status": "published",
        "description": "Authors: Yoshua Bengio, Stephen Clare, Carina Prunkl...",
        "paper_url": "https://arxiv.org/abs/2511.19863v1"
      },
      {
        "name": "Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness",
        "status": "published",
        "description": "Authors: Svitlana Volkova, Will Dupree, Hsien-Te Kao...",
        "paper_url": "https://arxiv.org/abs/2511.21749v1"
      },
      {
        "name": "Monte Carlo Expected Threat (MOCET) Scoring",
        "status": "published",
        "description": "Authors: Joseph Kim, Saahith Potluri",
        "paper_url": "https://arxiv.org/abs/2511.16823v1"
      },
      {
        "name": "How Should the Law Treat Future AI Systems? Fictional Legal Personhood versus Legal Identity",
        "status": "published",
        "description": "Authors: Heather J. Alexander, Jonathan A. Simon, Fr\u00e9d\u00e9ric Pinard",
        "paper_url": "https://arxiv.org/abs/2511.14964v1"
      },
      {
        "name": "SGuard-v1: Safety Guardrail for Large Language Models",
        "status": "published",
        "description": "Authors: JoonHo Lee, HyeonMin Cho, Jaewoong Yun...",
        "paper_url": "https://arxiv.org/abs/2511.12497v1"
      },
      {
        "name": "Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario",
        "status": "published",
        "description": "Authors: Dhanesh Ramachandram, Anne Loefler, Surain Roberts...",
        "paper_url": "https://arxiv.org/abs/2511.12409v1"
      },
      {
        "name": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning",
        "status": "published",
        "description": "Authors: Zhiyu An, Wan Du",
        "paper_url": "https://arxiv.org/abs/2511.12271v1"
      },
      {
        "name": "Can AI Models be Jailbroken to Phish Elderly Victims? An End-to-End Evaluation",
        "status": "published",
        "description": "Authors: Fred Heiding, Simon Lermen",
        "paper_url": "https://arxiv.org/abs/2511.11759v1"
      },
      {
        "name": "Consensus Sampling for Safer Generative AI",
        "status": "published",
        "description": "Authors: Adam Tauman Kalai, Yael Tauman Kalai, Or Zamir",
        "paper_url": "https://arxiv.org/abs/2511.09493v1"
      },
      {
        "name": "3D Guard-Layer: An Integrated Agentic AI Safety System for Edge Artificial Intelligence",
        "status": "published",
        "description": "Authors: Eren Kurshan, Yuan Xie, Paul Franzon",
        "paper_url": "https://arxiv.org/abs/2511.08842v1"
      },
      {
        "name": "Investigating CoT Monitorability in Large Reasoning Models",
        "status": "published",
        "description": "Authors: Shu Yang, Junchao Wu, Xilin Gong...",
        "paper_url": "https://arxiv.org/abs/2511.08525v2"
      },
      {
        "name": "A Self-Improving Architecture for Dynamic Safety in Large Language Models",
        "status": "published",
        "description": "Authors: Tyler Slater",
        "paper_url": "https://arxiv.org/abs/2511.07645v1"
      },
      {
        "name": "EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers",
        "status": "published",
        "description": "Authors: Yilin Jiang, Mingzi Zhang, Xuanyu Yin...",
        "paper_url": "https://arxiv.org/abs/2511.06890v1"
      },
      {
        "name": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models",
        "status": "published",
        "description": "Authors: Isha Gupta, David Khachaturov, Robert Mullins",
        "paper_url": "https://arxiv.org/abs/2502.00718v2"
      },
      {
        "name": "Characterizing Out-of-Distribution Error via Optimal Transport",
        "status": "published",
        "description": "Authors: Yuzhe Lu, Yilong Qin, Runtian Zhai...",
        "paper_url": "https://arxiv.org/abs/2305.15640v3"
      },
      {
        "name": "SIFU: Sequential Informed Federated Unlearning for Efficient and Provable Client Unlearning in Federated Optimization",
        "status": "published",
        "description": "Authors: Yann Fraboni, Martin Van Waerebeke, Kevin Scaman...",
        "paper_url": "https://arxiv.org/abs/2211.11656v5"
      },
      {
        "name": "Unifying Evaluation of Machine Learning Safety Monitors",
        "status": "published",
        "description": "Authors: Joris Guerin, Raul Sena Ferreira, Kevin Delmas...",
        "paper_url": "https://arxiv.org/abs/2208.14660v1"
      },
      {
        "name": "Exploring the Design of Adaptation Protocols for Improved Generalization and Machine Learning Safety",
        "status": "published",
        "description": "Authors: Puja Trivedi, Danai Koutra, Jayaraman J. Thiagarajan",
        "paper_url": "https://arxiv.org/abs/2207.12615v1"
      },
      {
        "name": "Learn2Weight: Parameter Adaptation against Similar-domain Adversarial Attacks",
        "status": "published",
        "description": "Authors: Siddhartha Datta",
        "paper_url": "https://arxiv.org/abs/2205.07315v2"
      },
      {
        "name": "Taxonomy of Machine Learning Safety: A Survey and Primer",
        "status": "published",
        "description": "Authors: Sina Mohseni, Haotao Wang, Zhiding Yu...",
        "paper_url": "https://arxiv.org/abs/2106.04823v2"
      },
      {
        "name": "Soft Labeling Affects Out-of-Distribution Detection of Deep Neural Networks",
        "status": "published",
        "description": "Authors: Doyup Lee, Yeongjae Cheon",
        "paper_url": "https://arxiv.org/abs/2007.03212v1"
      },
      {
        "name": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles",
        "status": "published",
        "description": "Authors: Sina Mohseni, Mandar Pitale, Vasu Singh...",
        "paper_url": "https://arxiv.org/abs/1912.09630v1"
      },
      {
        "name": "On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products",
        "status": "published",
        "description": "Authors: Kush R. Varshney, Homa Alemzadeh",
        "paper_url": "https://arxiv.org/abs/1610.01256v2"
      },
      {
        "name": "When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate",
        "status": "published",
        "description": "Authors: Florent Forest, Amaury Wei, Olga Fink",
        "paper_url": "https://arxiv.org/abs/2512.03578v1"
      }
    ],
    "key_people": [
      {
        "name": "Jan Batzner",
        "role": "Author"
      },
      {
        "name": "Volker Stocker",
        "role": "Author"
      },
      {
        "name": "Stefan Schmid",
        "role": "Author"
      },
      {
        "name": "Deep Patel",
        "role": "Author"
      },
      {
        "name": "Emmanouil-Vasileios Vlatakis-Gkaragkounis",
        "role": "Author"
      },
      {
        "name": "Rufin VanRullen",
        "role": "Author"
      },
      {
        "name": "Subramanyam Sahoo",
        "role": "Author"
      },
      {
        "name": "Aman Chadha",
        "role": "Author"
      },
      {
        "name": "Vinija Jain",
        "role": "Author"
      },
      {
        "name": "Austin Spizzirri",
        "role": "Author"
      },
      {
        "name": "Hao Lang",
        "role": "Author"
      },
      {
        "name": "Fei Huang",
        "role": "Author"
      },
      {
        "name": "Yongbin Li",
        "role": "Author"
      },
      {
        "name": "Andreas Chouliaras",
        "role": "Author"
      },
      {
        "name": "Dimitris Chatzopoulos",
        "role": "Author"
      },
      {
        "name": "Dena Mujtaba",
        "role": "Author"
      },
      {
        "name": "Brian Hu",
        "role": "Author"
      },
      {
        "name": "Anthony Hoogs",
        "role": "Author"
      },
      {
        "name": "Ayush Pandey",
        "role": "Author"
      },
      {
        "name": "Jai Bardhan",
        "role": "Author"
      },
      {
        "name": "Ishita Jain",
        "role": "Author"
      },
      {
        "name": "Vijay Keswani",
        "role": "Author"
      },
      {
        "name": "Cyrus Cousins",
        "role": "Author"
      },
      {
        "name": "Breanna Nguyen",
        "role": "Author"
      },
      {
        "name": "Samih Fadli",
        "role": "Author"
      },
      {
        "name": "Shigeki Kusaka",
        "role": "Author"
      },
      {
        "name": "Keita Saito",
        "role": "Author"
      },
      {
        "name": "Mikoto Kudo",
        "role": "Author"
      },
      {
        "name": "Zhen Wang",
        "role": "Author"
      },
      {
        "name": "Yufan Zhou",
        "role": "Author"
      },
      {
        "name": "Zhongyan Luo",
        "role": "Author"
      },
      {
        "name": "Mohammad Afzal",
        "role": "Author"
      },
      {
        "name": "S. Akshay",
        "role": "Author"
      },
      {
        "name": "Ashutosh Gupta",
        "role": "Author"
      },
      {
        "name": "Weiyan Shi",
        "role": "Author"
      },
      {
        "name": "Kenny Tsu Wei Choo",
        "role": "Author"
      },
      {
        "name": "Claire Yang",
        "role": "Author"
      },
      {
        "name": "Maya Cakmak",
        "role": "Author"
      },
      {
        "name": "Max Kleiman-Weiner",
        "role": "Author"
      },
      {
        "name": "Rom Himelstein",
        "role": "Author"
      },
      {
        "name": "Amit LeVi",
        "role": "Author"
      },
      {
        "name": "Brit Youngmann",
        "role": "Author"
      },
      {
        "name": "Bryce-Allen Bagley",
        "role": "Author"
      },
      {
        "name": "Navin Khoshnan",
        "role": "Author"
      },
      {
        "name": "Joshua Ashkinaze",
        "role": "Author"
      },
      {
        "name": "Hua Shen",
        "role": "Author"
      },
      {
        "name": "Sai Avula",
        "role": "Author"
      },
      {
        "name": "Kyung-Hoon Kim",
        "role": "Author"
      },
      {
        "name": "Ana-Maria Cretu",
        "role": "Author"
      },
      {
        "name": "Klim Kireev",
        "role": "Author"
      },
      {
        "name": "Amro Abdalla",
        "role": "Author"
      },
      {
        "name": "Panuthep Tasawong",
        "role": "Author"
      },
      {
        "name": "Jian Gang Ngui",
        "role": "Author"
      },
      {
        "name": "Alham Fikri Aji",
        "role": "Author"
      },
      {
        "name": "Afshin Khadangi",
        "role": "Author"
      },
      {
        "name": "Hanna Marxen",
        "role": "Author"
      },
      {
        "name": "Amir Sartipi",
        "role": "Author"
      },
      {
        "name": "Alexander Boyd",
        "role": "Author"
      },
      {
        "name": "Franz Nowak",
        "role": "Author"
      },
      {
        "name": "David Hyland",
        "role": "Author"
      },
      {
        "name": "Lily Stelling",
        "role": "Author"
      },
      {
        "name": "Malcolm Murray",
        "role": "Author"
      },
      {
        "name": "Simeon Campos",
        "role": "Author"
      },
      {
        "name": "Kaike Zhang",
        "role": "Author"
      },
      {
        "name": "Jiakai Tang",
        "role": "Author"
      },
      {
        "name": "Du Su",
        "role": "Author"
      },
      {
        "name": "Yoshua Bengio",
        "role": "Author"
      },
      {
        "name": "Stephen Clare",
        "role": "Author"
      },
      {
        "name": "Carina Prunkl",
        "role": "Author"
      },
      {
        "name": "Svitlana Volkova",
        "role": "Author"
      },
      {
        "name": "Will Dupree",
        "role": "Author"
      },
      {
        "name": "Hsien-Te Kao",
        "role": "Author"
      },
      {
        "name": "Joseph Kim",
        "role": "Author"
      },
      {
        "name": "Saahith Potluri",
        "role": "Author"
      },
      {
        "name": "Heather J. Alexander",
        "role": "Author"
      },
      {
        "name": "Jonathan A. Simon",
        "role": "Author"
      },
      {
        "name": "Fr\u00e9d\u00e9ric Pinard",
        "role": "Author"
      },
      {
        "name": "JoonHo Lee",
        "role": "Author"
      },
      {
        "name": "HyeonMin Cho",
        "role": "Author"
      },
      {
        "name": "Jaewoong Yun",
        "role": "Author"
      },
      {
        "name": "Dhanesh Ramachandram",
        "role": "Author"
      },
      {
        "name": "Anne Loefler",
        "role": "Author"
      },
      {
        "name": "Surain Roberts",
        "role": "Author"
      },
      {
        "name": "Zhiyu An",
        "role": "Author"
      },
      {
        "name": "Wan Du",
        "role": "Author"
      },
      {
        "name": "Fred Heiding",
        "role": "Author"
      },
      {
        "name": "Adam Tauman Kalai",
        "role": "Author"
      },
      {
        "name": "Yael Tauman Kalai",
        "role": "Author"
      },
      {
        "name": "Or Zamir",
        "role": "Author"
      },
      {
        "name": "Eren Kurshan",
        "role": "Author"
      },
      {
        "name": "Yuan Xie",
        "role": "Author"
      },
      {
        "name": "Paul Franzon",
        "role": "Author"
      },
      {
        "name": "Shu Yang",
        "role": "Author"
      },
      {
        "name": "Junchao Wu",
        "role": "Author"
      },
      {
        "name": "Xilin Gong",
        "role": "Author"
      },
      {
        "name": "Tyler Slater",
        "role": "Author"
      },
      {
        "name": "Yilin Jiang",
        "role": "Author"
      },
      {
        "name": "Mingzi Zhang",
        "role": "Author"
      },
      {
        "name": "Xuanyu Yin",
        "role": "Author"
      },
      {
        "name": "Isha Gupta",
        "role": "Author"
      },
      {
        "name": "David Khachaturov",
        "role": "Author"
      },
      {
        "name": "Robert Mullins",
        "role": "Author"
      },
      {
        "name": "Yuzhe Lu",
        "role": "Author"
      },
      {
        "name": "Yilong Qin",
        "role": "Author"
      },
      {
        "name": "Runtian Zhai",
        "role": "Author"
      },
      {
        "name": "Yann Fraboni",
        "role": "Author"
      },
      {
        "name": "Martin Van Waerebeke",
        "role": "Author"
      },
      {
        "name": "Kevin Scaman",
        "role": "Author"
      },
      {
        "name": "Joris Guerin",
        "role": "Author"
      },
      {
        "name": "Raul Sena Ferreira",
        "role": "Author"
      },
      {
        "name": "Kevin Delmas",
        "role": "Author"
      },
      {
        "name": "Puja Trivedi",
        "role": "Author"
      },
      {
        "name": "Danai Koutra",
        "role": "Author"
      },
      {
        "name": "Jayaraman J. Thiagarajan",
        "role": "Author"
      },
      {
        "name": "Siddhartha Datta",
        "role": "Author"
      },
      {
        "name": "Sina Mohseni",
        "role": "Author"
      },
      {
        "name": "Haotao Wang",
        "role": "Author"
      },
      {
        "name": "Zhiding Yu",
        "role": "Author"
      },
      {
        "name": "Doyup Lee",
        "role": "Author"
      },
      {
        "name": "Yeongjae Cheon",
        "role": "Author"
      },
      {
        "name": "Mandar Pitale",
        "role": "Author"
      },
      {
        "name": "Vasu Singh",
        "role": "Author"
      },
      {
        "name": "Kush R. Varshney",
        "role": "Author"
      },
      {
        "name": "Homa Alemzadeh",
        "role": "Author"
      },
      {
        "name": "Florent Forest",
        "role": "Author"
      },
      {
        "name": "Amaury Wei",
        "role": "Author"
      },
      {
        "name": "Olga Fink",
        "role": "Author"
      }
    ]
  },
  {
    "name": "RAND TASP",
    "url": "",
    "type": "Think Tank",
    "country": "United States",
    "mission": "RAND Technology and Security Policy program focusing on AI safety research and policy.",
    "employees": 75,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 4,
    "managers": 5,
    "subteams": 4
  },
  {
    "name": "CSET",
    "url": "",
    "type": "Think Tank",
    "country": "United States",
    "mission": "Center for Security and Emerging Technology at Georgetown University.",
    "employees": 63,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "80,000 Hours",
    "url": "",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Career advice organization focused on high-impact careers including AI safety.",
    "employees": 35,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Lakera",
    "url": "",
    "type": "Lab Safety Team",
    "country": "Switzerland",
    "mission": "AI security company building guardrails for LLM applications.",
    "employees": 33,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "AISLE",
    "url": "",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "AI Safety Labs Europe.",
    "employees": 33,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Partnership on AI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Multi-stakeholder organization working on AI best practices.",
    "employees": 30,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Longview",
    "url": "",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Philanthropic advisory organization focused on AI safety and other cause areas.",
    "employees": 25,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 4
  },
  {
    "name": "Virtue AI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety startup.",
    "employees": 24,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Palisade Research",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 20,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Goodfire",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI interpretability startup.",
    "employees": 20,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "LawAI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI and law research organization.",
    "employees": 13,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "CSER",
    "url": "",
    "type": "Academic",
    "country": "United Kingdom",
    "mission": "Centre for the Study of Existential Risk at Cambridge University.",
    "employees": 19,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Gray Swan AI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company focused on adversarial robustness.",
    "employees": 18,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "Constellation",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research and community organization.",
    "employees": 18,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2,
    "managers": 3,
    "subteams": 3
  },
  {
    "name": "The Future Society",
    "url": "",
    "type": "Think Tank",
    "country": "International",
    "mission": "AI governance and policy research organization.",
    "employees": 17,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CLTC",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for Long-Term Cybersecurity at UC Berkeley.",
    "employees": 16,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "IAPS",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Institute for AI Policy and Strategy.",
    "employees": 15,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 4,
    "managers": 2
  },
  {
    "name": "Dreadnode",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI red teaming and security company.",
    "employees": 15,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "Transluce",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI transparency and interpretability company.",
    "employees": 15,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "AI Now Institute",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Research institute examining the social implications of AI.",
    "employees": 14,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Global Center on AI Governance",
    "url": "",
    "type": "Think Tank",
    "country": "International",
    "mission": "Research center focused on AI governance frameworks.",
    "employees": 14,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Timaeus",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization focused on developmental interpretability.",
    "employees": 13,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 4
  },
  {
    "name": "CLTR",
    "url": "",
    "type": "Think Tank",
    "country": "United Kingdom",
    "mission": "Centre for Long-Term Resilience.",
    "employees": 13,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Iliad",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "LawZero",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI legal safety company.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "CeSIA",
    "url": "",
    "type": "Academic",
    "country": "Italy",
    "mission": "Center for AI Safety Italy.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 4
  },
  {
    "name": "AOI",
    "url": "",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "AI Objectives Institute.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "Concordia AI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "HAIST",
    "url": "",
    "type": "Academic",
    "country": "South Korea",
    "mission": "Human-centered AI Safety & Trust lab.",
    "employees": 12,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "BAIF",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Berkeley AI Futures lab.",
    "employees": 11,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "SaferAI",
    "url": "",
    "type": "Nonprofit",
    "country": "France",
    "mission": "French AI safety organization.",
    "employees": 11,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2,
    "managers": 5
  },
  {
    "name": "CARMA",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for AI Risk Management and Alignment.",
    "employees": 11,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1
  },
  {
    "name": "Horizon Institute",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 10,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Atla AI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI evaluation company.",
    "employees": 10,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CIP",
    "url": "",
    "type": "Think Tank",
    "country": "United States",
    "mission": "Center for AI Policy.",
    "employees": 9,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Fathom",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Leap Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI interpretability research lab.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "ERA",
    "url": "",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "Existential Risk Alliance.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Guide Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CSIS Wadhwani AI Center",
    "url": "",
    "type": "Think Tank",
    "country": "United States",
    "mission": "AI policy center at CSIS.",
    "employees": 8,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Encode AI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI policy organization.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Meridian",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "MAIA",
    "url": "",
    "type": "Academic",
    "country": "United Kingdom",
    "mission": "Machine Intelligence and Autonomy lab.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Tarbell Fellowship",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Fellowship program for AI journalism.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Safe AI Forum",
    "url": "",
    "type": "Nonprofit",
    "country": "International",
    "mission": "International AI safety forum.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Forethought",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Arcadia Impact",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety philanthropy advisory.",
    "employees": 7,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Pivotal Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Seismic",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "BlueDot Impact",
    "url": "",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "AI safety education organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2,
    "subteams": 1
  },
  {
    "name": "Odyssean Institute",
    "url": "",
    "type": "Think Tank",
    "country": "United Kingdom",
    "mission": "Technology policy think tank.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Hortus AI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "AVERI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Tilde Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 6,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "Conscium",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI consciousness research.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "LISA",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Laboratory for Intelligent Systems and AI Safety.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Geodesic Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Dovetail Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "ARENA",
    "url": "",
    "type": "Nonprofit",
    "country": "United Kingdom",
    "mission": "AI safety education program.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "AI Futures Project",
    "url": "",
    "type": "Think Tank",
    "country": "United States",
    "mission": "AI futures research.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Whitebox Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI interpretability research.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Cadenza Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "EleosAI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Realm Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Haize Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI red teaming company.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "PRISM Eval",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI evaluation company founded by MATS alumni.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Heron AI Security",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI security company.",
    "employees": 5,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "TFI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Technical AI safety research.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CBAI",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for Beneficial AI.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "PIBBSS",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Program for AI and Neuroscience Safety.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Harmony Intelligence",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2,
    "subteams": 1
  },
  {
    "name": "AI Safety Awareness Project",
    "url": "",
    "type": "Nonprofit",
    "country": "International",
    "mission": "AI safety awareness and education.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Noema Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Evitable",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "GPAI Policy Lab",
    "url": "",
    "type": "Think Tank",
    "country": "International",
    "mission": "Global Partnership on AI policy research.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Golden Gate Institute for AI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research institute.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Charles University ACS",
    "url": "",
    "type": "Academic",
    "country": "Czech Republic",
    "mission": "AI safety research at Charles University.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Kairos",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 4,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Truthful AI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI truthfulness and honesty research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1,
    "subteams": 1
  },
  {
    "name": "AI Underwriting Company",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI risk assessment company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Midas Project",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Oxford Martin AIGI",
    "url": "",
    "type": "Academic",
    "country": "United Kingdom",
    "mission": "Oxford Martin AI Governance Initiative.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Watertight AI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI safety and security company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "MAI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "CLAIR",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for Language and AI Research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Asymmetric Security",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI security research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Atlas Computing",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety computing infrastructure.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3,
    "subteams": 1
  },
  {
    "name": "Fulcrum Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Safeguarded AI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1
  },
  {
    "name": "Secure AI Project",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI security research project.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 3
  },
  {
    "name": "Lucid Computing",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI interpretability company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Contramont Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Cosmos Institute",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Long-term AI research institute.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "CaML",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for AI and Machine Learning safety.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1
  },
  {
    "name": "Equilibria Network",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI coordination research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "EconTAI",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Economics of transformative AI research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Aether",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI ethics and safety research.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 1
  },
  {
    "name": "CivAI",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Civic AI safety organization.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "Equistamp",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI evaluation company.",
    "employees": 3,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "dmodel",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI model analysis company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": [],
    "directors": 2
  },
  {
    "name": "CORAL",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Center for Open and Responsible AI Lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Seldon Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Principia Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "DeepResponse",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI incident response company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Decode Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI interpretability research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Coordinal",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI coordination research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Mosaic Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Andon Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety monitoring company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Aligned AI",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI alignment company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Orthogonal",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United Kingdom",
    "mission": "AI safety research company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Workshop Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety experimentation lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Aelus",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety company.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Simplex",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety simplification research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Aethra Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Freestyle Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Independent AI safety research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Theorem Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI verification research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Ulyssean",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "Long-term AI safety research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Trajectory Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI trajectory research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "TamperSec",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI tamper-resistance security.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "AI Standards Lab",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety standards research.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "LASST",
    "url": "",
    "type": "Academic",
    "country": "United States",
    "mission": "Lab for AI Safety and Security Testing.",
    "employees": 2,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Groundless",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "AI Impacts",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI forecasting and impact research.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Poseidon Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Formation Research",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Theomachia Labs",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI safety research lab.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Ashgro",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Deducto",
    "url": "",
    "type": "Lab Safety Team",
    "country": "United States",
    "mission": "AI reasoning verification company.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  },
  {
    "name": "Luthien",
    "url": "",
    "type": "Nonprofit",
    "country": "United States",
    "mission": "AI safety research organization.",
    "employees": 1,
    "focus_areas": [
      "Alignment"
    ],
    "projects": [],
    "benchmarks": []
  }
]