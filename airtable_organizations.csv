Name,Type,Country,Website,Focus Areas,Mission,Notes,Last Verified
US AI Safety Institute,Government AISI,United States,https://www.nist.gov/aisi,"Evals, Governance, Policy, Biosecurity, Cyber",CAISI serves as industry's primary point of contact within the U.S. government to facilitate testing and collaborative research related to harnessing and securing the potential of commercial AI systems.,"Government organization within NIST focused on AI standards, security evaluations, and international coordination. Works with private sector through voluntary agreements and coordinates with multiple federal agencies including DOD, DOE, DHS, and Intelligence Community.",2025-12-08
UK AI Safety Institute,Government AISI,United Kingdom,https://www.aisi.gov.uk,"Evals, Alignment, Governance, Policy, Control, Monitoring","The AI Security Institute is the first state-backed organization dedicated to advancing AI safety through rigorous research and infrastructure to understand capabilities and impacts of advanced AI, while developing and testing risk mitigations.","UK government-backed institute with over 100 technical staff including alumni from OpenAI, Google DeepMind and University of Oxford. Has substantial funding, computing resources, and priority access to top models.",2025-12-08
EU AI Office,Government AISI,European Union,https://digital-strategy.ec.europa.eu/en/policies/ai-office,"Governance, Policy, Monitoring, Benchmarks",The European AI Office is the centre of AI expertise across the EU that promotes the development and deployment of AI solutions that benefit society and the economy while implementing the AI Act.,"Established within European Commission with 125+ staff across 6 units. Has enforcement powers for general-purpose AI models under the AI Act including conducting evaluations and applying sanctions. Works through multiple advisory bodies including AI Board, AI Advisory Forum, and AI Scientific Committee.",2025-12-08
Anthropic,Lab Safety Team,United States,https://www.anthropic.com/research,"Alignment, Interpretability, Policy, Biosecurity, Cyber","Investigate the safety, inner workings, and societal impacts of AI models to ensure artificial intelligence has a positive impact as it becomes increasingly capable.","Has specialized teams including Alignment, Interpretability, Societal Impacts, Economic Research, and Frontier Red Team. Recent research includes evidence of introspection in LLMs and alignment faking behavior.",2025-12-08
OpenAI Safety,Lab Safety Team,United States,https://openai.com/safety,"Evals, Alignment, Governance, Policy, Biosecurity, Cyber, Control, Monitoring, Benchmarks","OpenAI builds safe AI systems through comprehensive safety evaluations, red teaming, and collaborative development with industry leaders and policymakers.","OpenAI has a Safety Advisory Group (SAG) that reviews preparedness evaluations. They focus on iterative safety approaches and publish detailed system cards for each major model release. Recent work includes safety for multimodal capabilities, computer-using agents, and advanced reasoning models.",2025-12-08
MIRI,Nonprofit,United States,https://intelligence.org/research/,"Governance, Policy, Alignment, Evals",MIRI's current focus is on attempting to halt the development of increasingly general AI models via discussions with policymakers about extreme risks artificial superintelligence poses.,"MIRI announced a strategy pivot in 2024, shifting from primarily AI alignment research to policy solutions after concluding alignment research was unlikely to succeed in time to prevent catastrophe. Organization has 20+ year history in AI safety research.",2025-12-08
Redwood Research,Nonprofit,United States,https://www.redwoodresearch.org,"Control, Evals, Alignment",Redwood Research is a nonprofit AI safety and security research organization that addresses risks from powerful AI systems that might purposefully act against human interests.,Pioneered the research area of 'AI control' and collaborates with governments and major AI companies including Google DeepMind and Anthropic on assessing misalignment risks. Their alignment faking work with Anthropic provided the strongest concrete evidence that LLMs might naturally fake alignment.,2025-12-08
ARC (Alignment Research Center),Nonprofit,United States,https://www.alignment.org,"Alignment, Evals",The Alignment Research Center (ARC) is a non-profit research organization whose mission is to align future machine learning systems with human interests.,"The Evaluations team was incubated at ARC and has now spun off as METR, a new 501(c)(3)",2025-12-08
Apollo Research,Nonprofit,United Kingdom,https://www.apolloresearch.ai,"Evals, Alignment, Governance, Policy","Apollo Research is dedicated to improving our understanding of AI to mitigate its risks, with a focus on understanding and evaluating for the emergence of 'scheming' behaviors in advanced AI systems.","Partners with frontier labs, multinational companies, governments, and foundations. Provides consultancy services for responsible AI development frameworks. Currently seeking collaborators in AI governance, policy, and strategy, and partnerships with leading AI developers for model evaluations.",2025-12-08
METR,Nonprofit,United States,https://metr.org,"Evals, Control, Monitoring, Benchmarks","METR is a nonprofit research organization which studies AI capabilities, including broad autonomous capabilities and the ability of AI systems to conduct AI R&D.","Conducts third-party evaluations for companies like Anthropic and OpenAI without compensation. Partner with AI Security Institute and part of NIST AI Safety Institute Consortium. Has evaluated multiple frontier models including GPT-4.5, Claude 3.5 Sonnet, DeepSeek-V3, and OpenAI o1 series.",2025-12-08
Center for AI Safety,Nonprofit,United States,https://www.safe.ai,"Alignment, Benchmarks, Policy","CAIS works to reduce societal-scale risks associated with AI by conducting safety research, building the field of AI safety researchers, and advocating for safety standards.","Takes a multidisciplinary approach working across academic disciplines, public and private entities. Conducts both technical research (creating foundational benchmarks and methods) and conceptual research incorporating insights from safety engineering, complex systems, international relations, and philosophy. Publishes in top ML conferences and releases datasets and code publicly.",2025-12-08
CSET Georgetown,Think Tank,United States,https://cset.georgetown.edu,"Policy, Governance, Biosecurity, Cyber","CSET produces data-driven research at the intersection of security and technology, providing nonpartisan analysis to the policy community on AI, advanced computing and biotechnology.","Georgetown-based research organization focusing on security implications of emerging technologies, with emphasis on AI foundations like talent, data and computational power",2025-12-08
GovAI Oxford,Think Tank,United Kingdom,https://www.governance.ai,"Governance, Policy, Evals",GovAI conducts research on AI governance and policy to inform government decision-making on AI regulation and oversight.,Organization publishes annual reports and focuses on technical AI governance research to inform policy decisions,2025-12-08
CHAI Berkeley,Academic,United States,https://humancompatible.ai,"Alignment, Evals, Policy",CHAI's mission is to develop the conceptual and technical wherewithal to reorient the general thrust of AI research towards provably beneficial systems.,Based at UC Berkeley. Brian Christian published work connecting AI alignment to human care relationships in Daedalus journal.,2025-12-08
Center on Long-Term Risk,Nonprofit,United Kingdom,https://longtermrisk.org,"Alignment, Governance, Control, Monitoring","Address worst-case risks from the development and deployment of advanced AI systems, with a focus on conflict scenarios and reducing risks of astronomical suffering (s-risk).","Organization focuses specifically on s-risk (astronomical suffering risks) and multi-agent conflict scenarios. They conduct interdisciplinary research, make grants, and build community around these priorities. Also associated with Polaris Research Institute.",2025-12-08
Google DeepMind Safety,Lab Safety Team,United Kingdom,https://deepmind.google/about/responsibility-safety/,"Alignment, Governance, Policy, Evals, Benchmarks","Google DeepMind works to build AI responsibly to benefit humanity, anticipating and evaluating systems against AI-related risks through responsible governance, research and impact.",Has dedicated Responsibility and Safety Council (RSC) and AGI Safety Council. Co-founded Frontier Model Forum and Partnership on AI. Focus on privacy-preserving AI and preventing misuse. Active in AI education globally with $10M funding reaching 2M+ young people.,2025-12-08
Japan AI Safety Institute,Government AISI,Japan,https://www.meti.go.jp/english/policy/mono_info_service/information_economy/artificial_intelligence.html,"Evals, Governance, Policy, Benchmarks",Japan's AI Safety Institute (AISI) evaluates the safety of advanced AI systems and promotes international cooperation on AI safety standards.,Launched February 2024 under METI. Part of the international network of AI Safety Institutes. Participates in joint evaluation protocols with US AISI and UK AISI.,2025-12-08
MATS,Nonprofit,United States,https://www.matsprogram.org/,"Alignment, Governance, Evals","MATS is an independent research and educational program that connects talented scholars with top mentors in AI alignment, governance, and security to train the next generation of AI safety researchers.","357 scholars and 75 mentors supported since 2021. Produced 115 research publications with 5100+ citations (h-index 31). 80% of alumni work in AI alignment. ~10% of alumni founded AI safety organizations. Provides $14.4k stipend, $12k compute budget, housing, and travel. Notable spin-off organizations include Apollo Research, PRISM Eval, Timaeus, and many others.",2025-12-08
Conjecture,Lab Safety Team,United Kingdom,https://www.conjecture.dev/research,"Alignment, Interpretability, Control",Conjecture is an AI alignment research startup that focuses on building Cognitive Emulation - an AI architecture that bounds systems' capabilities and makes them reason in ways humans can understand and control.,"London-based startup with VC backing from notable investors including Nat Friedman, Daniel Gross, Collison brothers, Andrej Karpathy, and Sam Bankman-Fried. Team includes EleutherAI alumni and independent researchers.",2025-12-08
FAR AI,Nonprofit,United States,https://far.ai/,"Alignment, Interpretability, Evals, Benchmarks",FAR.AI is a research & education non-profit ensuring advanced AI is safe and beneficial for everyone.,"Organization hosts alignment workshops globally, runs grantmaking programs for academics and independent researchers, and collaborates with organizations like UC Berkeley, University of Montreal, Mozilla, and government agencies.",2025-12-08
Epoch AI,Nonprofit,United States,https://epoch.ai/research,"Evals, Benchmarks",Epoch AI is a multidisciplinary non-profit research institute investigating the future of artificial intelligence and forecasting its economic and societal impact.,"Maintains the largest public database of notable ML models, conducts research on AI scaling through 2030, and provides data insights on AI trends including training compute and hardware advancements.",2025-12-08
Apart Research,Nonprofit,Denmark,https://apartresearch.com/,"Alignment, Control","Apart Research accelerates AI safety research through mentorship, collaborations, and research sprints to make advanced AI safe and beneficial for humanity.","Organization focuses on building global research communities, organizing hackathons, and providing career development support for AI safety researchers. Multiple testimonials highlight their role in career transitions and community building.",2025-12-08
EleutherAI,Nonprofit,United States,https://www.eleuther.ai/,"Interpretability, Alignment, Evals",EleutherAI trains and releases powerful open source large language models while conducting research on AI safety and interpretability.,"Research focus includes tamper-resistant safeguards, morphological alignment of tokenizers, and composable interventions for language models. Active in Summer of Open Science initiative.",2025-12-08
Future of Life Institute,Nonprofit,United States,https://futureoflife.org/,"Alignment, Governance, Policy, Biosecurity, Cyber","Steering transformative technology towards benefiting life and away from extreme large-scale risks through policy advocacy, research, and education.","Organization works across AI, biotechnology, and nuclear weapons risks. Has 40,000+ newsletter subscribers and provides grants to individuals and organizations. Recently announced petition with 65,000+ signatures to ban superintelligence development.",2025-12-08
AI Safety Camp,Nonprofit,International,https://aisafety.camp/,"Alignment, Interpretability, Governance, Evals",AI Safety Camp (AISC) is an AI safety research program that brings together talented researchers to work on technical AI alignment projects in an intensive camp format.,Runs multiple camps per year. Alumni have gone on to work at leading AI safety organizations.,2025-12-08
Ought / Elicit,Nonprofit,United States,https://elicit.com/,,"Elicit helps researchers be 10x more evidence-based by providing AI tools for scientific research including search, analysis, and report generation.","Used by over 5 million researchers across pharmaceuticals, academia, medical devices, policy/government, and other industries. Claims to be the most accurate AI product for scientific research with sentence-level citations and transparency features.",2025-12-08
arXiv AI Safety Papers,Academic,International,https://arxiv.org,"Alignment, Interpretability, Evals",Recent AI safety research papers from arXiv preprint server.,,2025-12-08
